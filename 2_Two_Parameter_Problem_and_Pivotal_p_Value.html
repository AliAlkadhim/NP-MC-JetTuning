
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>2 - Two-Parameter Problem and Pivotal Likelihood-Free p-Values &#8212; Pivotal Likelihood-Free Inference for Particle Physics</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="3 - 2-Dimensional Inference in \(\theta - \nu\) Space and Replacing Observed Data with Test Statistics" href="3_Replacing_Data_with_Lambda_and_2D_Inference.html" />
    <link rel="prev" title="1 - Pivotal LFI for Count Data in Particle Physics: Background and one-parameter Problem" href="1_Intro_and_One_Parameter_Problem.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Pivotal Likelihood-Free Inference for Particle Physics</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the Likelihood-Free Inference for Particle Physics Jupyter book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1_Intro_and_One_Parameter_Problem.html">
   1 - Pivotal LFI for Count Data in Particle Physics: Background and one-parameter Problem
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2 - Two-Parameter Problem and Pivotal Likelihood-Free p-Values
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3_Replacing_Data_with_Lambda_and_2D_Inference.html">
   3 -  2-Dimensional Inference in
   <span class="math notranslate nohighlight">
    \(\theta - \nu\)
   </span>
   Space and Replacing Observed Data with Test Statistics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4_Mapping_Confidence_Sets_to_Confidence_Intervals.html">
   4 - Mapping Confidence Sets to Confidence Intervals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5_Imposing_Pivotal_Conditions_on_Lambda.html">
   5. Imposing Pivotal Conditions on
   <span class="math notranslate nohighlight">
    \(\lambda\)
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6_More_Ideas.html">
   More Discussions
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/AliAlkadhim/LFI_HEP/master?urlpath=tree/JupyterBook/2_Two_Parameter_Problem_and_Pivotal_p_Value.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/2_Two_Parameter_Problem_and_Pivotal_p_Value.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   2 - Two-Parameter Problem and Pivotal Likelihood-Free p-Values
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pivotal-likelihood-free-inference-for-count-data-in-particle-physics">
     Pivotal Likelihood-Free Inference for Count Data in Particle Physics
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#external-imports">
       External imports
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#import-utils">
       import utils
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background-and-problem-statement">
   Background and Problem Statement
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-two-parameter-problem">
   The two-parameter problem
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#suppose-we-have-the-following-data-in-the-context-of-our-two-parameter-problem">
     Suppose we have the following data, in the context of our two-parameter problem:
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#observed-data">
     Observed Data:
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parameters">
     Parameters:
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#auxiliary-simulated-data-simulated-on-the-fly-for-each-observation">
     Auxiliary (simulated) Data (simulated on-the-fly for each observation):
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ann-lee-s-algorithm">
       Ann Lee’s Algorithm
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#our-algorithm-for-finding-p-value-with-lfi">
       Our Algorithm for Finding p-value with LFI
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#we-ll-discuss-this-block-of-code-after-the-plot">
       we’ll discuss this block of code after the plot
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lfi-to-calculate-p-values">
     LFI to Calculate p-values
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#generate-6-pairs-tuples-of-theta-nu-values">
       Generate 6 pairs (tuples) of
       <span class="math notranslate nohighlight">
        \((\theta, \nu)\)
       </span>
       values
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#generate-training-data-or-take-a-look-at-the-saved-training-data">
       Generate Training data (or take a look at the saved training data)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generate-theta-i-nu-i-n-i-m-i-z-i-data-according-to-the-following-and-according-to-whether-mle-true-and-save-as-dataframes">
     Generate
     <span class="math notranslate nohighlight">
      \(\{\theta_i, \nu_i, N_i, M_i, Z_i \}\)
     </span>
     data according to the following, and according to whether MLE=True, and save as dataframes
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#write-custom-data-loader">
   Write Custom Data Loader
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#plot-the-histogrammed-approximations-for-the-mle-vs-non-mle-cases-for-a-single-value-of-mathbf-nu">
     Plot the histogrammed approximations for the MLE vs non-MLE cases for a single value of
     <span class="math notranslate nohighlight">
      \(\mathbf{\nu}\)
     </span>
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#plot-the-histogrammed-approximation-mathbf-h-for-the-mle-vs-non-mle-cases-for-multiple-values-of-mathbf-nu-indicating-the-dependence-on-the-nuissance-parameter">
     Plot the histogrammed approximation
     <span class="math notranslate nohighlight">
      \(\mathbf{h}\)
     </span>
     , for the MLE vs non-MLE cases for multiple values of
     <span class="math notranslate nohighlight">
      \(\mathbf{\nu}\)
     </span>
     , indicating the dependence on the nuissance parameter
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ml">
   ML
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#define-model-mathbf-f-which-will-approximate-the-expectation-value-above">
     Define Model
     <span class="math notranslate nohighlight">
      \(\mathbf{f}\)
     </span>
     , which will approximate the expectation value above
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#make-a-hyperparameter-tuning-workflow">
   Make a hyperparameter Tuning Workflow
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#don-t-run-the-one-cell-below-unless-you-want-to-tune">
     Don’t run the one cell below, unless you want to tune!
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-the-dictionary-of-the-best-hyperparameters-that-was-saved-from-our-hyperparameter-tuning-workflow-and-retrieve-the-values">
   Load the dictionary of the best hyperparameters that was saved from our hyperparameter tuning workflow, and retrieve the values
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#define-network-node-shapes-parameters-and-training-data">
     Define network node shapes, parameters, and training data
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initiate-model-based-on-choice-of-whose-model-ali-or-harrison-and-parameters">
     Initiate model based on choice of whose model (Ali or Harrison) and parameters
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#then-train">
       Then train
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-you-can-scroll-down-to-load-up-trained-model-instead-of-training-now">
   Training: You can scroll down to load up trained model instead of training now
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-mle-model">
     Train MLE model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-non-mle-model">
     Train non-MLE model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#make-sure-the-train-df-has-the-same-ranges-as-the-data-you-want-to-generate-for-evaluation">
       Make sure the train df has the same ranges as the data you want to generate for evaluation
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#make-on-the-fly-generated-evaluation-data">
     Make “on-the-fly” generated evaluation data
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#look-at-an-example-of-a-on-the-fly-generated-evaluation-data">
       Look at an example of a “on-the-fly” generated evaluation data
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-and-evaluate-trained-model-at-generated-data">
   Load and Evaluate Trained model at generated data
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluate-model-at-an-example-set-of-eval-data-points-to-see-the-predicted-p-value-hat-p">
   Evaluate model at an example set of “eval_data” points to see the predicted
   <span class="math notranslate nohighlight">
    \(p\)
   </span>
   -value (
   <span class="math notranslate nohighlight">
    \(\hat{p}\)
   </span>
   )
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#save-trained-model-if-they-re-good-and-if-you-havent-saved-by-now">
     SAVE TRAINED MODEL (if they’re good, and if you havent saved by now)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#this-is-how-you-load-a-trained-model">
     This is how you load a trained model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#you-could-also-evaluate-the-trained-model-on-the-validation-data">
     You could also evaluate the trained model on the validation data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-to-pivot">
   Learning to Pivot
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aside-a-few-words-on-pytorch-s-autograd-more-on-this-in-https-github-com-alialkadhim-lfi-hep-blob-main-src-two-params-autograd2-ipynb">
     Aside: a few words on pytorch’s autograd (more on this in https://github.com/AliAlkadhim/LFI_HEP/blob/main/src/Two_params/autograd2.ipynb)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-the-results-image-of-the-pivotal-model-so-far">
   Load the results image of the pivotal model (so far)
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#explore-the-new-pivotal-models-and-save-if-good">
   Explore the new pivotal models and save if good
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#try-to-automatically-tune-kappa-under-construction">
   Try to automatically tune
   <span class="math notranslate nohighlight">
    \(\kappa\)
   </span>
   (under construction)
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>2 - Two-Parameter Problem and Pivotal Likelihood-Free p-Values</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   2 - Two-Parameter Problem and Pivotal Likelihood-Free p-Values
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pivotal-likelihood-free-inference-for-count-data-in-particle-physics">
     Pivotal Likelihood-Free Inference for Count Data in Particle Physics
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#external-imports">
       External imports
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#import-utils">
       import utils
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background-and-problem-statement">
   Background and Problem Statement
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-two-parameter-problem">
   The two-parameter problem
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#suppose-we-have-the-following-data-in-the-context-of-our-two-parameter-problem">
     Suppose we have the following data, in the context of our two-parameter problem:
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#observed-data">
     Observed Data:
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parameters">
     Parameters:
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#auxiliary-simulated-data-simulated-on-the-fly-for-each-observation">
     Auxiliary (simulated) Data (simulated on-the-fly for each observation):
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ann-lee-s-algorithm">
       Ann Lee’s Algorithm
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#our-algorithm-for-finding-p-value-with-lfi">
       Our Algorithm for Finding p-value with LFI
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#we-ll-discuss-this-block-of-code-after-the-plot">
       we’ll discuss this block of code after the plot
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lfi-to-calculate-p-values">
     LFI to Calculate p-values
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#generate-6-pairs-tuples-of-theta-nu-values">
       Generate 6 pairs (tuples) of
       <span class="math notranslate nohighlight">
        \((\theta, \nu)\)
       </span>
       values
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#generate-training-data-or-take-a-look-at-the-saved-training-data">
       Generate Training data (or take a look at the saved training data)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generate-theta-i-nu-i-n-i-m-i-z-i-data-according-to-the-following-and-according-to-whether-mle-true-and-save-as-dataframes">
     Generate
     <span class="math notranslate nohighlight">
      \(\{\theta_i, \nu_i, N_i, M_i, Z_i \}\)
     </span>
     data according to the following, and according to whether MLE=True, and save as dataframes
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#write-custom-data-loader">
   Write Custom Data Loader
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#plot-the-histogrammed-approximations-for-the-mle-vs-non-mle-cases-for-a-single-value-of-mathbf-nu">
     Plot the histogrammed approximations for the MLE vs non-MLE cases for a single value of
     <span class="math notranslate nohighlight">
      \(\mathbf{\nu}\)
     </span>
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#plot-the-histogrammed-approximation-mathbf-h-for-the-mle-vs-non-mle-cases-for-multiple-values-of-mathbf-nu-indicating-the-dependence-on-the-nuissance-parameter">
     Plot the histogrammed approximation
     <span class="math notranslate nohighlight">
      \(\mathbf{h}\)
     </span>
     , for the MLE vs non-MLE cases for multiple values of
     <span class="math notranslate nohighlight">
      \(\mathbf{\nu}\)
     </span>
     , indicating the dependence on the nuissance parameter
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ml">
   ML
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#define-model-mathbf-f-which-will-approximate-the-expectation-value-above">
     Define Model
     <span class="math notranslate nohighlight">
      \(\mathbf{f}\)
     </span>
     , which will approximate the expectation value above
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#make-a-hyperparameter-tuning-workflow">
   Make a hyperparameter Tuning Workflow
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#don-t-run-the-one-cell-below-unless-you-want-to-tune">
     Don’t run the one cell below, unless you want to tune!
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-the-dictionary-of-the-best-hyperparameters-that-was-saved-from-our-hyperparameter-tuning-workflow-and-retrieve-the-values">
   Load the dictionary of the best hyperparameters that was saved from our hyperparameter tuning workflow, and retrieve the values
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#define-network-node-shapes-parameters-and-training-data">
     Define network node shapes, parameters, and training data
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#initiate-model-based-on-choice-of-whose-model-ali-or-harrison-and-parameters">
     Initiate model based on choice of whose model (Ali or Harrison) and parameters
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#then-train">
       Then train
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-you-can-scroll-down-to-load-up-trained-model-instead-of-training-now">
   Training: You can scroll down to load up trained model instead of training now
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-mle-model">
     Train MLE model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-non-mle-model">
     Train non-MLE model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#make-sure-the-train-df-has-the-same-ranges-as-the-data-you-want-to-generate-for-evaluation">
       Make sure the train df has the same ranges as the data you want to generate for evaluation
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#make-on-the-fly-generated-evaluation-data">
     Make “on-the-fly” generated evaluation data
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#look-at-an-example-of-a-on-the-fly-generated-evaluation-data">
       Look at an example of a “on-the-fly” generated evaluation data
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-and-evaluate-trained-model-at-generated-data">
   Load and Evaluate Trained model at generated data
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluate-model-at-an-example-set-of-eval-data-points-to-see-the-predicted-p-value-hat-p">
   Evaluate model at an example set of “eval_data” points to see the predicted
   <span class="math notranslate nohighlight">
    \(p\)
   </span>
   -value (
   <span class="math notranslate nohighlight">
    \(\hat{p}\)
   </span>
   )
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#save-trained-model-if-they-re-good-and-if-you-havent-saved-by-now">
     SAVE TRAINED MODEL (if they’re good, and if you havent saved by now)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#this-is-how-you-load-a-trained-model">
     This is how you load a trained model
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#you-could-also-evaluate-the-trained-model-on-the-validation-data">
     You could also evaluate the trained model on the validation data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-to-pivot">
   Learning to Pivot
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aside-a-few-words-on-pytorch-s-autograd-more-on-this-in-https-github-com-alialkadhim-lfi-hep-blob-main-src-two-params-autograd2-ipynb">
     Aside: a few words on pytorch’s autograd (more on this in https://github.com/AliAlkadhim/LFI_HEP/blob/main/src/Two_params/autograd2.ipynb)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#load-the-results-image-of-the-pivotal-model-so-far">
   Load the results image of the pivotal model (so far)
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#explore-the-new-pivotal-models-and-save-if-good">
   Explore the new pivotal models and save if good
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#try-to-automatically-tune-kappa-under-construction">
   Try to automatically tune
   <span class="math notranslate nohighlight">
    \(\kappa\)
   </span>
   (under construction)
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="two-parameter-problem-and-pivotal-likelihood-free-p-values">
<h1>2 - Two-Parameter Problem and Pivotal Likelihood-Free p-Values<a class="headerlink" href="#two-parameter-problem-and-pivotal-likelihood-free-p-values" title="Permalink to this headline">#</a></h1>
<section id="pivotal-likelihood-free-inference-for-count-data-in-particle-physics">
<h2>Pivotal Likelihood-Free Inference for Count Data in Particle Physics<a class="headerlink" href="#pivotal-likelihood-free-inference-for-count-data-in-particle-physics" title="Permalink to this headline">#</a></h2>
<p>Ali Al Kadhim and Harrison B. Prosper <br>
Department of Physics, Florida State University <br></p>
<p>Please make sure to have read the content of <a class="reference internal" href="1_Intro_and_One_Parameter_Problem.html"><span class="doc std std-doc">the previous notebook</span></a> as we will use the concepts and theory used there.</p>
<section id="external-imports">
<h3>External imports<a class="headerlink" href="#external-imports" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span><span class="p">;</span> <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span><span class="p">;</span> <span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">st</span>
<span class="kn">import</span> <span class="nn">torch</span><span class="p">;</span> <span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="c1">#use numba&#39;s just-in-time compiler to speed things up</span>
<span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">njit</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">;</span> <span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mp</span><span class="p">;</span> <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span><span class="p">;</span> 
<span class="c1">#reset matplotlib stle/parameters</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">mpl</span><span class="o">.</span><span class="n">rcParamsDefault</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-deep&#39;</span><span class="p">)</span>
<span class="n">mp</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;agg.path.chunksize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">font_legend</span> <span class="o">=</span> <span class="mi">15</span><span class="p">;</span> <span class="n">font_axes</span><span class="o">=</span><span class="mi">15</span>
<span class="c1"># %matplotlib inline</span>
<span class="kn">import</span> <span class="nn">copy</span><span class="p">;</span> <span class="kn">import</span> <span class="nn">sys</span><span class="p">;</span> <span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">importlib</span> <span class="kn">import</span> <span class="n">import_module</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">optuna</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;optuna is only used for hyperparameter tuning, not critical!&#39;</span><span class="p">)</span>
    <span class="k">pass</span>
<span class="c1"># import sympy as sy</span>
<span class="c1">#sometimes jupyter doesnt initialize MathJax automatically for latex, so do this</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">wid</span><span class="p">;</span> <span class="n">wid</span><span class="o">.</span><span class="n">HTMLMath</span><span class="p">(</span><span class="s1">&#39;$\LaTeX$&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "6ee94ce3dd5e4700a1b4c71228514815", "version_major": 2, "version_minor": 0}
</script></div>
</div>
</section>
<section id="import-utils">
<h3>import utils<a class="headerlink" href="#import-utils" title="Permalink to this headline">#</a></h3>
<p>To be able to load <code class="docutils literal notranslate"><span class="pre">utils</span></code>, please do</p>
<p><code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">setup.sh</span></code></p>
<p><strong>From the same shell that you launch jupyter notebook/jupyterlab/jupyterbook with!</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">LFI_PIVOT_BASE</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;LFI_PIVOT_BASE&#39;</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;BASE directoy properly set = &#39;</span><span class="p">,</span> <span class="n">LFI_PIVOT_BASE</span><span class="p">)</span>
    <span class="n">utils_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">LFI_PIVOT_BASE</span><span class="p">,</span> <span class="s1">&#39;utils&#39;</span><span class="p">)</span>
    <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">utils_dir</span><span class="p">)</span>
    <span class="kn">import</span> <span class="nn">utils</span>
    <span class="c1">#usually its not recommended to import everything from a module, but we know</span>
    <span class="c1">#whats in it so its fine</span>
    <span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;BASE directory not properly set. Read repo README.</span><span class="se">\</span>
<span class="s2">    If you need a function from utils, use the decorator below, or add utils to sys.path&quot;&quot;&quot;</span><span class="p">)</span>
    <span class="k">pass</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/LFI_HEP
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="background-and-problem-statement">
<h1>Background and Problem Statement<a class="headerlink" href="#background-and-problem-statement" title="Permalink to this headline">#</a></h1>
<p>In particle physics, the most important experiment is a counting experiment, represented by a Poisson probability model.Suppose we have a count experiment, we we observe in each bin (or channel) <span class="math notranslate nohighlight">\(k\)</span> a count <span class="math notranslate nohighlight">\(n_k\)</span>. Given <span class="math notranslate nohighlight">\(N\)</span> total channels, the probability of obtaining the observed result is given by the Poisson</p>
<div class="math notranslate nohighlight">
\[ L(\theta,\nu)= \prod_{k=1}^{N} \frac{e^{-(\epsilon_k \sigma + b_k)} (\epsilon_k \sigma + b_k)^{n_k}}{n_k !} \]</div>
<p>Where <span class="math notranslate nohighlight">\(\sigma\)</span>, the cross section (<strong>the parameter of interest</strong>), <span class="math notranslate nohighlight">\(b_k\)</span> is expected background for the <span class="math notranslate nohighlight">\(k\)</span>th channel (in this context, it is <strong>the nuissance parameter</strong>, which is a parameter that is not unkown precisely, typically constrained in a control measurement). <span class="math notranslate nohighlight">\(\epsilon_k\)</span> is the “acceptance parameter”, for the <span class="math notranslate nohighlight">\(k\)</span>th channel, which is typically a product of the detector efficiency, branching fraction, and luminosity.</p>
<p>This is the prototype of many statistical models in astronomy and particle physics in which data are binned and the count in each bin consist a priori of the sum of counts from signal and background. Written in terms of the simplified 2-parameter model, the expected count in each bin <span class="math notranslate nohighlight">\(k\)</span> takes the form</p>
<div class="amsmath math notranslate nohighlight" id="equation-e59fe70a-da2b-4793-900b-585bec8b5911">
<span class="eqno">(8)<a class="headerlink" href="#equation-e59fe70a-da2b-4793-900b-585bec8b5911" title="Permalink to this equation">#</a></span>\[\begin{equation}
    n_{expected}=\theta+\nu
\end{equation}\]</div>
<p>Where <span class="math notranslate nohighlight">\(n_{expected}\)</span> is the expected signal count, <span class="math notranslate nohighlight">\(\theta\)</span> is the unknown mean count, which is the parameter of interest (the cross section), and <span class="math notranslate nohighlight">\(\nu\)</span> is the background unknown mean count, which is the nuissance parameter. In other words,  the total number of evented, <span class="math notranslate nohighlight">\(n_{expected}\)</span>, in a single bin is sampled from a Poisson with mean with mean <span class="math notranslate nohighlight">\(\epsilon \sigma + b\)</span>.</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="the-two-parameter-problem">
<h1>The two-parameter problem<a class="headerlink" href="#the-two-parameter-problem" title="Permalink to this headline">#</a></h1>
<p>Notice that the likelihood also depends on nuissnace parameter <span class="math notranslate nohighlight">\(\nu\)</span>, which we don’t know precisely. Suppose then, that instead of measuring everying all at once, we control our measurement a bit more, such that we divide the space into a “control measurement”, where we are sure that the signal doesn’t occor (e.g. by selecting a region of phase space that we are sure a particular event doesnt happen from physics), and a “signal region”, in which both signal and background can occur. the conditional probability of observing <span class="math notranslate nohighlight">\(n\)</span> signal counts and <span class="math notranslate nohighlight">\(m\)</span> background counts (in a control measurement which is a single Poisson value) in a <em>single channel or bin</em> is given by</p>
<div class="math notranslate nohighlight">
\[
    \text{Prob}(n, m \mid \theta, v)= L(\theta, \nu) =  \underbrace{L(\theta+\nu)}_{\text{controlled measurement, e.g. } L(\mu n_{\text{signal}} + b(\nu)) } =  \underbrace{\frac{e^{-(\theta+v)}(\theta+v)^{n}}{n !}}_{\text{signal region}} \underbrace{\frac{e^{-v} v^{m}}{m !}}_{\text{background region}}, \tag{1}
    \label{prob_model}
\]</div>
<p>where, once the counts <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(m\)</span> have been observed becomes the likelihood <span class="math notranslate nohighlight">\(L(\theta,\nu)\)</span>. <span class="math notranslate nohighlight">\(L(\theta,\nu)\)</span> could be viewed as a product of the main measurement where signal events (<span class="math notranslate nohighlight">\(s\)</span>) could be present, and a control measurement where only background events (<span class="math notranslate nohighlight">\(b\)</span>) exist, which helps us constrain the nuissance parameters.</p>
<p>If you’re familiar with using “strength parameter” <span class="math notranslate nohighlight">\(\mu\)</span>,</p>
<div class="math notranslate nohighlight">
\[\mu \equiv \frac{\mathcal{L} \sigma_{\text{observed}} (\theta)}{\mathcal{L} \sigma_{\text{expected}} (\theta)},\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta\)</span> is the parameter of interest (such as <span class="math notranslate nohighlight">\(m_\text{Higgs}\)</span>), and “expected” means expected by the SM, of course.</p>
<div class="math notranslate nohighlight">
\[L(\theta,\nu) = L_{s+b}(\theta,\nu) \ L_b(\nu)\]</div>
<p>then <span class="math notranslate nohighlight">\(L_{s+b}\)</span> corresponds to having <span class="math notranslate nohighlight">\(\mu=1\)</span> and <span class="math notranslate nohighlight">\(L_b\)</span> corresponds to <span class="math notranslate nohighlight">\(\mu=0\)</span> (backround-only, i.e. control, region).</p>
<p>The likelihood function (the compatibility of a hypothesis given a data set) contains all the information from the experiment that is relevent to inference for the parameters (also known as the <em>likelihood principle</em>). The likelihood function is defined only upto an arbitrary constant, and so only ratios of likelihoods containing different values of the parameters are meaningful.</p>
<p>We have discussed the role of test statistics in confidence intervals was briefly in the <a class="reference internal" href="1_Intro_and_One_Parameter_Problem.html"><span class="doc std std-doc">previous notebook</span></a>, but it is important that we discuss this in more detail. A test statistic <span class="math notranslate nohighlight">\(\lambda\)</span> usually has the form <span class="math notranslate nohighlight">\(\frac{f(X)}{g(x)}\)</span> where <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> are both functions of the data <span class="math notranslate nohighlight">\(X\)</span>. Of course, it is used in hypothesis testing and inference on parameters.</p>
<p>Likelihood ratio tests tests are often used in frequentist statistical analysis to provide both significance tests and confidence intervals for the parameters of interest.</p>
<p>It was proven, by the Neyman-Pearson Lemma that <span class="math notranslate nohighlight">\(\lambda_{NP}\)</span> is the most powerful statistic for hypothesis testing (where the subscript “NP” refers to Neyman-Person” or “New Physics” or “Not Pivotal”, if you like). Suppose that we have two hypotheses <span class="math notranslate nohighlight">\(H_0\)</span> and <span class="math notranslate nohighlight">\(H_1\)</span> parameterized by <span class="math notranslate nohighlight">\(\theta_0\)</span> and <span class="math notranslate nohighlight">\(\theta_1\)</span>, respectively, and observed data <span class="math notranslate nohighlight">\(x\)</span>, the NP test statistic is given by:</p>
<div class="math notranslate nohighlight">
\[
\lambda_{NP} \equiv -2 \log \frac{L(H_0)}{L(H_1)} = -2 \log \frac{L(x \mid \theta_0)}{L(x \mid \theta_1)}.
\]</div>
<p>The test statistic in the Eq above is used due to its well-known behavior of it converging to ti a <span class="math notranslate nohighlight">\(\chi^2_k\)</span> variate with <span class="math notranslate nohighlight">\(k\)</span> degrees of freedom, according to Wilk’s Theorem, where <span class="math notranslate nohighlight">\(k\)</span> is the number of free parameters (parameters of interest after the nuissance parameters have been replaecd by their MLEs).</p>
<hr class="docutils" />
<section id="suppose-we-have-the-following-data-in-the-context-of-our-two-parameter-problem">
<h2>Suppose we have the following data, in the context of our two-parameter problem:<a class="headerlink" href="#suppose-we-have-the-following-data-in-the-context-of-our-two-parameter-problem" title="Permalink to this headline">#</a></h2>
</section>
<section id="observed-data">
<h2>Observed Data:<a class="headerlink" href="#observed-data" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> (observed counts for signal)</p></li>
<li><p><span class="math notranslate nohighlight">\(M\)</span> (observed counts for background)</p></li>
</ul>
</section>
<section id="parameters">
<h2>Parameters:<a class="headerlink" href="#parameters" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta\)</span>: parameter of interest, proportional to <span class="math notranslate nohighlight">\(\sigma\)</span> in HEP (unknown signal mean)</p></li>
<li><p><span class="math notranslate nohighlight">\(\nu\)</span>: nuissance parameter (unknown background mean)</p></li>
</ul>
</section>
<section id="auxiliary-simulated-data-simulated-on-the-fly-for-each-observation">
<h2>Auxiliary (simulated) Data (simulated on-the-fly for each observation):<a class="headerlink" href="#auxiliary-simulated-data-simulated-on-the-fly-for-each-observation" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span>: expected signal count</p></li>
<li><p><span class="math notranslate nohighlight">\(m\)</span>: expected backround count</p></li>
</ul>
<p>The objective now is to derive confidence intervals or limits for the parameter of interest <span class="math notranslate nohighlight">\(\theta\)</span>. What makes this challenging is the presence of the <strong>nuissance parameter</strong> <span class="math notranslate nohighlight">\(\nu\)</span>, which is a parameter whose value is not known precisely, but affects the inference on the parameter of interest <span class="math notranslate nohighlight">\(\theta\)</span>. In HEP <span class="math notranslate nohighlight">\(\nu\)</span> can be related to the physics (theory) or the experimental aparatus/detector. Basu (1977) also discusses the problem nicely: basically, if <span class="math notranslate nohighlight">\(\theta \leftrightarrow P_{\theta}\)</span> i.e. if <span class="math notranslate nohighlight">\(\theta\)</span> maps oone-to-one onto <span class="math notranslate nohighlight">\(P_\theta\)</span> where <span class="math notranslate nohighlight">\(P_{\theta}\)</span> is a probability measure on the space of observation (i.e. <span class="math notranslate nohighlight">\(P_{\theta} = L(x|\theta)\)</span> in modern language), then there is no problem and the inference on <span class="math notranslate nohighlight">\(\theta\)</span> is good. However, in most cases, we have to work with a (total) likelihood that is indexed as <span class="math notranslate nohighlight">\(L(X| \theta \in \Theta, \phi \in \Phi)\)</span> where <span class="math notranslate nohighlight">\(\theta\)</span>, the POI, is a member of parameter space <span class="math notranslate nohighlight">\(\Theta\)</span> and <span class="math notranslate nohighlight">\(\phi\)</span> is an additional unknown parameter in family <span class="math notranslate nohighlight">\(\Phi\)</span>.</p>
<p>In the Bayesian approach, nuissance parameters are assigned prior probabilities <span class="math notranslate nohighlight">\(\pi(\nu) d \nu\)</span> and are integrated out in order to arrive at the posterior for <span class="math notranslate nohighlight">\(\theta\)</span> in a process called marginalization: <span class="math notranslate nohighlight">\(L(\theta) = \int L(\theta,\nu) \pi (\nu) d \nu\)</span>. However, this introduces a fair bit of subjectivity into the problem through the choice of the prior. Frequentist methods deal with nuissance parameters by profiling them the likelihood in order to arrive at their MLEs.</p>
<div class="math notranslate nohighlight">
\[\text{Prob}(N,M|\theta, \nu) = \text{Poiss}(N; \theta+\nu)\text{Poiss}(M;\nu) =L(\theta, \nu)= \frac{e^{-(\theta+\nu)} (\theta+\nu)^N }{N !} \ \frac{e^{-\nu} \nu^M}{M !} \tag{2}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\nu\)</span> is the nuissance parameter, <span class="math notranslate nohighlight">\(\theta\)</span> is the paramter of interest, for which we want to estimate confidence intervals or (upper) limits.</p>
<div class="math notranslate nohighlight">
\[ \lambda_{\text{NP}} \equiv - 2 \log{\frac{p(n,m|\theta, \hat{\nu}(\theta) )}{ p(n,m|\hat{\theta}, \hat{\nu}(\theta) )}} = -2 \log \frac{L_{\text{prof}} \big(\theta, \hat{\nu}(\theta) \big) }{L_{\text{prof}} \big( \hat{\theta}, \hat{\nu}(\theta) \big)}, \tag{3}\]</div>
<p>where <span class="math notranslate nohighlight">\(L_{\text{prof}} \big( n, m, \theta, \hat{\nu}(\theta) \big)\)</span> is the profiled likelihood - that is - the likelihood function when the nuissance parameters are replaced by their maximum likelihood estimates (MLE) for a given value of the parameter of interest. We have kept the dependece of all the parameters for now in the likelihood for clarity.</p>
<p>One goal is to model the distribution of <span class="math notranslate nohighlight">\(\lambda\)</span>, and when <span class="math notranslate nohighlight">\(\hat{\theta}=\hat{\theta}_{\text{MLE}}\)</span>, i.e.</p>
<div class="math notranslate nohighlight">
\[\lambda^{\text{(MLE)}}_{\text{NP}} \equiv -2 \log \frac{L_{\text{prof}} \big(\theta \big) }{L_{\text{prof}} \big( \hat{\theta}_{\text{MLE}} \big)} \tag{3},\]</div>
<p>where we have dropped the dependence on <span class="math notranslate nohighlight">\(\hat{\nu}\)</span> in Eq (3) since the likelihood does not depend on it anymore. We have a nice functional form for <span class="math notranslate nohighlight">\(\lambda\)</span>, where The MLE of <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\hat{\theta}^{\text{(MLE)}}=n-m.\]</div>
<p>Low-count data can sometimes yield spurious results, where the MLE of a parameter of interest <span class="math notranslate nohighlight">\(\theta\)</span>, could yield a negative result. In the case that <span class="math notranslate nohighlight">\(\theta\)</span> is the cross section, yielding a negative result is non-physical, which leads to the ad-hoc fix: taking ignoring the MLE solution and taking <span class="math notranslate nohighlight">\(\hat{\theta}=0\)</span> when <span class="math notranslate nohighlight">\(n&lt;m\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{\theta}^{\text{(non-MLE)}} =\left\{
\begin{array}{ll}
    n-m &amp; \quad  n&gt;m \\
    0 &amp; \quad n \le m
\end{array}
\right.
\end{split}\]</div>
<p>The MLE <span class="math notranslate nohighlight">\(\hat{\nu}(\theta)\)</span> is attained by solving <span class="math notranslate nohighlight">\(\frac{\partial \log{p(n,m|\theta,\nu)}}{ \partial \nu} =0\)</span>, leading to</p>
<div class="math notranslate nohighlight">
\[\log{p(n,m|\theta,\nu)} = -(\theta+\nu)+n\log{(\theta+\nu)|-\nu+m\log{\nu}} + \text{constants}\]</div>
<div class="math notranslate nohighlight">
\[\hat{\nu}(\theta)=\left(g+\sqrt{g^2 + 8 m \theta} \right)/4,\]</div>
<p>where <span class="math notranslate nohighlight">\(g \equiv n+m-2 \theta\)</span>.</p>
<hr class="docutils" />
<p>In dealing with statistical inference it is important to deal with nuissance parameters (i.e. systematics, e.g. jet energy scale, or any other parameters dealing with experiment or theory that are not known exactly) because we want to isolate to what extent your result is sensetive to a particular nuissance parameter (as opposed to, say, statistical fluctuations).</p>
<p>Bayesian and Frequentist methods alike treat nuissance parameters in a similary way: they <strong>try to</strong> partition the likelihood into components representing information on the parameter of interest, and into components representing parameters of interest.</p>
<p>Broadly speaking, The standard procedure for removal of nuissance parameters, as discussed above is Bayesian marginalization, i.e. integrate out the dependence of the nuissance parameter in the posterior to arrive at the posterior marginal distribution of the parameter of interest:</p>
<div class="math notranslate nohighlight">
\[ L(\theta) = P(x|\theta) = \int P(x \mid \theta, \nu) \pi (\nu) d \nu\]</div>
<p>where the uncertainty in the nuissance parameter is characterized by a prior PDF <span class="math notranslate nohighlight">\(\pi(\nu)\)</span> (and <span class="math notranslate nohighlight">\(p(x|\theta)\)</span> is sometimes called a marginal model of the data and POI, which is a 1-D posterior for he POI)</p>
<p>or Frequentist profiling,</p>
<div class="math notranslate nohighlight">
\[L_{\text{profile}}(\theta) = L(\theta, \hat{\hat{\nu}}(\theta) ),  \]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\hat{\nu}}\)</span> means the profiled values (values that maximize <span class="math notranslate nohighlight">\(L\)</span>) for a specified <span class="math notranslate nohighlight">\(\theta\)</span> (in our case, <span class="math notranslate nohighlight">\(\hat{\hat{\nu}}(\theta)=\hat{\nu}(\theta)\)</span>).</p>
<p>This means that in order to derive <strong>confidence intervals</strong>, we can either:</p>
<ol class="simple">
<li><p>Follow the Neyman construction (as was done in the <a class="reference external" href="#notebook_1">first notebook</a> )</p></li>
</ol>
<blockquote>
<div><p>Neyman has shown a method for calculation of confidence intervals that guarantees coverage via a method usually referred to as “Neyman Construction” (which was discussed in <a class="reference internal" href="1_Intro_and_One_Parameter_Problem.html"><span class="doc std std-doc">the first notebook</span></a>. One problem with Neyman construction, however, is that with multi-parameter problems, it is very difficult to construct intervals that guarantee coverage (that is, they have coverage of at least <span class="math notranslate nohighlight">\(1-\alpha\)</span>). The reason for this difficulty is that one has to repeat this construction for each point in the space of nuissance parameters, by treating the nuissance parameter as fixed, and constructing the interval for the POI.</p>
</div></blockquote>
<ol class="simple">
<li><p>Construct Bayesian (“credible”) intervals <span class="math notranslate nohighlight">\([\theta^{low}, \theta^{up}]\)</span> by solving for <span class="math notranslate nohighlight">\(\theta^{low}, \theta^{up}\)</span> in the equation</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ CL = \int_{\theta^{low} }^{\theta^{up}} p(\vec{x} | \theta) d \theta \tag{2} \]</div>
<blockquote>
<div><p>As discussed in the first notebook, there is a big instinsic problem in the choice this prior (both for subjective priors and non-subjective, e.g. “reference priors”). Furthermore, it is a big computational and technical challenge to solve the integral, which is often calculated by MC, but becomes exponentially harder to compute as the number of nuissance parameters increases.</p>
</div></blockquote>
<ol class="simple">
<li><p>Using the likelihood ratio (<span class="math notranslate nohighlight">\(\lambda_{\text{NP}}\)</span>).</p></li>
</ol>
<blockquote>
<div><p>We follow this procedure, discussed at length in this work, in constructing</p>
</div></blockquote>
<p>Each of these methods has many problems, discussed throughout this jupyterbook, and they all don’t guarantee coverage for multi-parameter problems with nuissance parameters. Guaranteeing coverage for such problems for POIs and nuissance parameters remains a topic of statistical research.</p>
<hr class="docutils" />
<p>In this study we adopt LFI with robust frequentest critical value estimation. We generate data comprising the quadruplets <span class="math notranslate nohighlight">\(\{\theta_i, \nu_i, N_i, M_i, Z_i \}\)</span> where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\theta &amp; \sim \textrm{uniform}(0, 20), \\
\nu &amp; \sim \textrm{uniform}(0, 20), \\
\end{align}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\left\{
\begin{align}
n &amp; \sim \textrm{poisson}(\theta + \nu),\\
m &amp; \sim \textrm{poisson}(\nu),\\
\end{align}
\right\} \rightarrow \lambda_\text{gen}(n, m \mid \theta,\nu)
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\left\{
\begin{align}
N &amp; \sim \textrm{uniform}(0,10),\\
M &amp; \sim \textrm{uniform}(0, 10), \\
\end{align}
\right\} \rightarrow \lambda_D(N, M \mid \theta,\nu)
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
Z  = \mathbb{I}
\left[ \lambda_\text{gen}(n, m \mid \theta,\nu) \leq \lambda_D(N, M \mid \theta,\nu) \right],
\]</div>
<p>where the size of each of these samples is <span class="math notranslate nohighlight">\(B'\)</span>, <span class="math notranslate nohighlight">\(\mathbb{I}\)</span> is the indicator function, and <span class="math notranslate nohighlight">\(\lambda_D (D, \theta) = \lambda_{NP}(N, M, \theta)\)</span> is our chosen test statistic</p>
<div class="math notranslate nohighlight">
\[ \lambda \equiv - 2 \log{\frac{p(n,m|\theta, \hat{\nu}(\theta) )}{ p(n,m|\hat{\theta}, \hat{\nu}(\theta) )}} = -2 \log \frac{L_{\text{prof}} \big(\theta \big) }{L_{\text{prof}} \big( \hat{\theta} \big)}, \tag{4}\]</div>
<p>where <span class="math notranslate nohighlight">\(L_{\text{prof}} \big( n, m, \theta, \hat{\nu}(\theta) \big)\)</span> is the profiled likelihood - that is - the likelihood function when the nuissance parameters are replaced by their maximum likelihood estimates (MLE) for a given value of the parameter of interest.</p>
<div class="math notranslate nohighlight">
\[
L_{\text{prof}}(\theta) \equiv \frac{e^{-(\theta+\hat{\nu})} (\theta+\hat{\nu})^N }{N !} \ \frac{e^{-\hat{\nu}} \hat{\nu}^M}{M !}.
\]</div>
<section id="ann-lee-s-algorithm">
<h3><a class="reference external" href="https://arxiv.org/pdf/2107.03920.pdf">Ann Lee</a>’s Algorithm<a class="headerlink" href="#ann-lee-s-algorithm" title="Permalink to this headline">#</a></h3>
<p>Before proceeding, let us recall Ann Lee et al.’s brilliant algorithm for estimating p-values with robust frequentist guarantees.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">algorithm2</span> <span class="o">=</span> <span class="n">Image</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">LFI_PIVOT_BASE</span><span class="p">,</span> <span class="s1">&#39;images&#39;</span><span class="p">,</span> <span class="s1">&#39;Algorithm2.jpg&#39;</span><span class="p">));</span> <span class="n">display</span><span class="p">(</span><span class="n">algorithm2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_13_0.jpg" src="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_13_0.jpg" />
</div>
</div>
<p>The procedure can be summarized by the following algorithm:</p>
</section>
<section id="our-algorithm-for-finding-p-value-with-lfi">
<h3>Our Algorithm for Finding p-value with LFI<a class="headerlink" href="#our-algorithm-for-finding-p-value-with-lfi" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">our_algorithm_1</span> <span class="o">=</span> <span class="n">Image</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">LFI_PIVOT_BASE</span><span class="p">,</span><span class="s1">&#39;images&#39;</span><span class="p">,</span><span class="s1">&#39;NMLambda_algorithm.png&#39;</span><span class="p">));</span> <span class="n">display</span><span class="p">(</span><span class="n">our_algorithm_1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_15_0.png" src="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_15_0.png" />
</div>
</div>
<p>Clearly, our choice of using the test statistic above is motivated by the <a class="reference external" href="https://royalsocietypublishing.org/doi/epdf/10.1098/rsta.1933.0009">Neyman-Pearson Lemma</a> and Wilk’s theorem (and the fact that everyone in particle physics is familiar with this test statistic) Instead of using different statistics for LFI as in the <a class="reference external" href="https://arxiv.org/pdf/2107.03920.pdf">Ann Lee paper</a>, we use this one since in Ann lee’s paper, for example the ACORE statistic <span class="math notranslate nohighlight">\(\Lambda\left(\mathcal{D} ; \Theta_{0}\right):=\log \frac{\sup _{\theta_{0} \in \Theta_{0}} \prod_{i=1}^{n} \mathbb{O}\left(\mathbf{X}_{i}^{\text {obs }} ; \theta_{0}\right)}{\sup _{\theta \in \Theta} \prod_{i=1}^{n} \mathbb{O}\left(\mathbf{X}_{i}^{\text {obs }} ; \theta\right)}\)</span>, since the odds <span class="math notranslate nohighlight">\(\mathbb{O}\left(\mathbf{X}_{i}^{\text {obs }}\right) \)</span>, are merely estimates of the likelihood, and not the likelihood itselt, therefore Wilk’s theorem is not guaranteed to work, even if we have a large sample size.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@njit</span>
<span class="k">def</span> <span class="nf">DR</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sp</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">gammainc</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>

<span class="nd">@njit</span>
<span class="k">def</span> <span class="nf">DL</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">sp</span><span class="o">.</span><span class="n">special</span><span class="o">.</span><span class="n">gammainc</span><span class="p">(</span><span class="n">s</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">L_prof</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">,</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">k</span><span class="o">=</span><span class="mi">1</span>
    <span class="n">k1</span> <span class="o">=</span> <span class="n">k</span><span class="o">+</span><span class="mi">1</span>
    <span class="n">k2</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">/</span><span class="n">k1</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">n</span><span class="o">+</span><span class="n">m</span> <span class="o">-</span> <span class="n">k1</span><span class="o">*</span><span class="n">theta</span>
    <span class="n">nu_hat</span> <span class="o">=</span> <span class="n">k2</span><span class="o">*</span> <span class="p">(</span><span class="n">g</span><span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">g</span><span class="o">*</span><span class="n">g</span> <span class="o">+</span><span class="mi">4</span><span class="o">*</span><span class="n">k1</span><span class="o">*</span><span class="n">m</span><span class="o">*</span><span class="n">theta</span><span class="p">))</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mu</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">+</span> <span class="n">nu_hat</span><span class="p">)</span>
    <span class="n">p2</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">mu</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">nu_hat</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">p1</span><span class="o">*</span><span class="n">p2</span>


<span class="k">def</span> <span class="nf">theta_hat</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">theta_hat</span> <span class="o">=</span> <span class="n">n</span><span class="o">-</span><span class="n">m</span>
    
    <span class="k">if</span> <span class="ow">not</span> <span class="n">MLE</span><span class="p">:</span>
        <span class="n">theta_hat</span> <span class="o">=</span> <span class="n">theta_hat</span> <span class="o">*</span> <span class="p">(</span><span class="n">theta_hat</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">theta_hat</span>

<span class="c1"># @njit</span>
<span class="k">def</span> <span class="nf">lambda_test</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">Ln</span> <span class="o">=</span> <span class="n">L_prof</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">,</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">Ld</span> <span class="o">=</span> <span class="n">L_prof</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">,</span> <span class="n">MLE</span><span class="p">))</span>
    <span class="n">lambda_</span>  <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Ln</span><span class="o">/</span><span class="n">Ld</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lambda_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Prior to reporting the results of our method, we validate that our method by comparing it to the well-known results of Wilk’s Theorem. That is, the test statistic <span class="math notranslate nohighlight">\(\lambda(\theta, n, m , \nu(\theta))\)</span> should be distributed as a <span class="math notranslate nohighlight">\(\chi^2_1\)</span> (a <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution with a number of degrees of freedom equal to the number of free parameters left in our problem).</p>
</section>
<section id="we-ll-discuss-this-block-of-code-after-the-plot">
<h3>we’ll discuss this block of code after the plot<a class="headerlink" href="#we-ll-discuss-this-block-of-code-after-the-plot" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chi2_exp_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e5</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">run_sim</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">MLE</span><span class="p">,</span> <span class="n">lambda_size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sample n ~ Pois(theta+nu), </span>
<span class="sd">              m ~ Pois(nu), </span>
<span class="sd">    and compute </span>
<span class="sd">              lambda(theta, n, m)</span>
<span class="sd">              </span>
<span class="sd">    return: (n, m, lambda_), where each are np arrays of length lambda_size</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">theta</span><span class="o">+</span><span class="n">nu</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">lambda_size</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nu</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">lambda_size</span><span class="p">)</span>
    <span class="n">lambda_</span> <span class="o">=</span> <span class="n">lambda_test</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="n">MLE</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">run_sims</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">MLE</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Run an entire simulation, that is, generate n and m from </span>
<span class="sd">    run_sim above, and calculate lambda, for</span>
<span class="sd">    </span>
<span class="sd">    input: a tuple of (theta, nu) scalars</span>
<span class="sd">    </span>
<span class="sd">    Reurns:df, lambda_results</span>
<span class="sd">    </span>
<span class="sd">    where lambda_results is a list of tuples </span>
<span class="sd">        (n, m, lambda_, theta, nu)</span>
<span class="sd">    and df is just a dataframe of [n,m,lambda,theta,nu]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lambda_results</span><span class="o">=</span><span class="p">[]</span>
    <span class="n">df</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">points</span><span class="p">:</span>
        <span class="n">theta</span><span class="p">,</span> <span class="n">nu</span> <span class="o">=</span> <span class="n">p</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">theta</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;nu&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">nu</span>
        <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">lambda_</span> <span class="o">=</span> <span class="n">run_sim</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">MLE</span><span class="p">,</span> <span class="n">lambda_size</span> <span class="o">=</span><span class="n">chi2_exp_size</span><span class="p">)</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;n&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">n</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;m&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">m</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;lambda&#39;</span><span class="p">]</span><span class="o">=</span><span class="n">lambda_</span>
        <span class="n">lambda_results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">nu</span><span class="p">))</span>
    
        <span class="nb">print</span><span class="p">(</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> </span><span class="se">\n</span><span class="s1"> (theta, nu) =  (%.f, %.f) </span><span class="se">\n</span><span class="s1"> &#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">nu</span><span class="p">)</span> <span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1"> </span><span class="se">\t</span><span class="s1"> with associated n =  </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">, </span><span class="se">\n</span><span class="s1"> </span><span class="se">\n</span><span class="s1"> </span><span class="se">\t</span><span class="s1"> </span><span class="se">\t</span><span class="s1"> m = </span><span class="si">{</span><span class="n">m</span><span class="si">}</span><span class="s1">, </span><span class="se">\n</span><span class="s1"> </span><span class="se">\n</span><span class="s1"> </span><span class="se">\t</span><span class="s1"> </span><span class="se">\t</span><span class="s1"> lambda = </span><span class="si">{</span><span class="n">lambda_</span><span class="si">}</span><span class="s1">&#39;</span>  <span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span><span class="p">,</span> <span class="n">lambda_results</span>

<span class="k">def</span> <span class="nf">plot_one</span><span class="p">(</span><span class="n">lambda_</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">ax</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Histogram the CDF of  lambda_t = -2log(Lp(theta)/Lp(theta_hat)), </span>
<span class="sd">    for a given (fixed) theta and nu.</span>
<span class="sd">    Also, plot the actual CDF of a chi^2 distribution with 1 free parameter </span>
<span class="sd">    (since only theta is left after we profile nu) &quot;&quot;&quot;</span>
    <span class="n">ftsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">;</span> <span class="n">xmin</span><span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">xmax</span><span class="o">=</span> <span class="mi">10</span>
    <span class="n">ymin</span><span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">ymax</span><span class="o">=</span> <span class="mi">1</span>
    <span class="n">x_range</span> <span class="o">=</span> <span class="p">(</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">)</span>
    <span class="n">y_range</span> <span class="o">=</span> <span class="p">(</span><span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">x_range</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">y_range</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\lambda \left(\theta,\hat{\nu}(\theta) \mid n, m \right)$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="n">ftsize</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;cdf$(\lambda)$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">ftsize</span><span class="p">)</span>
    <span class="c1">##########HISTOGRAM CDF OF LAMBDA####################</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">lambda_</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">5</span><span class="o">*</span><span class="n">xmax</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="n">x_range</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.9</span><span class="p">),</span>
    <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cumulative</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">histtype</span><span class="o">=</span><span class="s1">&#39;stepfilled&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;CDF$(\lambda)$&#39;</span><span class="p">)</span>
    <span class="c1">############################################################</span>
    <span class="c1">########### HISTOGRAM CDF OF THE CHI2 OF OF X WITH 1 DOF</span>
    <span class="c1">#x is not theta, that&#39;s the whole point of Wilks thm, x is an arbitrary RV</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">xmax</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">chi2</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;CDF$(\chi^2_1)$&#39;</span><span class="p">)</span>
    <span class="c1"># annotate</span>
    <span class="n">xwid</span> <span class="o">=</span> <span class="p">(</span><span class="n">xmax</span><span class="o">-</span><span class="n">xmin</span><span class="p">)</span><span class="o">/</span><span class="mi">12</span>
    <span class="n">ywid</span> <span class="o">=</span> <span class="p">(</span><span class="n">ymax</span><span class="o">-</span><span class="n">ymin</span><span class="p">)</span><span class="o">/</span><span class="mi">12</span>
    <span class="n">xpos</span> <span class="o">=</span> <span class="n">xmin</span> <span class="o">+</span> <span class="n">xwid</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">ypos</span> <span class="o">=</span> <span class="n">ymin</span> <span class="o">+</span> <span class="n">ywid</span><span class="o">*</span><span class="mi">2</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">xpos</span><span class="p">,</span> <span class="n">ypos</span><span class="p">,</span>
    <span class="sa">r</span><span class="s1">&#39;$ \theta = </span><span class="si">%d</span><span class="s1">, \nu = </span><span class="si">%d</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">nu</span><span class="p">),</span>
    <span class="n">fontsize</span><span class="o">=</span><span class="n">ftsize</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We test this theorem with our algorithm, but stopping at step 9. We then histogram the comulative distribution function (CDF) of <span class="math notranslate nohighlight">\(\lambda(\theta)\)</span> for a given (fixed) <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(\nu\)</span>, and compare it to the analystical CDF of a <span class="math notranslate nohighlight">\(\chi^2_1\)</span> distribtion. The figure (below) shows that the results of our test statistic does indeed agree with what we expect from Wilk’s Theorem. I.e. we follow these simple steps:</p>
<ol class="simple">
<li><p><strong>Generate one scalar <span class="math notranslate nohighlight">\(\theta\)</span> and one scalar <span class="math notranslate nohighlight">\(\nu\)</span></strong></p></li>
<li><p><strong>Generate <span class="math notranslate nohighlight">\(\lambda(\theta,\nu)\)</span> of size <span class="math notranslate nohighlight">\(B'\)</span>.</strong></p></li>
<li><p><strong>Observe that in the case MLE=True, and where the counts <span class="math notranslate nohighlight">\(N,M\)</span> are not too small, the distribution of this <span class="math notranslate nohighlight">\(\lambda\)</span> will assymptotiaclly (i.e. when the sample size <span class="math notranslate nohighlight">\(B'\)</span> is large) approach the CDF of a <span class="math notranslate nohighlight">\(\chi^2_{dof}(x)\)</span> of an RV of one dof (since <span class="math notranslate nohighlight">\(\theta\)</span> is the only free parameter left), confirming Wilk’s theorem.</strong></p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#points=(theta,nu)</span>
<span class="n">points_1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">points_2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">points_3</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">MLE</span><span class="o">=</span><span class="kc">True</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="p">,</span> <span class="n">lambda_1</span> <span class="o">=</span> <span class="n">run_sim</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">points_1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">nu</span><span class="o">=</span><span class="n">points_1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">MLE</span><span class="o">=</span><span class="n">MLE</span><span class="p">,</span> <span class="n">lambda_size</span><span class="o">=</span><span class="n">chi2_exp_size</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="p">,</span> <span class="n">lambda_2</span> <span class="o">=</span> <span class="n">run_sim</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">points_2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">nu</span><span class="o">=</span><span class="n">points_2</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">MLE</span><span class="o">=</span><span class="n">MLE</span><span class="p">,</span> <span class="n">lambda_size</span><span class="o">=</span><span class="n">chi2_exp_size</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="p">,</span> <span class="n">lambda_3</span> <span class="o">=</span> <span class="n">run_sim</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">points_3</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">nu</span><span class="o">=</span><span class="n">points_3</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">MLE</span><span class="o">=</span><span class="n">MLE</span><span class="p">,</span> <span class="n">lambda_size</span><span class="o">=</span><span class="n">chi2_exp_size</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plot_one</span><span class="p">(</span><span class="n">lambda_1</span><span class="p">,</span> <span class="n">points_1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">points_1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plot_one</span><span class="p">(</span><span class="n">lambda_2</span><span class="p">,</span> <span class="n">points_2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">points_2</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plot_one</span><span class="p">(</span><span class="n">lambda_3</span><span class="p">,</span> <span class="n">points_3</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">points_3</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\lambda \rightarrow \chi^2_1$; MLE=</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">MLE</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">);</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_21_0.png" src="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_21_0.png" />
</div>
</div>
<p>We see from above that the statistic <span class="math notranslate nohighlight">\(\lambda\)</span> is an approximate sampling distribution of the <span class="math notranslate nohighlight">\(\chi^2\)</span> PDF, proving <a class="reference external" href="https://www.jstor.org/stable/2957648?seq=1#metadata_info_tab_contents">Wilk’s theorem</a>, and hence, asymptotically, the <span class="math notranslate nohighlight">\(p\)</span>-value could also be caluclated by</p>
<div class="math notranslate nohighlight">
\[p^{\text{MLE}}_\theta (\nu) \rightarrow \int_{\chi^2}^\infty f(z; k) dz\]</div>
<p>where <span class="math notranslate nohighlight">\(k\)</span> is the number of degrees of freedom (free parameters) which is 1 in our 2-parameter problem , and <span class="math notranslate nohighlight">\(f(z;k)\)</span> is the <span class="math notranslate nohighlight">\(\chi^2\)</span> PDF. This can be checked directly since it has a functinoal form, and since it approaches a <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution, it has a functional form that can be directly computed:</p>
<div class="math notranslate nohighlight">
\[
p^{\text{MLE}}_\theta (\nu) \rightarrow 1-\alpha \left( 1- F_{\chi^2} [\theta, \nu] \right),
\]</div>
<p>where <span class="math notranslate nohighlight">\( F_{\chi^2}\)</span> is the comulative <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution function (as a function of <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(\nu\)</span>).</p>
<p>Let us now repeat the exercise for the non-MLE case.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#points=(theta,nu)</span>
<span class="n">points_1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">points_2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">points_3</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">MLE</span><span class="o">=</span><span class="kc">False</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="p">,</span> <span class="n">lambda_1</span> <span class="o">=</span> <span class="n">run_sim</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">points_1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">nu</span><span class="o">=</span><span class="n">points_1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">MLE</span><span class="o">=</span><span class="n">MLE</span><span class="p">,</span> <span class="n">lambda_size</span><span class="o">=</span><span class="n">chi2_exp_size</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="p">,</span> <span class="n">lambda_2</span> <span class="o">=</span> <span class="n">run_sim</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">points_2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">nu</span><span class="o">=</span><span class="n">points_2</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">MLE</span><span class="o">=</span><span class="n">MLE</span><span class="p">,</span> <span class="n">lambda_size</span><span class="o">=</span><span class="n">chi2_exp_size</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="p">,</span> <span class="n">lambda_3</span> <span class="o">=</span> <span class="n">run_sim</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">points_3</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">nu</span><span class="o">=</span><span class="n">points_3</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">MLE</span><span class="o">=</span><span class="n">MLE</span><span class="p">,</span> <span class="n">lambda_size</span><span class="o">=</span><span class="n">chi2_exp_size</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span><span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plot_one</span><span class="p">(</span><span class="n">lambda_1</span><span class="p">,</span> <span class="n">points_1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">points_1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plot_one</span><span class="p">(</span><span class="n">lambda_2</span><span class="p">,</span> <span class="n">points_2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">points_2</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plot_one</span><span class="p">(</span><span class="n">lambda_3</span><span class="p">,</span> <span class="n">points_3</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">points_3</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\lambda \rightarrow \chi^2_1$;  MLE=False&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">25</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_23_0.png" src="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_23_0.png" />
</div>
</div>
<p>Clearly it does not approach a <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution, and hence Wilk’s theorem does not hold, making the calculation of p-values extra difficult.</p>
</section>
</section>
<hr class="docutils" />
<section id="lfi-to-calculate-p-values">
<h2>LFI to Calculate p-values<a class="headerlink" href="#lfi-to-calculate-p-values" title="Permalink to this headline">#</a></h2>
<p>As shall be discussed later in this notebook, one way of calculating <span class="math notranslate nohighlight">\(p\)</span>-value for <span class="math notranslate nohighlight">\(\theta\)</span> corresponding to an observed <span class="math notranslate nohighlight">\(\{N,M\}\)</span> pair (which will still be dependent on <span class="math notranslate nohighlight">\(\nu\)</span>) is by calculating the test statistic at the <span class="math notranslate nohighlight">\(\{N,M\}\)</span> pair and calculating
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-753337ab-5ae1-4325-af10-ab8ee8c56efb">
<span class="eqno">(9)<a class="headerlink" href="#equation-753337ab-5ae1-4325-af10-ab8ee8c56efb" title="Permalink to this equation">#</a></span>\[\begin{align}
p_\theta (\nu) &amp;=\int_{\lambda_D}^\infty f \big(\lambda_{gen}(n,m \mid \theta,\nu) \mid H_{0} \big) \ d \lambda_{gen} \\
 &amp;= 1- \text{Prob} \big(\lambda_{gen}(n,m;\theta,\nu) \le \lambda_D(N,M;\theta,\nu) \big), \tag{2}
\end{align}\]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}where $f$ is the PDF of $\lambda$. Note that Eq 2 is true regardless of whether we use MLE=True or MLE=False (i.e. $\hat{\theta}=\hat{\theta}_{MLE}$ or alternatively $\hat{\theta}=\hat{\theta}_{non-MLE}$). \\
It is very interesting that the distribution of $\chi^2$ will only depend on $\theta$ and has no dependence on $\nu$, which makes it pivotal (although an individual $\chi^2$ will depend on both $\theta$ and $\nu$). We will explore this concept more in the next notebook.\\We will be esitmaing this $p$-value with our LFI framework. If $\lambda=\lambda_{MLE}$, then according to Eq (2) the p-value is given by:\end{aligned}\end{align} \]</div>
<p>p^{\text{MLE}}<em>\theta (\nu) = 1- \text{Prob} \left( - 2 \log \frac{L</em>{\text{prof}} \big(n, m, \theta, \hat{\nu}(\theta) \big) }{L_{\text{prof}} \big( n, m, \hat{\theta}_{\text{MLE}}, \hat{\nu}(\theta) \big)} \le</p>
<ul class="simple">
<li><p>2 \log \frac{L_{\text{prof}} \big(N, M, \theta, \hat{\nu}(\theta) \big) }{L_{\text{prof}} \big( N, M, \hat{\theta}_{\text{MLE}}, \hat{\nu}(\theta) \big)}  \right),
$$</p></li>
</ul>
<p>Where we have kept all of the parameters explicitly for clarity.</p>
<p>where <span class="math notranslate nohighlight">\( F_{\chi^2}\)</span> is the comulative <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution function. <span class="math notranslate nohighlight">\(p^{\text{MLE}}_\theta (\nu) \)</span> is approximated in two ways: first by histogramming the quantity (2) (we call this <span class="math notranslate nohighlight">\(\mathbf{h}\)</span>), and second by plotting the <span class="math notranslate nohighlight">\(\chi^2\)</span> CDF directly for comparison.</p>
<p>The rest of the work aims at estimating the quantity <span class="math notranslate nohighlight">\(p^{\text{MLE}}_\theta (\nu)\)</span> using our LFI framework, where the estimate is denoted by <span class="math notranslate nohighlight">\(\mathbf{f}\)</span>. After validating that our procedure works for the MLE case, we proceed to estimate <span class="math notranslate nohighlight">\(p^{\text{MLE}}_\theta (\nu) \)</span> for non-MLE case, and provide a procedure for which it can be calculate independently of nuissance parameter <span class="math notranslate nohighlight">\(\nu\)</span>.</p>
<section id="generate-6-pairs-tuples-of-theta-nu-values">
<h3>Generate 6 pairs (tuples) of <span class="math notranslate nohighlight">\((\theta, \nu)\)</span> values<a class="headerlink" href="#generate-6-pairs-tuples-of-theta-nu-values" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">points</span> <span class="o">=</span> <span class="p">[(</span><span class="n">theta</span><span class="p">,</span> <span class="n">nu</span><span class="p">)</span> <span class="k">for</span> <span class="n">theta</span><span class="p">,</span> <span class="n">nu</span> <span class="ow">in</span> 
          <span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">high</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span> 
              <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">high</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">))]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">,</span> <span class="n">results</span> <span class="o">=</span> <span class="n">run_sims</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 
 (theta, nu) =  (2, 3) 
 
	 	 with associated n =  [7 4 3 ... 2 7 2], 
 
 	 	 m = [6 3 3 ... 2 2 4], 
 
 	 	 lambda = [0.07640065 0.1396362  0.63419698 ... 0.90394862 1.07969347 2.51359335]

 
 (theta, nu) =  (1, 0) 
 
	 	 with associated n =  [4 1 1 ... 1 1 2], 
 
 	 	 m = [0 0 0 ... 0 0 0], 
 
 	 	 lambda = [ 3.54517744 -0.         -0.         ... -0.         -0.
  0.77258872]

 
 (theta, nu) =  (1, 0) 
 
	 	 with associated n =  [2 2 1 ... 0 0 3], 
 
 	 	 m = [0 0 0 ... 0 0 0], 
 
 	 	 lambda = [ 0.77258872  0.77258872 -0.         ...  2.          2.
  2.15888308]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>theta</th>
      <th>nu</th>
      <th>n</th>
      <th>m</th>
      <th>lambda</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0.772589</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0.772589</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>-0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">lambda_</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">nu</span> <span class="o">=</span><span class="n">results</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;results[1] =  (n1, m1, lambda1, theta1, nu1) =  (  </span><span class="se">\n</span><span class="s1"> </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s1">, </span><span class="se">\n</span><span class="s1"> </span><span class="si">{</span><span class="n">m</span><span class="si">}</span><span class="s1">, </span><span class="se">\n</span><span class="s1"> </span><span class="si">{</span><span class="n">lambda_</span><span class="si">}</span><span class="s1">, </span><span class="se">\n</span><span class="s1"> </span><span class="si">{</span><span class="n">theta</span><span class="si">}</span><span class="s1">, </span><span class="se">\n</span><span class="s1"> </span><span class="si">{</span><span class="n">nu</span><span class="si">}</span><span class="s1">)&#39;</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;&#39;&#39;</span><span class="se">\n</span><span class="s1"> (n.shape, m.shape, lambda_.shape, theta.size, nu.size) = </span>
<span class="s1">      (</span><span class="si">{</span><span class="n">n</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, </span><span class="se">\t</span><span class="s1"> </span><span class="si">{</span><span class="n">m</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, </span><span class="se">\t</span><span class="s1"> </span><span class="si">{</span><span class="n">lambda_</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">, </span><span class="se">\t</span><span class="s1"> </span><span class="si">{</span><span class="n">theta</span><span class="o">.</span><span class="n">size</span><span class="si">}</span><span class="s1">, </span><span class="se">\t</span><span class="s1"> </span><span class="si">{</span><span class="n">nu</span><span class="o">.</span><span class="n">size</span><span class="si">}</span><span class="s1">)&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>results[1] =  (n1, m1, lambda1, theta1, nu1) =  (  
 [4 1 1 ... 1 1 2], 
 [0 0 0 ... 0 0 0], 
 [ 3.54517744 -0.         -0.         ... -0.         -0.
  0.77258872], 
 1, 
 0)

 (n.shape, m.shape, lambda_.shape, theta.size, nu.size) = 
      ((100000,), 	 (100000,), 	 (100000,), 	 1, 	 1)
</pre></div>
</div>
</div>
</div>
</section>
<section id="generate-training-data-or-take-a-look-at-the-saved-training-data">
<h3>Generate Training data (or take a look at the saved training data)<a class="headerlink" href="#generate-training-data-or-take-a-look-at-the-saved-training-data" title="Permalink to this headline">#</a></h3>
<p>We then generate training data where the number of training examples is <span class="math notranslate nohighlight">\(B'\)</span> according to Alg. 2 of Anne Lee et al. (shown below). The training data now has <span class="math notranslate nohighlight">\(\{\theta, \nu, N, M \} \)</span> as training features and <span class="math notranslate nohighlight">\(Z\)</span> as the target. We then use Pytorch to build MLP regression model with average quadratic loss to estimate the distribution of <span class="math notranslate nohighlight">\(Z\)</span>, <span class="math notranslate nohighlight">\(E[Z|\theta,\nu]\)</span>, which according to Alg. 2 is the p-value.</p>
<p>As we know, the p-value is the probability under the null hypothesis <span class="math notranslate nohighlight">\(H_{null}\)</span> (which is in this case parameterized by <span class="math notranslate nohighlight">\(\theta\)</span>) of finding data of equal or greater <em>incompatibility</em> with the predictions of <span class="math notranslate nohighlight">\(H_{null}\)</span>. Therefore, in our case, the p-value under the null hypothesis (defined by <span class="math notranslate nohighlight">\(\theta\)</span>) is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p_\theta (\nu) &amp;=\int_{\lambda_D}^\infty f \big(\lambda_{gen}(n,m \mid \theta,\nu) \mid H_{null} \big) \ d \lambda_{gen} \\
 &amp;= 1- \text{Prob} \big(\lambda_{gen}(n,m;\theta,\nu) \le \lambda_D(N,M;\theta,\nu) \big), \tag{2}
\end{align}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(f\)</span> is the PDF of <span class="math notranslate nohighlight">\(\lambda\)</span>. In the strict frequentist approach, <span class="math notranslate nohighlight">\(\theta\)</span> is rejected only if the <span class="math notranslate nohighlight">\(p\)</span>-value is less than the significance level of a hypothesis test <span class="math notranslate nohighlight">\(\alpha\)</span> (i.e. accepeted if <span class="math notranslate nohighlight">\(p_\theta (\nu) \le \alpha\)</span>)</p>
<p>In our study we approximate this integral as the histogram of <span class="math notranslate nohighlight">\(\theta\)</span> weighted by <span class="math notranslate nohighlight">\(Z\)</span> divided by the histogram of <span class="math notranslate nohighlight">\(\theta\)</span>, and denote this histogram as <span class="math notranslate nohighlight">\(\mathbf{h}\)</span>. We compare this exact p-value with the outcome of our MLP, <span class="math notranslate nohighlight">\(\mathbf{f}=\hat{p}(\theta, \nu, N,M)\)</span> for the case where <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is taken as the MLE in Fig. below we see almost perfect match.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Train_data_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">LFI_PIVOT_BASE</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">,</span><span class="s1">&#39;two_parameters_theta_0_20_1000k_Examples_MLE_True.csv&#39;</span><span class="p">)</span>
<span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">Train_data_path</span><span class="p">)</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>Z</th>
      <th>theta</th>
      <th>nu</th>
      <th>theta_hat</th>
      <th>N</th>
      <th>M</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>499999.500000</td>
      <td>0.870163</td>
      <td>10.000631</td>
      <td>9.998143</td>
      <td>0.001890</td>
      <td>4.999536</td>
      <td>4.997646</td>
    </tr>
    <tr>
      <th>std</th>
      <td>288675.278933</td>
      <td>0.336124</td>
      <td>5.770812</td>
      <td>5.775031</td>
      <td>3.654818</td>
      <td>2.582486</td>
      <td>2.582186</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000003</td>
      <td>0.000029</td>
      <td>-8.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>249999.750000</td>
      <td>1.000000</td>
      <td>5.003307</td>
      <td>4.993816</td>
      <td>-3.000000</td>
      <td>3.000000</td>
      <td>3.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>499999.500000</td>
      <td>1.000000</td>
      <td>10.001563</td>
      <td>9.995425</td>
      <td>0.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>749999.250000</td>
      <td>1.000000</td>
      <td>15.001295</td>
      <td>15.001992</td>
      <td>3.000000</td>
      <td>7.000000</td>
      <td>7.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>999999.000000</td>
      <td>1.000000</td>
      <td>19.999993</td>
      <td>19.999995</td>
      <td>8.000000</td>
      <td>9.000000</td>
      <td>9.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bins_</span> <span class="o">=</span> <span class="s1">&#39;auto&#39;</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;Z&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins_</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$Z$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins_</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;nu&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins_</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\nu$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;N&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins_</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$N$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;M&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins_</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$M$&#39;</span><span class="p">)</span>

<span class="p">[</span><span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="n">font_legend</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Generated Data&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_33_0.png" src="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_33_0.png" />
</div>
</div>
</section>
</section>
<section id="generate-theta-i-nu-i-n-i-m-i-z-i-data-according-to-the-following-and-according-to-whether-mle-true-and-save-as-dataframes">
<h2>Generate <span class="math notranslate nohighlight">\(\{\theta_i, \nu_i, N_i, M_i, Z_i \}\)</span>  data according to the following, and according to whether MLE=True, and save as dataframes<a class="headerlink" href="#generate-theta-i-nu-i-n-i-m-i-z-i-data-according-to-the-following-and-according-to-whether-mle-true-and-save-as-dataframes" title="Permalink to this headline">#</a></h2>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\theta &amp; \sim \textrm{uniform}(0, 20), \\
\nu &amp; \sim \textrm{uniform}(0, 20), \\
\end{align}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\left\{
\begin{align}
n &amp; \sim \textrm{poisson}(\theta + \nu),\\
m &amp; \sim \textrm{poisson}(\nu),\\
\end{align}
\right\} \rightarrow \lambda_\text{gen}(n, m \mid \theta,\nu)
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\left\{
\begin{align}
N &amp; \sim \textrm{uniform}(0,10),\\
M &amp; \sim \textrm{uniform}(0, 10), \\
\end{align}
\right\} \rightarrow \lambda_D(N, M \mid \theta,\nu)
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
Z  = \mathbb{I}
\left[ \lambda_\text{gen}(n, m \mid \theta,\nu) \leq \lambda_D(N, M \mid \theta,\nu) \right].
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thetaMin</span><span class="p">,</span> <span class="n">thetaMax</span> <span class="o">=</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">20</span>
<span class="n">numin</span><span class="p">,</span> <span class="n">numax</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">20</span>
<span class="n">Nmin</span><span class="p">,</span> <span class="n">Nmax</span> <span class="o">=</span>  <span class="mi">1</span><span class="p">,</span><span class="mi">10</span>
<span class="n">Mmin</span><span class="p">,</span> <span class="n">Mmax</span> <span class="o">=</span>  <span class="mi">1</span> <span class="p">,</span> <span class="mi">10</span>

<span class="k">def</span> <span class="nf">generate_training_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">MLE</span><span class="p">,</span> <span class="n">save_data</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generate the training data, that is, features=[theta, nu, N, M], targets=Z&quot;&quot;&quot;</span>
    <span class="c1">#sample theta and nu from uniform(0,20)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">thetaMin</span><span class="p">,</span> <span class="n">thetaMax</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="c1"># nu = st.uniform.rvs(nuMin, nuMax, size=Bprime)</span>
    <span class="n">nu</span><span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">numin</span><span class="p">,</span> <span class="n">numax</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="c1">#n,m ~ F_{\theta,\nu}, ie our simulator. sample n from a Poisson with mean theta+nu </span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">theta</span><span class="o">+</span> <span class="n">nu</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="c1">#sample m from a poisson with mean nu</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nu</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="c1">#sample our observed counts (N,M), which take the place of D</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">Nmin</span><span class="p">,</span> <span class="n">Nmax</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">Mmin</span><span class="p">,</span> <span class="n">Mmax</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="n">theta_hat_</span> <span class="o">=</span> <span class="n">theta_hat</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">M</span><span class="p">,</span> <span class="n">MLE</span><span class="p">)</span>
    <span class="n">SUBSAMPLE</span><span class="o">=</span><span class="mi">10</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;n=&#39;</span><span class="p">,</span> <span class="n">n</span><span class="p">[:</span><span class="n">SUBSAMPLE</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;m=&#39;</span><span class="p">,</span> <span class="n">m</span><span class="p">[:</span><span class="n">SUBSAMPLE</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;N=&#39;</span><span class="p">,</span> <span class="n">N</span><span class="p">[:</span><span class="n">SUBSAMPLE</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;M=&#39;</span><span class="p">,</span> <span class="n">M</span><span class="p">[:</span><span class="n">SUBSAMPLE</span><span class="p">])</span>
    <span class="n">lambda_gen</span> <span class="o">=</span> <span class="n">lambda_test</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">MLE</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;lambda_gen= &#39;</span><span class="p">,</span> <span class="n">lambda_gen</span><span class="p">[:</span><span class="n">SUBSAMPLE</span><span class="p">])</span>
    <span class="n">lambda_D</span> <span class="o">=</span> <span class="n">lambda_test</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">MLE</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;lambda_D= &#39;</span><span class="p">,</span> <span class="n">lambda_D</span><span class="p">[:</span><span class="n">SUBSAMPLE</span><span class="p">])</span>
    <span class="c1">#if lambda_gen &lt;= lambda_D: Z=1, else Z=0</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="p">(</span><span class="n">lambda_gen</span> <span class="o">&lt;=</span> <span class="n">lambda_D</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    
    <span class="n">data_2_param</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Z&#39;</span> <span class="p">:</span> <span class="n">Z</span><span class="p">,</span> <span class="s1">&#39;theta&#39;</span> <span class="p">:</span> <span class="n">theta</span><span class="p">,</span> <span class="s1">&#39;nu&#39;</span><span class="p">:</span> <span class="n">nu</span><span class="p">,</span> <span class="s1">&#39;theta_hat&#39;</span><span class="p">:</span> <span class="n">theta_hat_</span><span class="p">,</span> <span class="s1">&#39;N&#39;</span><span class="p">:</span><span class="n">N</span><span class="p">,</span> <span class="s1">&#39;M&#39;</span><span class="p">:</span><span class="n">M</span><span class="p">}</span>

    <span class="n">data_2_param</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">data_2_param</span><span class="p">)</span>
    <span class="n">PATH</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">LFI_PIVOT_BASE</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">,</span><span class="s1">&#39;two_parameters_theta_</span><span class="si">%s</span><span class="s1">_</span><span class="si">%s</span><span class="s1">_</span><span class="si">%s</span><span class="s1">k_Examples_MLE_</span><span class="si">%s</span><span class="s1">.csv&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">thetaMin</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">thetaMax</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">Bprime</span><span class="o">/</span><span class="mi">1000</span><span class="p">)),</span> <span class="nb">str</span><span class="p">(</span><span class="n">MLE</span><span class="p">))</span> <span class="p">)</span>
    <span class="k">if</span> <span class="n">save_data</span><span class="p">:</span>
        <span class="n">data_2_param</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">data_2_param</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">data_2_param</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="write-custom-data-loader">
<h1>Write Custom Data Loader<a class="headerlink" href="#write-custom-data-loader" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># %writefile data/dataloader.py</span>

<span class="k">def</span> <span class="nf">split_t_x</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">source</span><span class="p">):</span>
    <span class="c1"># change from pandas dataframe format to a numpy </span>
    <span class="c1"># array of the specified types</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">target</span><span class="p">])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">source</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="c1"># the numpy function choice(length, number)</span>
    <span class="c1"># selects at random &quot;batch_size&quot; integers from </span>
    <span class="c1"># the range [0, length-1] corresponding to the</span>
    <span class="c1"># row indices.</span>
    <span class="n">rows</span>    <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">batch_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">rows</span><span class="p">]</span>
    <span class="c1"># batch_x.T[-1] = np.random.uniform(0, 1, batch_size)</span>
    <span class="k">return</span> <span class="n">batch_x</span>


<span class="k">def</span> <span class="nf">get_data_sets</span><span class="p">(</span><span class="n">simulate_data</span><span class="p">,</span> <span class="n">batchsize</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;write custom data generator because who wants to read pytorch&#39;s DataLoader source code</span>
<span class="sd">    (and its sometimes slow for some reason)&quot;&quot;&quot;</span>
    <span class="c1"># if simulate_data:</span>
    <span class="c1">#     Train_data_MLE_True = generate_training_data(Bprime=100000, MLE=True, save_data=False)</span>
        
    <span class="c1"># if SUBSAMPLE:</span>
    <span class="c1">#     data=load_df(&#39;data/TWO_PARAMETERS_TRAINING_DATA_1M.csv&#39;, SUBSAMPLE=10000)#This is MLE DATA!</span>
    <span class="c1"># else:</span>
    <span class="c1">#     data=load_df(&#39;data/TWO_PARAMETERS_TRAINING_DATA_1M.csv&#39;)</span>
    <span class="c1"># data=load_df(&#39;data/TWO_PARAMETERS_TRAINING_DATA_1M.csv&#39;)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">LFI_PIVOT_BASE</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">,</span><span class="s1">&#39;two_parameters_theta_0_20_1000k_Examples_MLE_True.csv&#39;</span><span class="p">),</span> 
                 <span class="c1"># nrows=SUBSAMPLE,</span>
                 <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Z&#39;</span><span class="p">,</span><span class="s1">&#39;theta&#39;</span><span class="p">,</span> <span class="s1">&#39;nu&#39;</span><span class="p">,</span> <span class="s1">&#39;theta_hat&#39;</span><span class="p">,</span> <span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;M&#39;</span><span class="p">]</span>
                <span class="p">)</span>
    <span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> 
                                         <span class="n">test_size</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="c1">#split the train data (0.8 of whole set) again into 0.8*0.8=0.64 of whole set</span>
    <span class="c1"># train_data, valid_data = train_test_split(train_data, test_size=0.2)</span>
    <span class="n">train_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># valid_data = valid_data.reset_index(drop=True)</span>
    <span class="n">test_data</span>  <span class="o">=</span> <span class="n">test_data</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">target</span><span class="o">=</span><span class="s1">&#39;Z&#39;</span>
    <span class="n">source</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">,</span> <span class="s1">&#39;nu&#39;</span><span class="p">,</span><span class="s1">&#39;theta_hat&#39;</span><span class="p">,</span> <span class="s1">&#39;N&#39;</span><span class="p">,</span><span class="s1">&#39;M&#39;</span><span class="p">]</span>
    <span class="n">train_t</span><span class="p">,</span> <span class="n">train_x</span> <span class="o">=</span> <span class="n">split_t_x</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">source</span><span class="o">=</span><span class="n">source</span><span class="p">)</span>
    <span class="c1"># valid_t, valid_x = split_t_x(valid_data, target=target, source=source)</span>
    <span class="n">test_t</span><span class="p">,</span>  <span class="n">test_x</span>  <span class="o">=</span> <span class="n">split_t_x</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span>  <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">source</span><span class="o">=</span><span class="n">source</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">training_set_features</span><span class="p">():</span>
            <span class="c1">#start with an infinite loop, so that you can keep calling next (i.e. set = train_set(); set.next() ) until you run out of training examples</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="c1">#get a random batch of the defined size</span>
            <span class="n">batch_x</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">batchsize</span><span class="p">)</span>
            <span class="c1">#print(&#39;batch_x&#39;, batch_x)</span>
            <span class="c1">#index of one of the items in our examples</span>
            <span class="k">yield</span> <span class="n">batch_x</span>

    <span class="k">def</span> <span class="nf">evaluation_set_features</span><span class="p">():</span>
        <span class="c1">#start with an infinite loop, so that you can keep calling next (i.e. set = train_set(); set.next() ) until you run out of training examples</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">batch_x</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span><span class="n">batchsize</span><span class="p">)</span>
            <span class="c1">#index of one of the items in our examples</span>
            <span class="k">yield</span> <span class="n">batch_x</span>


    <span class="k">def</span> <span class="nf">training_set_targets</span><span class="p">():</span>
            <span class="c1">#start with an infinite loop, so that you can keep calling next (i.e. set = train_set(); set.next() ) until you run out of training examples</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="c1">#get a random batch of the defined size</span>
            <span class="n">batch_x</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="n">train_t</span><span class="p">,</span> <span class="n">batchsize</span><span class="p">)</span>
            <span class="c1">#print(&#39;batch_x&#39;, batch_x)</span>
            <span class="c1">#index of one of the items in our examples</span>
            <span class="k">yield</span> <span class="n">batch_x</span>

    <span class="k">def</span> <span class="nf">evaluation_set_targets</span><span class="p">():</span>
            <span class="c1">#start with an infinite loop, so that you can keep calling next (i.e. set = train_set(); set.next() ) until you run out of training examples</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="c1">#get a random batch of the defined size</span>
            <span class="n">batch_x</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="n">test_t</span><span class="p">,</span> <span class="n">batchsize</span><span class="p">)</span>
            <span class="c1">#print(&#39;batch_x&#39;, batch_x)</span>
            <span class="c1">#index of one of the items in our examples</span>
            <span class="k">yield</span> <span class="n">batch_x</span>

    <span class="k">return</span> <span class="n">training_set_features</span><span class="p">,</span> <span class="n">training_set_targets</span><span class="p">,</span> <span class="n">evaluation_set_features</span><span class="p">,</span> <span class="n">evaluation_set_targets</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">training_set_features</span><span class="p">,</span> <span class="n">training_set_targets</span><span class="p">,</span> <span class="n">evaluation_set_features</span><span class="p">,</span> <span class="n">evaluation_set_targets</span> <span class="o">=</span> <span class="n">get_data_sets</span><span class="p">(</span><span class="n">simulate_data</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batchsize</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

<span class="n">first_features_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">training_set_features</span><span class="p">())</span>
<span class="n">sample_x</span> <span class="o">=</span><span class="n">first_features_batch</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;first features batch </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">first_features_batch</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">first features batch shape </span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">first_features_batch</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>first features batch 
 [[ 3.83720104 11.39067517 -5.          3.          8.        ]
 [ 3.76615642  3.86821434  1.          4.          3.        ]
 [ 7.83838534  7.36589312  7.          9.          2.        ]
 [ 1.04025137 10.42523933 -7.          2.          9.        ]
 [16.06030956 16.17231306  4.          6.          2.        ]]

first features batch shape 
 (300, 5)
</pre></div>
</div>
</div>
</div>
<p>Plot the histogrammed function the histogrammed function <span class="math notranslate nohighlight">\(h(\tilde{\theta}, \nu, N, M) = h(\theta_{min}, \theta_{max}, \nu, N, M)\)</span> where <span class="math notranslate nohighlight">\(\tilde{\theta}\)</span> means that it is simulated (inside the function).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_hist_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span>
              <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span>
              <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
                <span class="n">nbins</span><span class="p">,</span>
             <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>

    <span class="n">theta</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">nu</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nu</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    
    <span class="n">Z</span> <span class="o">=</span> <span class="p">(</span><span class="n">lambda_test</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="n">MLE</span><span class="p">)</span> <span class="o">&lt;</span> 
         <span class="n">lambda_test</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="n">MLE</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">thetarange</span> <span class="o">=</span> <span class="p">(</span><span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">)</span>
    <span class="c1"># bins = binsize(Bprime)</span>

    <span class="c1"># weighted histogram   (count the number of ones per bin)</span>
    <span class="n">y1</span><span class="p">,</span> <span class="n">bb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> 
                          <span class="n">bins</span><span class="o">=</span><span class="n">nbins</span><span class="p">,</span> 
                          <span class="nb">range</span><span class="o">=</span><span class="n">thetarange</span><span class="p">,</span> 
                          <span class="n">weights</span><span class="o">=</span><span class="n">Z</span><span class="p">)</span>
    
    <span class="c1"># unweighted histogram (count number of ones and zeros per bin)</span>
    <span class="n">yt</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> 
                         <span class="n">bins</span><span class="o">=</span><span class="n">nbins</span><span class="p">,</span> 
                         <span class="nb">range</span><span class="o">=</span><span class="n">thetarange</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span>  <span class="n">y1</span> <span class="o">/</span> <span class="n">yt</span>    
    
    <span class="k">return</span> <span class="n">y</span><span class="p">,</span> <span class="n">bb</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h</span><span class="p">,</span> <span class="n">h_bins</span> <span class="o">=</span> <span class="n">make_hist_data</span><span class="p">(</span><span class="n">Bprime</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> 
                           <span class="n">thetamin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">thetamax</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
                           <span class="n">nu</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">N</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">M</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                           <span class="n">nbins</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">h</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.33333333, 0.        , 0.        , 0.        , 0.5       ,
       0.66666667, 0.22222222, 0.5       , 0.5       , 0.5       ,
       0.42857143, 0.        , 0.45454545, 0.66666667, 1.        ,
       0.        , 0.75      , 0.66666667, 1.        , 0.66666667,
       0.71428571, 0.75      , 0.66666667, 0.83333333, 0.66666667,
       1.        , 1.        , 1.        , 1.        , 0.85714286,
       1.        , 0.6       , 1.        , 1.        , 1.        ,
       1.        , 0.9       , 0.5       , 0.875     , 1.        ,
       1.        , 0.66666667, 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
       1.        , 1.        , 1.        , 1.        , 1.        ])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_one_hist</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">MLE</span><span class="p">,</span> <span class="n">nbins</span><span class="p">,</span> <span class="n">ax</span><span class="p">):</span>
    <span class="n">counts</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span> <span class="n">make_hist_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span>
              <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span>
              <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
            <span class="n">nbins</span><span class="p">,</span>
             <span class="n">MLE</span><span class="p">)</span>
    <span class="n">bin_centers</span> <span class="o">=</span> <span class="p">(</span><span class="n">bins</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">bins</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bin_centers</span><span class="p">,</span> <span class="n">counts</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{h}</span><span class="s1">$ Example&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="n">font_axes</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$E(Z|\theta,\nu)$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="n">font_axes</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;center right&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="n">font_legend</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">();</span><span class="n">ax</span><span class="o">=</span><span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">()</span>

<span class="c1">#Example:</span>
<span class="n">plot_one_hist</span><span class="p">(</span><span class="n">Bprime</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">thetamin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">thetamax</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">N</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">M</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">nbins</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_44_0.png" src="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_44_0.png" />
</div>
</div>
<section id="plot-the-histogrammed-approximations-for-the-mle-vs-non-mle-cases-for-a-single-value-of-mathbf-nu">
<h2>Plot the histogrammed approximations for the MLE vs non-MLE cases for a single value of <span class="math notranslate nohighlight">\(\mathbf{\nu}\)</span><a class="headerlink" href="#plot-the-histogrammed-approximations-for-the-mle-vs-non-mle-cases-for-a-single-value-of-mathbf-nu" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_data_one_nu</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">MLE</span><span class="p">,</span> 
              <span class="n">FONTSIZE</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
              <span class="n">func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">fgsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">)):</span>
    
    <span class="c1"># make room for 6 sub-plots</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                           <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
                           <span class="n">figsize</span><span class="o">=</span><span class="n">fgsize</span><span class="p">)</span>
    
    <span class="c1"># padding</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.20</span><span class="p">)</span>
    
    <span class="c1"># use flatten() to convert a numpy array of </span>
    <span class="c1"># shape (nrows, ncols) to a 1-d array. </span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">D</span><span class="p">):</span>
        
        <span class="n">y</span><span class="p">,</span> <span class="n">bb</span> <span class="o">=</span> <span class="n">make_hist_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span>
                              <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span>
                              <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
                              <span class="n">nbins</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                              <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="o">-</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">FONTSIZE</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$E(Z|\theta, \nu)$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">FONTSIZE</span><span class="p">)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">bb</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">bb</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{h}</span><span class="s1">$, MLE&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
        <span class="c1">#h is histogram approximation</span>

        <span class="n">y_nonMLE</span><span class="p">,</span> <span class="n">bb_nonMLE</span> <span class="o">=</span> <span class="n">make_hist_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span>
                              <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span>
                              <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
                              <span class="n">nbins</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                              <span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
        
        <span class="n">x_nonMLE</span> <span class="o">=</span> <span class="p">(</span><span class="n">bb_nonMLE</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">bb_nonMLE</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_nonMLE</span><span class="p">,</span> <span class="n">y_nonMLE</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{h}</span><span class="s1">$, non-MLE&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
        
        
        <span class="k">if</span> <span class="n">func</span><span class="p">:</span>
            <span class="n">p</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>
            <span class="c1">#f is model approximation</span>
        
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">0.42</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$N, M = </span><span class="si">%d</span><span class="s1">, </span><span class="si">%d</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">font_legend</span><span class="o">-</span><span class="mi">3</span>
                   <span class="c1"># fontsize=FONTSIZE</span>
                  <span class="p">)</span> 

        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">0.30</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$\nu = </span><span class="si">%5.1f</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="n">nu</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">font_legend</span><span class="o">-</span><span class="mi">3</span>
                   <span class="c1"># fontsize=FONTSIZE</span>
                  <span class="p">)</span> 

        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="n">font_legend</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
        
    <span class="c1"># hide unused sub-plots</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ax</span><span class="p">)):</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="p">[</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span> <span class="k">for</span> <span class="n">N</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">M</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
<span class="n">plot_data_one_nu</span><span class="p">(</span><span class="n">Bprime</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">thetamin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">thetamax</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">D</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_47_0.png" src="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_47_0.png" />
</div>
</div>
</section>
<section id="plot-the-histogrammed-approximation-mathbf-h-for-the-mle-vs-non-mle-cases-for-multiple-values-of-mathbf-nu-indicating-the-dependence-on-the-nuissance-parameter">
<h2>Plot the histogrammed approximation <span class="math notranslate nohighlight">\(\mathbf{h}\)</span>, for the MLE vs non-MLE cases for multiple values of <span class="math notranslate nohighlight">\(\mathbf{\nu}\)</span>, indicating the dependence on the nuissance parameter<a class="headerlink" href="#plot-the-histogrammed-approximation-mathbf-h-for-the-mle-vs-non-mle-cases-for-multiple-values-of-mathbf-nu-indicating-the-dependence-on-the-nuissance-parameter" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">MLE</span><span class="p">,</span> 
              <span class="n">FONTSIZE</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
              <span class="n">func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">fgsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">13</span><span class="p">)):</span>
    
    <span class="c1"># make room for 6 sub-plots</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> 
                           <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                           <span class="n">figsize</span><span class="o">=</span><span class="n">fgsize</span><span class="p">)</span>
    
    <span class="c1"># padding</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.20</span><span class="p">)</span>
    
    <span class="c1"># use flatten() to convert a numpy array of </span>
    <span class="c1"># shape (nrows, ncols) to a 1-d array. </span>
    <span class="c1"># ax = ax.flatten()</span>
    
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">D</span><span class="p">):</span>
        <span class="n">NU1</span><span class="o">=</span><span class="mi">1</span>
        <span class="n">y</span><span class="p">,</span> <span class="n">bb</span> <span class="o">=</span> <span class="n">make_hist_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span>
                              <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span>
                               <span class="n">NU1</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
                              <span class="n">nbins</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                              <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="o">-</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.03</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">FONTSIZE</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$E(Z|\theta, \nu)$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">FONTSIZE</span><span class="p">)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">bb</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">bb</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{h}</span><span class="s1">$, MLE&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.45</span><span class="p">)</span>
        <span class="c1">#h is histogram approximation</span>

        <span class="n">y_nonMLE</span><span class="p">,</span> <span class="n">bb_nonMLE</span> <span class="o">=</span> <span class="n">make_hist_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span>
                              <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span>
                              <span class="n">NU1</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
                              <span class="n">nbins</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                              <span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
        
        <span class="n">x_nonMLE</span> <span class="o">=</span> <span class="p">(</span><span class="n">bb_nonMLE</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">bb_nonMLE</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_nonMLE</span><span class="p">,</span> <span class="n">y_nonMLE</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{h}</span><span class="s1">$, non-MLE&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.45</span><span class="p">)</span>
        
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">6.1</span><span class="p">,</span> <span class="mf">0.42</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$N, M = </span><span class="si">%d</span><span class="s1">, </span><span class="si">%d</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">),</span> 
                   <span class="c1"># fontsize=FONTSIZE</span>
                  <span class="p">)</span> 

        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">6.1</span><span class="p">,</span> <span class="mf">0.30</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$\nu = </span><span class="si">%5.1f</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="n">NU1</span><span class="p">,</span> 
                   <span class="c1"># fontsize=FONTSIZE</span>
                  <span class="p">)</span> 

        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">font_legend</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
        <span class="c1">############define second nu value to see difference</span>
        <span class="n">NU2</span><span class="o">=</span><span class="mi">9</span>
        <span class="n">y</span><span class="p">,</span> <span class="n">bb</span> <span class="o">=</span> <span class="n">make_hist_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span>
                              <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span>
                               <span class="n">NU2</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
                              <span class="n">nbins</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                              <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="o">-</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.03</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">FONTSIZE</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$E(Z|\theta, \nu)$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">FONTSIZE</span><span class="p">)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">bb</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">bb</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{h}</span><span class="s1">$, MLE&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.45</span><span class="p">)</span>
        <span class="c1">#h is histogram approximation</span>

        <span class="n">y_nonMLE</span><span class="p">,</span> <span class="n">bb_nonMLE</span> <span class="o">=</span> <span class="n">make_hist_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span>
                              <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span>
                              <span class="n">NU1</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
                              <span class="n">nbins</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                              <span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
        
        <span class="n">x_nonMLE</span> <span class="o">=</span> <span class="p">(</span><span class="n">bb_nonMLE</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">bb_nonMLE</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_nonMLE</span><span class="p">,</span> <span class="n">y_nonMLE</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{h}</span><span class="s1">$, non-MLE&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.45</span><span class="p">)</span>
        
         
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">6.1</span><span class="p">,</span> <span class="mf">0.42</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$N, M = </span><span class="si">%d</span><span class="s1">, </span><span class="si">%d</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">),</span> 
                   <span class="c1"># fontsize=FONTSIZE</span>
                  <span class="p">)</span> 

        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">6.1</span><span class="p">,</span> <span class="mf">0.30</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$\nu = </span><span class="si">%5.1f</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="n">NU2</span><span class="p">,</span> 
                   <span class="c1"># fontsize=FONTSIZE</span>
                  <span class="p">)</span> 

        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">font_legend</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">func</span><span class="p">:</span>
            <span class="n">p</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;f&#39;</span><span class="p">)</span>
            <span class="c1">#f is model approximation</span>
        

        
    <span class="c1"># hide unused sub-plots</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ax</span><span class="p">)):</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="p">[</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span> <span class="k">for</span> <span class="n">N</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span> <span class="k">for</span> <span class="n">M</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
<span class="n">plot_data</span><span class="p">(</span><span class="n">Bprime</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">thetamin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">thetamax</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">D</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_50_0.png" src="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_50_0.png" />
</div>
</div>
<hr class="docutils" />
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="ml">
<h1>ML<a class="headerlink" href="#ml" title="Permalink to this headline">#</a></h1>
<p>We begin our LFI methods which use machine learning.
It’s important to remember “the master formula” of all of machine learning:</p>
<div class="math notranslate nohighlight">
\[\int \frac{\partial L}{\partial f} p(y|x) dy  = 0 \tag{1}\]</div>
<p>or, equivalently,</p>
<div class="math notranslate nohighlight">
\[ \frac{\delta R}{\delta f}=0,\]</div>
<p>where <span class="math notranslate nohighlight">\(L\)</span> is the loss function, <span class="math notranslate nohighlight">\(f\)</span> is the model (neural network/classifier/regressor, etc), which is implicitly parameterized by potentially a  gazillion model parameters, <span class="math notranslate nohighlight">\(y\)</span> is the target(s) that we want to estimate, <span class="math notranslate nohighlight">\(x\)</span> is the (set of) training features, <span class="math notranslate nohighlight">\(R\)</span> is the risk functional (sometime called objective function or cost function):</p>
<div class="math notranslate nohighlight">
\[ R[f] = \int \cdots \int \, p(y, \mathbf{x}) \, L(f(\mathbf{x}, \theta), y) \, dy \, d\mathbf{x}\]</div>
<p>where the <span class="math notranslate nohighlight">\(R[f]\)</span> is approximated by the normalized sum of the losses over the all the samples.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train_data_MLE_True = generate_training_data(Bprime=1000000, MLE=True, save_data=True)</span>
<span class="n">Train_data_MLE_False</span> <span class="o">=</span> <span class="n">generate_training_data</span><span class="p">(</span><span class="n">Bprime</span><span class="o">=</span><span class="mi">1000000</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">save_data</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="n">Train_data_MLE_False</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>n= [19 19 22 25 16 28  3 17 13 26]
m= [ 9  6 10  9  3 11  0  1 13 19]
N= [3 9 4 5 1 3 3 6 4 7]
M= [9 2 2 1 9 2 3 5 1 6]
lambda_gen=  [5.80446216e-02 4.66461900e-02 5.15648524e-02 3.85369974e-01
 1.75592591e+00 3.44759184e+00 1.74781961e-02 1.66367027e-05
 3.63826972e-03 3.34587552e-03]
lambda_D=  [15.85586909  3.43176285 11.12579457 16.78905695 13.1769769   3.19793214
  1.17598077 13.25191639  1.57108146  2.7966759 ]


                    Z           theta              nu       theta_hat  \
count  1000000.000000  1000000.000000  1000000.000000  1000000.000000   
mean         0.870360       10.002061       10.004964        1.481481   
std          0.335907        5.771444        5.771873        2.115271   
min          0.000000        0.000004        0.000005        0.000000   
25%          1.000000        5.006408        5.012335        0.000000   
50%          1.000000       10.002464       10.004190        0.000000   
75%          1.000000       14.999081       15.009455        3.000000   
max          1.000000       19.999973       19.999992        8.000000   

                    N               M  
count  1000000.000000  1000000.000000  
mean         4.999822        4.999232  
std          2.581183        2.581157  
min          1.000000        1.000000  
25%          3.000000        3.000000  
50%          5.000000        5.000000  
75%          7.000000        7.000000  
max          9.000000        9.000000  
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">getwholedata</span><span class="p">(</span><span class="n">MLE_or_nonMLE</span><span class="p">,</span> <span class="n">valid</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">MLE</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/two_parameters_theta_0_20_1000k_Examples_MLE_True.csv&#39;</span><span class="p">,</span> 
                     <span class="c1"># nrows=SUBSAMPLE,</span>
                     <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Z&#39;</span><span class="p">,</span><span class="s1">&#39;theta&#39;</span><span class="p">,</span> <span class="s1">&#39;nu&#39;</span><span class="p">,</span> <span class="s1">&#39;theta_hat&#39;</span><span class="p">,</span> <span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;M&#39;</span><span class="p">]</span>
                    <span class="p">)</span>
        
    <span class="k">else</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/two_parameters_theta_0_20_1000k_Examples_MLE_False.csv&#39;</span><span class="p">,</span> 
             <span class="c1"># nrows=SUBSAMPLE,</span>
             <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Z&#39;</span><span class="p">,</span><span class="s1">&#39;theta&#39;</span><span class="p">,</span> <span class="s1">&#39;nu&#39;</span><span class="p">,</span> <span class="s1">&#39;theta_hat&#39;</span><span class="p">,</span> <span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;M&#39;</span><span class="p">]</span>
            <span class="p">)</span>
    <span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="c1">#split the train data (0.8 of whole set) again into 0.8*0.8=0.64 of whole set</span>
    

    <span class="n">train_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">test_data</span>  <span class="o">=</span> <span class="n">test_data</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">target</span><span class="o">=</span><span class="s1">&#39;Z&#39;</span>
    <span class="n">source</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">,</span><span class="s1">&#39;nu&#39;</span><span class="p">,</span><span class="s1">&#39;theta_hat&#39;</span><span class="p">,</span><span class="s1">&#39;N&#39;</span><span class="p">,</span><span class="s1">&#39;M&#39;</span><span class="p">]</span>

    <span class="n">train_t</span><span class="p">,</span> <span class="n">train_x</span> <span class="o">=</span> <span class="n">split_t_x</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">source</span><span class="o">=</span><span class="n">source</span><span class="p">)</span>
    <span class="n">test_t</span><span class="p">,</span>  <span class="n">test_x</span>  <span class="o">=</span> <span class="n">split_t_x</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span>  <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">source</span><span class="o">=</span><span class="n">source</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;train_t shape = &#39;</span><span class="p">,</span> <span class="n">train_t</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;train_x shape = &#39;</span><span class="p">,</span> <span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">valid</span><span class="p">:</span>
        <span class="c1">#if you want to also make a validation data set</span>
        <span class="n">train_data</span><span class="p">,</span> <span class="n">valid_data</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="n">valid_data</span> <span class="o">=</span> <span class="n">valid_data</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">valid_t</span><span class="p">,</span> <span class="n">valid_x</span> <span class="o">=</span> <span class="n">split_t_x</span><span class="p">(</span><span class="n">valid_data</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">source</span><span class="o">=</span><span class="n">source</span><span class="p">)</span>

        
    <span class="k">return</span> <span class="n">train_t</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">test_t</span><span class="p">,</span>  <span class="n">test_x</span>
</pre></div>
</div>
</div>
</div>
<section id="define-model-mathbf-f-which-will-approximate-the-expectation-value-above">
<h2>Define Model <span class="math notranslate nohighlight">\(\mathbf{f}\)</span>, which will approximate the expectation value above<a class="headerlink" href="#define-model-mathbf-f-which-will-approximate-the-expectation-value-above" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_inputs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_nodes</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>

        <span class="c1"># call constructor of base (or super, or parent) class</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="c1"># create input layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer0</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_nodes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer0</span><span class="p">)</span>

        <span class="c1"># create &quot;hidden&quot; layers</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">):</span>
            <span class="n">cmd</span> <span class="o">=</span> <span class="s1">&#39;self.layer</span><span class="si">%d</span><span class="s1"> = nn.Linear(</span><span class="si">%d</span><span class="s1">, </span><span class="si">%d</span><span class="s1">)&#39;</span> <span class="o">%</span> \
            <span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">n_nodes</span><span class="p">,</span> <span class="n">n_nodes</span><span class="p">)</span>
            <span class="n">exec</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span>
            <span class="n">cmd</span> <span class="o">=</span> <span class="s1">&#39;self.layers.append(self.layer</span><span class="si">%d</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="n">l</span>
            <span class="n">exec</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span>
          
        <span class="c1"># create output layer</span>
        <span class="n">cmd</span> <span class="o">=</span> <span class="s1">&#39;self.layer</span><span class="si">%d</span><span class="s1"> = nn.Linear(</span><span class="si">%d</span><span class="s1">, 1)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">n_nodes</span><span class="p">)</span>
        <span class="n">exec</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span>
        <span class="n">cmd</span> <span class="o">=</span> <span class="s1">&#39;self.layers.append(self.layer</span><span class="si">%d</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="n">n_layers</span>
        <span class="n">exec</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span>

    <span class="c1"># define (required) method to compute output of network</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">y</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>

<span class="n">model_</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model(
  (layer0): Linear(in_features=4, out_features=20, bias=True)
  (layer1): Linear(in_features=20, out_features=20, bias=True)
  (layer2): Linear(in_features=20, out_features=20, bias=True)
  (layer3): Linear(in_features=20, out_features=20, bias=True)
  (layer4): Linear(in_features=20, out_features=20, bias=True)
  (layer5): Linear(in_features=20, out_features=1, bias=True)
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">average_quadratic_loss</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># f and t must be of the same shape</span>
    <span class="k">return</span>  <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">f</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">average_loss</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="c1"># f and t must be of the same shape</span>
    <span class="k">return</span>  <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">f</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">avloss</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="c1"># make sure we set evaluation mode so that any training specific</span>
    <span class="c1"># operations are disabled.</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="c1"># evaluation mode</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c1"># no need to compute gradients wrt. x and t</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="c1"># remember to reshape!</span>
        <span class="n">o</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avloss</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_features_training_batch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="c1"># the numpy function choice(length, number)</span>
    <span class="c1"># selects at random &quot;batch_size&quot; integers from </span>
    <span class="c1"># the range [0, length-1] corresponding to the</span>
    <span class="c1"># row indices.</span>
    <span class="n">rows</span>    <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">batch_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">rows</span><span class="p">]</span>
    <span class="n">batch_t</span> <span class="o">=</span> <span class="n">t</span><span class="p">[</span><span class="n">rows</span><span class="p">]</span>
    <span class="c1"># batch_x.T[-1] = np.random.uniform(0, 1, batch_size)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_t</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">avloss</span><span class="p">,</span>
          <span class="n">batch_size</span><span class="p">,</span> 
          <span class="n">n_iterations</span><span class="p">,</span> <span class="n">traces</span><span class="p">,</span> 
          <span class="n">step</span><span class="p">,</span> <span class="n">window</span><span class="p">,</span> <span class="n">MLE</span><span class="p">):</span>
    
    <span class="c1"># to keep track of average losses</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy_t</span><span class="p">,</span> <span class="n">yy_v</span><span class="p">,</span> <span class="n">yy_v_avg</span> <span class="o">=</span> <span class="n">traces</span>
    

    
    <span class="k">if</span> <span class="n">MLE</span><span class="p">:</span>
        <span class="n">train_t</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">test_t</span><span class="p">,</span>  <span class="n">test_x</span> <span class="o">=</span> <span class="n">getwholedata</span><span class="p">(</span><span class="n">MLE_or_nonMLE</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">valid</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">train_t</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">test_t</span><span class="p">,</span>  <span class="n">test_x</span> <span class="o">=</span> <span class="n">getwholedata</span><span class="p">(</span><span class="n">MLE_or_nonMLE</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">valid</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration vs average loss&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%10s</span><span class="se">\t</span><span class="si">%10s</span><span class="se">\t</span><span class="si">%10s</span><span class="s2">&quot;</span> <span class="o">%</span> \
          <span class="p">(</span><span class="s1">&#39;iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;train-set&#39;</span><span class="p">,</span> <span class="s1">&#39;valid-set&#39;</span><span class="p">))</span>
    
    <span class="c1"># training_set_features, training_set_targets, evaluation_set_features, evaluation_set_targets = get_data_sets(simulate_data=False, batchsize=batch_size)</span>
    
    <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>

        <span class="c1"># set mode to training so that training specific </span>
        <span class="c1"># operations such as dropout are enabled.</span>

        
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        
        <span class="c1"># get a random sample (a batch) of data (as numpy arrays)</span>
        
        <span class="c1">#Harrison-like Loader</span>
        <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_t</span> <span class="o">=</span> <span class="n">get_features_training_batch</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_t</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        
        <span class="c1">#Or Ali&#39;s Loader</span>
        <span class="c1"># batch_x, batch_t = next(training_set_features()), next(training_set_targets())</span>
        <span class="c1"># batch_x_eval, batch_t_eval = next(evaluation_set_features()), next(evaluation_set_targets())</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c1"># no need to compute gradients </span>
            <span class="c1"># wrt. x and t</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">batch_t</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>      


        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
   
        <span class="c1"># compute a noisy approximation to the average loss</span>
        <span class="n">empirical_risk</span> <span class="o">=</span> <span class="n">avloss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># use automatic differentiation to compute a </span>
        <span class="c1"># noisy approximation of the local gradient</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>       <span class="c1"># clear previous gradients</span>
        <span class="n">empirical_risk</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>   <span class="c1"># compute gradients</span>
        
        <span class="c1"># finally, advance one step in the direction of steepest </span>
        <span class="c1"># descent, using the noisy local gradient. </span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>            <span class="c1"># move one step</span>
        
        <span class="k">if</span> <span class="n">ii</span> <span class="o">%</span> <span class="n">step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            
            
            <span class="c1">#using Harrison-like loader</span>
            <span class="n">acc_t</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">avloss</span><span class="p">,</span> <span class="n">train_x</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> <span class="n">train_t</span><span class="p">[:</span><span class="n">n</span><span class="p">])</span> 
            <span class="n">acc_v</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">avloss</span><span class="p">,</span> <span class="n">test_x</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> <span class="n">test_t</span><span class="p">[:</span><span class="n">n</span><span class="p">])</span>
            
            <span class="c1">#using Ali&#39;s loader</span>
            <span class="c1"># acc_t = validate(model, avloss, batch_x, batch_t) </span>
            <span class="c1"># acc_v = validate(model, avloss, batch_x_eval, batch_t_eval)</span>
            

            <span class="n">yy_t</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_t</span><span class="p">)</span>
            <span class="n">yy_v</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_v</span><span class="p">)</span>
            
            <span class="c1"># compute running average for validation data</span>
            <span class="n">len_yy_v</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">yy_v</span><span class="p">)</span>
            <span class="k">if</span>   <span class="n">len_yy_v</span> <span class="o">&lt;</span> <span class="n">window</span><span class="p">:</span>
                <span class="n">yy_v_avg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">yy_v</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="p">)</span>
            <span class="k">elif</span> <span class="n">len_yy_v</span> <span class="o">==</span> <span class="n">window</span><span class="p">:</span>
                <span class="n">yy_v_avg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="nb">sum</span><span class="p">(</span><span class="n">yy_v</span><span class="p">)</span> <span class="o">/</span> <span class="n">window</span> <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">acc_v_avg</span>  <span class="o">=</span> <span class="n">yy_v_avg</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">window</span>
                <span class="n">acc_v_avg</span> <span class="o">+=</span> <span class="n">yy_v</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">yy_v</span><span class="p">[</span><span class="o">-</span><span class="n">window</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">yy_v_avg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_v_avg</span> <span class="o">/</span> <span class="n">window</span><span class="p">)</span>
                        
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">xx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%10d</span><span class="se">\t</span><span class="si">%10.6f</span><span class="se">\t</span><span class="si">%10.6f</span><span class="s2">&quot;</span> <span class="o">%</span> \
                      <span class="p">(</span><span class="n">xx</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yy_t</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yy_v</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">xx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xx</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">step</span><span class="p">)</span>
                    
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\r</span><span class="si">%10d</span><span class="se">\t</span><span class="si">%10.6f</span><span class="se">\t</span><span class="si">%10.6f</span><span class="se">\t</span><span class="si">%10.6f</span><span class="s2">&quot;</span> <span class="o">%</span> \
                          <span class="p">(</span><span class="n">xx</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yy_t</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yy_v</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yy_v_avg</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> 
                      <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
            
    <span class="nb">print</span><span class="p">()</span>      
    <span class="k">return</span> <span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy_t</span><span class="p">,</span> <span class="n">yy_v</span><span class="p">,</span> <span class="n">yy_v_avg</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_average_loss</span><span class="p">(</span><span class="n">traces</span><span class="p">,</span> <span class="n">ftsize</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span><span class="n">save_loss_plots</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy_t</span><span class="p">,</span> <span class="n">yy_v</span><span class="p">,</span> <span class="n">yy_v_avg</span> <span class="o">=</span> <span class="n">traces</span>
    
    <span class="c1"># create an empty figure</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">))</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    
    <span class="c1"># add a subplot to it</span>
    <span class="n">nrows</span><span class="p">,</span> <span class="n">ncols</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span>
    <span class="n">ax</span>  <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">nrows</span><span class="p">,</span><span class="n">ncols</span><span class="p">,</span><span class="n">index</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Average loss&quot;</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy_t</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy_v</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation&#39;</span><span class="p">)</span>
    <span class="c1">#ax.plot(xx, yy_v_avg, &#39;g&#39;, lw=2, label=&#39;Running average&#39;)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Iterations&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">ftsize</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;average loss&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">ftsize</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">save_loss_plots</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;images/loss_curves/IQN_&#39;</span><span class="o">+</span><span class="n">N</span><span class="o">+</span><span class="n">T</span><span class="o">+</span><span class="s1">&#39;_Consecutive_2.png&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">loss curve saved in images/loss_curves/IQN_&#39;</span><span class="o">+</span><span class="n">N</span><span class="o">+</span><span class="n">target</span><span class="o">+</span><span class="s1">&#39;_Consecutive.png&#39;</span><span class="p">)</span>
    <span class="c1"># if show_loss_plots:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Define my regularized regression model. Since the values are on the same scales, it is not necessary to include batchnormalization or to normalize the data</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RegularizedRegressionModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="c1">#inherit from the super class</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nfeatures</span><span class="p">,</span> <span class="n">ntargets</span><span class="p">,</span> <span class="n">nlayers</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nlayers</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
                <span class="c1">#inital layer has to have size of input features as its input layer</span>
                <span class="c1">#its output layer can have any size but it must match the size of the input layer of the next linear layer</span>
                <span class="c1">#here we choose its output layer as the hidden size (fully connected)</span>
                <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">nfeatures</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
                <span class="c1">#batch normalization</span>
                <span class="c1"># layers.append(nn.BatchNorm1d(hidden_size))</span>
                <span class="c1">#Dropout seems to worsen model performance</span>
                <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">))</span>
                <span class="c1">#ReLU activation </span>
                <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1">#if this is not the first layer (we dont have layers)</span>
                <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
                <span class="c1"># layers.append(nn.BatchNorm1d(hidden_size))</span>
                <span class="c1">#Dropout seems to worsen model performance</span>
                <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">))</span>
                <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
                <span class="c1">#output layer:</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">ntargets</span><span class="p">))</span> 

        <span class="c1"># ONLY IF ITS A CLASSIFICATION, ADD SIGMOID</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">())</span>
            <span class="c1">#we have defined sequential model using the layers in oulist </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="make-a-hyperparameter-tuning-workflow">
<h1>Make a hyperparameter Tuning Workflow<a class="headerlink" href="#make-a-hyperparameter-tuning-workflow" title="Permalink to this headline">#</a></h1>
<p>Use Optuna ( <a class="reference external" href="https://arxiv.org/pdf/1907.10902.pdf">axriv:1907.10902</a> ) for hyperparameter tuning. The search space for the hyperparameters that I’m tuning is defined in the <code class="docutils literal notranslate"><span class="pre">params</span></code> dictionary:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
      <span class="s2">&quot;nlayers&quot;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;nlayers&quot;</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">13</span><span class="p">),</span>      
      <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">130</span><span class="p">),</span>
      <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span><span class="mf">0.5</span><span class="p">),</span>
      <span class="s2">&quot;optimizer_name&quot;</span> <span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s2">&quot;optimizer_name&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;Adam&quot;</span><span class="p">,</span> <span class="s2">&quot;RMSprop&quot;</span><span class="p">]),</span>
      <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">),</span>
      <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>

    <span class="p">}</span>
</pre></div>
</div>
<p>Note that the best hyperparameter values that yield best results <em>are extremely problem-dependent</em>, therefore I think that one must always use such an automatic tuning workflow to avoid spending forever tuning these parameters by hand. From my experience, I tend to observe that the hyperparameters that have the biggest impact on performance are: <em>type of optimizer, learning rate, and batch size</em>, where again, their best values are highly dependent on your problem/data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Engine</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;loss, training and evaluation&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
                 <span class="c1">#, device):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="c1">#self.device= device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span>
        
    <span class="c1">#the loss function returns the loss function. It is a static method so it doesn&#39;t need self</span>
    <span class="c1"># @staticmethod</span>
    <span class="c1"># def loss_fun(targets, outputs):</span>
    <span class="c1">#   tau = torch.rand(outputs.shape)</span>
    <span class="c1">#   return torch.mean(torch.where(targets &gt;= outputs, </span>
    <span class="c1">#                                   tau * (targets - outputs), </span>
    <span class="c1">#                                   (1 - tau)*(outputs - targets)))</span>

<span class="c1">#     This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, </span>
<span class="c1">#     by combining the operations into one layer</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;the training function: takes the training dataloader&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">final_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_t</span> <span class="o">=</span> <span class="n">get_features_training_batch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span>  <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span><span class="c1">#x and t are train_x and train_t</span>

            <span class="c1"># with torch.no_grad():</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">targets</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">batch_t</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">average_quadratic_loss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">final_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">final_loss</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>
    
    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;the training function: takes the training dataloader&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">final_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
            <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_t</span> <span class="o">=</span> <span class="n">get_features_training_batch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span>  <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span><span class="c1">#x and t are train_x and train_t</span>

            <span class="c1"># with torch.no_grad():            </span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">targets</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">batch_t</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span><span class="n">average_quadratic_loss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
            <span class="n">final_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">final_loss</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>



<span class="n">EPOCHS</span><span class="o">=</span><span class="mi">1</span>
<span class="k">def</span> <span class="nf">run_train</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">save_model</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;For tuning the parameters&quot;&quot;&quot;</span>

    <span class="n">model</span> <span class="o">=</span>  <span class="n">RegularizedRegressionModel</span><span class="p">(</span>
              <span class="n">nfeatures</span><span class="o">=</span><span class="n">sample_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> 
                <span class="n">ntargets</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">nlayers</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;nlayers&quot;</span><span class="p">],</span> 
                <span class="n">hidden_size</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">],</span>
                <span class="n">dropout</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;dropout&quot;</span><span class="p">]</span>
                <span class="p">)</span>
    <span class="c1"># print(model)</span>
    

    <span class="n">learning_rate</span><span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">]</span>
    <span class="n">optimizer_name</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;optimizer_name&quot;</span><span class="p">]</span>
    
    <span class="c1"># optimizer = torch.optim.Adam(model.parameters(), lr=params[&quot;learning_rate&quot;]) </span>
    
    <span class="n">optimizer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="p">,</span> <span class="n">optimizer_name</span><span class="p">)(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    
    <span class="n">eng</span><span class="o">=</span><span class="n">Engine</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">])</span>
    <span class="n">best_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    <span class="n">early_stopping_iter</span><span class="o">=</span><span class="mi">10</span>
    <span class="n">early_stopping_coutner</span><span class="o">=</span><span class="mi">0</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCHS</span><span class="p">):</span>
        <span class="n">train_loss</span> <span class="o">=</span> <span class="n">eng</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_t</span><span class="p">)</span>
        <span class="n">valid_loss</span><span class="o">=</span><span class="n">eng</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_t</span><span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> </span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">train_loss</span><span class="si">}</span><span class="s2"> </span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="n">valid_loss</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">valid_loss</span><span class="o">&lt;</span><span class="n">best_loss</span><span class="p">:</span>
            <span class="n">best_loss</span><span class="o">=</span><span class="n">valid_loss</span>
            <span class="k">if</span> <span class="n">save_model</span><span class="p">:</span>
                <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">&quot;model_m.bin&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">early_stopping_coutner</span><span class="o">+=</span><span class="mi">1</span>
        <span class="k">if</span> <span class="n">early_stopping_coutner</span> <span class="o">&gt;</span> <span class="n">early_stopping_iter</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">best_loss</span>

<span class="c1"># run_train()</span>

<span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
      <span class="s2">&quot;nlayers&quot;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;nlayers&quot;</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">23</span><span class="p">),</span>      
      <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">130</span><span class="p">),</span>
      <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span><span class="mf">0.5</span><span class="p">),</span>
      <span class="s2">&quot;optimizer_name&quot;</span> <span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_categorical</span><span class="p">(</span><span class="s2">&quot;optimizer_name&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;Adam&quot;</span><span class="p">,</span> <span class="s2">&quot;RMSprop&quot;</span><span class="p">]),</span>
      <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_float</span><span class="p">(</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">),</span>
      <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">trial</span><span class="o">.</span><span class="n">suggest_int</span><span class="p">(</span><span class="s2">&quot;batch_size&quot;</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>

    <span class="p">}</span>
    <span class="c1"># all_losses=[]</span>

    <span class="n">temp_loss</span> <span class="o">=</span> <span class="n">run_train</span><span class="p">(</span><span class="n">params</span><span class="p">,</span><span class="n">save_model</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1"># all_losses.append(temp_loss)</span>
    <span class="k">return</span> <span class="n">temp_loss</span>

<span class="k">def</span> <span class="nf">tune_hyperparameters</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Getting best hyperparameters&#39;</span><span class="p">)</span>
    <span class="n">study</span><span class="o">=</span><span class="n">optuna</span><span class="o">.</span><span class="n">create_study</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="s2">&quot;minimize&quot;</span><span class="p">)</span>
    <span class="n">study</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">best_trial</span> <span class="o">=</span> <span class="n">study</span><span class="o">.</span><span class="n">best_trial</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;best model parameters&#39;</span><span class="p">,</span> <span class="n">best_trial</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>

    <span class="n">best_params</span><span class="o">=</span><span class="n">best_trial</span><span class="o">.</span><span class="n">params</span><span class="c1">#this is a dictionary</span>
    <span class="n">filename</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">LFI_PIVOT_BASE</span><span class="p">,</span><span class="s1">&#39;best_params&#39;</span><span class="p">,</span><span class="s1">&#39;best_params_Test_Trials.csv&#39;</span><span class="p">)</span>
    <span class="n">param_df</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
                            <span class="s1">&#39;n_layers&#39;</span><span class="p">:</span><span class="n">best_params</span><span class="p">[</span><span class="s2">&quot;nlayers&quot;</span><span class="p">],</span> 
                            <span class="s1">&#39;hidden_size&#39;</span><span class="p">:</span><span class="n">best_params</span><span class="p">[</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">],</span> 
                            <span class="s1">&#39;dropout&#39;</span><span class="p">:</span><span class="n">best_params</span><span class="p">[</span><span class="s2">&quot;dropout&quot;</span><span class="p">],</span>
                            <span class="s1">&#39;optimizer_name&#39;</span><span class="p">:</span><span class="n">best_params</span><span class="p">[</span><span class="s2">&quot;optimizer_name&quot;</span><span class="p">],</span>
                            <span class="s1">&#39;learning_rate&#39;</span><span class="p">:</span> <span class="n">best_params</span><span class="p">[</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">],</span> 
                            <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span><span class="n">best_params</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span> <span class="p">},</span>
                                    <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="n">param_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>   
</pre></div>
</div>
</div>
</div>
<section id="don-t-run-the-one-cell-below-unless-you-want-to-tune">
<h2>Don’t run the one cell below, unless you want to tune!<a class="headerlink" href="#don-t-run-the-one-cell-below-unless-you-want-to-tune" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># tune_hyperparameters()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="load-the-dictionary-of-the-best-hyperparameters-that-was-saved-from-our-hyperparameter-tuning-workflow-and-retrieve-the-values">
<h1>Load the dictionary of the best hyperparameters that was saved from our hyperparameter tuning workflow, and retrieve the values<a class="headerlink" href="#load-the-dictionary-of-the-best-hyperparameters-that-was-saved-from-our-hyperparameter-tuning-workflow-and-retrieve-the-values" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">BEST_PARAMS</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;best_params/best_params_Test_Trials.csv&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">)</span>

<span class="n">n_layers</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;n_layers&quot;</span><span class="p">])</span> 
<span class="n">hidden_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">])</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;dropout&quot;</span><span class="p">])</span>
<span class="n">optimizer_name</span> <span class="o">=</span> <span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;optimizer_name&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_string</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">learning_rate</span> <span class="o">=</span>  <span class="nb">float</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">])</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>   Unnamed: 0  n_layers  hidden_size  dropout optimizer_name  learning_rate  \
0           0         4           11  0.13208        RMSprop       0.006398   

   batch_size  
0        1000  
</pre></div>
</div>
</div>
</div>
<section id="define-network-node-shapes-parameters-and-training-data">
<h2>Define network node shapes, parameters, and training data<a class="headerlink" href="#define-network-node-shapes-parameters-and-training-data" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">BATCHSIZE</span><span class="o">=</span><span class="n">batch_size</span>
<span class="n">training_set_features</span><span class="p">,</span> <span class="n">training_set_targets</span><span class="p">,</span> <span class="n">evaluation_set_features</span><span class="p">,</span> <span class="n">evaluation_set_targets</span> <span class="o">=</span> \
<span class="n">get_data_sets</span><span class="p">(</span><span class="n">simulate_data</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batchsize</span><span class="o">=</span><span class="n">BATCHSIZE</span><span class="p">)</span>

<span class="n">sample_x</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">training_set_features</span><span class="p">())</span><span class="c1">#this is just to get the dimenstions of one batch</span>
<span class="n">sample_y</span><span class="o">=</span><span class="nb">next</span><span class="p">(</span><span class="n">training_set_targets</span><span class="p">())</span>
<span class="c1">#(batchsize,5) for mass</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sample x shape&#39;</span><span class="p">,</span> <span class="n">sample_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;sample t shape&#39;</span><span class="p">,</span> <span class="n">sample_y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">n_features</span> <span class="o">=</span> <span class="n">sample_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>



<span class="n">model</span> <span class="o">=</span>  <span class="n">RegularizedRegressionModel</span><span class="p">(</span>
    <span class="n">nfeatures</span><span class="o">=</span><span class="n">sample_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> 
    <span class="n">ntargets</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">nlayers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> 
    <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> 
    <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span>
    <span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>sample x shape (1000, 5)
sample t shape (1000,)


RegularizedRegressionModel(
  (model): Sequential(
    (0): Linear(in_features=5, out_features=11, bias=True)
    (1): Dropout(p=0.1320798105984151, inplace=False)
    (2): ReLU()
    (3): Linear(in_features=11, out_features=11, bias=True)
    (4): Dropout(p=0.1320798105984151, inplace=False)
    (5): ReLU()
    (6): Linear(in_features=11, out_features=11, bias=True)
    (7): Dropout(p=0.1320798105984151, inplace=False)
    (8): ReLU()
    (9): Linear(in_features=11, out_features=11, bias=True)
    (10): Dropout(p=0.1320798105984151, inplace=False)
    (11): ReLU()
    (12): Linear(in_features=11, out_features=1, bias=True)
    (13): Sigmoid()
  )
)
</pre></div>
</div>
</div>
</div>
</section>
<section id="initiate-model-based-on-choice-of-whose-model-ali-or-harrison-and-parameters">
<h2>Initiate model based on choice of whose model (Ali or Harrison) and parameters<a class="headerlink" href="#initiate-model-based-on-choice-of-whose-model-ali-or-harrison-and-parameters" title="Permalink to this headline">#</a></h2>
<section id="then-train">
<h3>Then train<a class="headerlink" href="#then-train" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">initiate_whose_model</span><span class="p">(</span><span class="n">Ali_or_Harrison</span><span class="p">,</span> <span class="n">MLE</span><span class="p">):</span>
    <span class="n">whose_model</span><span class="o">=</span><span class="s1">&#39;Ali&#39;</span>

    <span class="k">if</span> <span class="n">whose_model</span><span class="o">==</span><span class="s1">&#39;Harrison&#39;</span><span class="p">:</span>
        <span class="n">n_layers</span><span class="o">=</span><span class="mi">5</span>
        <span class="n">hidden_size</span><span class="o">=</span><span class="mi">5</span>
        <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">)</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span>
        <span class="n">optimizer</span>     <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">))</span> 
        <span class="n">model</span><span class="o">=</span><span class="n">Model</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">whose_model</span><span class="o">==</span><span class="s1">&#39;Ali&#39;</span><span class="p">:</span>
        <span class="n">n_layers</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;n_layers&quot;</span><span class="p">])</span> 
        <span class="n">hidden_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">])</span>
        <span class="n">dropout</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;dropout&quot;</span><span class="p">])</span>
        <span class="n">optimizer_name</span> <span class="o">=</span> <span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;optimizer_name&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_string</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">learning_rate</span> <span class="o">=</span>  <span class="nb">float</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">])</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">])</span>
        <span class="n">model</span> <span class="o">=</span>  <span class="n">RegularizedRegressionModel</span><span class="p">(</span>
            <span class="n">nfeatures</span><span class="o">=</span><span class="n">sample_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> 
            <span class="n">ntargets</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">nlayers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> 
            <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> 
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span>
            <span class="p">)</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">optimizer_name</span><span class="p">)</span> <span class="p">)(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">optimizer_name</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_layers</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">optimizer_name</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">model_MLE</span><span class="p">,</span> <span class="n">optimizer_MLE</span> <span class="o">=</span> <span class="n">initiate_whose_model</span><span class="p">(</span><span class="s1">&#39;Ali&#39;</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">optimizer_MLE</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_MLE</span><span class="p">)</span>

<span class="c1">#also initiate non-MLE model</span>
<span class="n">n_layers</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">optimizer_name</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">model_nonMLE</span><span class="p">,</span> <span class="n">optimizer_nonMLE</span> <span class="o">=</span> <span class="n">initiate_whose_model</span><span class="p">(</span><span class="s1">&#39;Ali&#39;</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RMSprop (
Parameter Group 0
    alpha: 0.99
    centered: False
    eps: 1e-08
    lr: 0.0063975512794992
    momentum: 0
    weight_decay: 0
)



RegularizedRegressionModel(
  (model): Sequential(
    (0): Linear(in_features=5, out_features=11, bias=True)
    (1): Dropout(p=0.1320798105984151, inplace=False)
    (2): ReLU()
    (3): Linear(in_features=11, out_features=11, bias=True)
    (4): Dropout(p=0.1320798105984151, inplace=False)
    (5): ReLU()
    (6): Linear(in_features=11, out_features=11, bias=True)
    (7): Dropout(p=0.1320798105984151, inplace=False)
    (8): ReLU()
    (9): Linear(in_features=11, out_features=11, bias=True)
    (10): Dropout(p=0.1320798105984151, inplace=False)
    (11): ReLU()
    (12): Linear(in_features=11, out_features=1, bias=True)
    (13): Sigmoid()
  )
)
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="training-you-can-scroll-down-to-load-up-trained-model-instead-of-training-now">
<h1>Training: You can scroll down to load up trained model instead of training now<a class="headerlink" href="#training-you-can-scroll-down-to-load-up-trained-model-instead-of-training-now" title="Permalink to this headline">#</a></h1>
<section id="train-mle-model">
<h2>Train MLE model<a class="headerlink" href="#train-mle-model" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#initiate MLE model </span>
<span class="n">n_layers</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">optimizer_name</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">model_MLE</span><span class="p">,</span> <span class="n">optimizer_MLE</span> <span class="o">=</span> <span class="n">initiate_whose_model</span><span class="p">(</span><span class="s1">&#39;Ali&#39;</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">optimizer_MLE</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_MLE</span><span class="p">)</span>

<span class="n">BATCHSIZE</span><span class="o">=</span><span class="n">batch_size</span>
<span class="n">traces_MLE</span> <span class="o">=</span> <span class="p">([],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[])</span>
<span class="n">traces_step</span> <span class="o">=</span> <span class="mi">200</span>


<span class="n">n_iterations</span><span class="o">=</span><span class="mi">100000</span>
<span class="c1">#train</span>
<span class="n">traces_MLE</span><span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_MLE</span><span class="p">,</span> 
              <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer_MLE</span><span class="p">,</span> 
              <span class="n">avloss</span><span class="o">=</span><span class="n">average_quadratic_loss</span><span class="p">,</span>
              <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCHSIZE</span><span class="p">,</span> 
              <span class="n">n_iterations</span><span class="o">=</span><span class="n">n_iterations</span><span class="p">,</span> 
              <span class="n">traces</span><span class="o">=</span><span class="n">traces_MLE</span><span class="p">,</span> 
              <span class="n">step</span><span class="o">=</span><span class="n">traces_step</span><span class="p">,</span> 
              <span class="n">window</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RMSprop (
Parameter Group 0
    alpha: 0.99
    centered: False
    eps: 1e-08
    lr: 0.0063975512794992
    momentum: 0
    weight_decay: 0
)



RegularizedRegressionModel(
  (model): Sequential(
    (0): Linear(in_features=5, out_features=11, bias=True)
    (1): Dropout(p=0.1320798105984151, inplace=False)
    (2): ReLU()
    (3): Linear(in_features=11, out_features=11, bias=True)
    (4): Dropout(p=0.1320798105984151, inplace=False)
    (5): ReLU()
    (6): Linear(in_features=11, out_features=11, bias=True)
    (7): Dropout(p=0.1320798105984151, inplace=False)
    (8): ReLU()
    (9): Linear(in_features=11, out_features=11, bias=True)
    (10): Dropout(p=0.1320798105984151, inplace=False)
    (11): ReLU()
    (12): Linear(in_features=11, out_features=1, bias=True)
    (13): Sigmoid()
  )
)
train_t shape =  (800000,) 

train_x shape =  (800000, 5) 

Iteration vs average loss
 iteration	 train-set	 valid-set
         0	  0.107258	  0.107904
     99800	  0.060430	  0.060400	  0.061000
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># n_iterations=10000</span>
<span class="c1"># BATCHSIZE=500</span>
<span class="c1"># traces= train(model=model, optimizer=optimizer, avloss=average_quadratic_loss,</span>
<span class="c1">#           batch_size=BATCHSIZE, </span>
<span class="c1">#           n_iterations=n_iterations, traces=traces, </span>
<span class="c1">#           step=traces_step, window=100)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_average_loss</span><span class="p">(</span><span class="n">traces_MLE</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_74_0.png" src="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_74_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span><span class="n">MLE</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">MLE</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">model_MLE</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">model_nonMLE</span>
    <span class="n">PATH</span><span class="o">=</span><span class="s1">&#39;models/MLE_</span><span class="si">%s</span><span class="s1">_Regressor_</span><span class="si">%s</span><span class="s1">K_training_iter.pt&#39;</span> <span class="o">%</span> <span class="p">(</span> <span class="nb">str</span><span class="p">(</span><span class="n">MLE</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_iterations</span><span class="o">/</span><span class="mi">1000</span><span class="p">)</span> <span class="p">)</span>
    <span class="n">with_theta_hat</span><span class="o">=</span><span class="kc">True</span>
    <span class="k">if</span> <span class="n">with_theta_hat</span><span class="p">:</span>
        <span class="n">PATH</span><span class="o">=</span><span class="s1">&#39;models/MLE_</span><span class="si">%s</span><span class="s1">_Regressor_</span><span class="si">%s</span><span class="s1">K_training_iter_with_theta_hat.pt&#39;</span> <span class="o">%</span> <span class="p">(</span> <span class="nb">str</span><span class="p">(</span><span class="n">MLE</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_iterations</span><span class="o">/</span><span class="mi">1000</span><span class="p">)</span> <span class="p">)</span>
    
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>  <span class="n">PATH</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">save_model</span><span class="p">(</span><span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-non-mle-model">
<h2>Train non-MLE model<a class="headerlink" href="#train-non-mle-model" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#also initiate non-MLE model</span>
<span class="n">n_layers</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">optimizer_name</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">model_nonMLE</span><span class="p">,</span> <span class="n">optimizer_nonMLE</span> <span class="o">=</span> <span class="n">initiate_whose_model</span><span class="p">(</span><span class="s1">&#39;Ali&#39;</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">BATCHSIZE</span><span class="o">=</span><span class="n">batch_size</span>
<span class="n">traces_nonMLE</span> <span class="o">=</span> <span class="p">([],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[])</span>
<span class="n">traces_step</span> <span class="o">=</span> <span class="mi">400</span>
<span class="nb">print</span><span class="p">(</span><span class="n">optimizer_MLE</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_MLE</span><span class="p">)</span>

<span class="n">n_iterations</span><span class="o">=</span><span class="mi">100000</span>
<span class="n">traces_nonMLE</span><span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_nonMLE</span><span class="p">,</span> 
              <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer_nonMLE</span><span class="p">,</span> 
              <span class="n">avloss</span><span class="o">=</span><span class="n">average_quadratic_loss</span><span class="p">,</span>
              <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCHSIZE</span><span class="p">,</span> 
              <span class="n">n_iterations</span><span class="o">=</span><span class="n">n_iterations</span><span class="p">,</span> 
              <span class="n">traces</span><span class="o">=</span><span class="n">traces_nonMLE</span><span class="p">,</span> 
              <span class="n">step</span><span class="o">=</span><span class="n">traces_step</span><span class="p">,</span> 
              <span class="n">window</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span>
                <span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plot_average_loss</span><span class="p">(</span><span class="n">traces_nonMLE</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train_t shape =  (800000,) 

train_x shape =  (800000, 5) 

Iteration vs average loss
 iteration	 train-set	 valid-set
         0	  0.151603	  0.151636
     99600	  0.063669	  0.063138	  0.063138
</pre></div>
</div>
<img alt="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_78_1.png" src="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_78_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">save_model</span><span class="p">(</span><span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="make-sure-the-train-df-has-the-same-ranges-as-the-data-you-want-to-generate-for-evaluation">
<h3>Make sure the train df has the same ranges as the data you want to generate for evaluation<a class="headerlink" href="#make-sure-the-train-df-has-the-same-ranges-as-the-data-you-want-to-generate-for-evaluation" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">load_train_df</span><span class="p">(</span><span class="n">MLE</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; returns the dataframe, can be used if the dataframe is saved in csv format</span>
<span class="sd">    of if it is already in dataframe format (e.g. generated in this notebook). &quot;&quot;&quot;</span>
    <span class="c1"># SUBSAMPLE=int(1e5)</span>
    <span class="c1"># if isinstance(df_name,str):</span>
    <span class="k">if</span> <span class="n">MLE</span><span class="p">:</span>
        <span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/two_parameters_theta_0_20_1000k_Examples_MLE_True.csv&#39;</span><span class="p">,</span> 
                         <span class="c1"># nrows=SUBSAMPLE,</span>
                         <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Z&#39;</span><span class="p">,</span><span class="s1">&#39;theta&#39;</span><span class="p">,</span> <span class="s1">&#39;nu&#39;</span><span class="p">,</span> <span class="s1">&#39;theta_hat&#39;</span><span class="p">,</span> <span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;M&#39;</span><span class="p">]</span>
                        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/two_parameters_theta_0_20_1000k_Examples_MLE_False.csv&#39;</span><span class="p">,</span> 
                 <span class="c1"># nrows=SUBSAMPLE,</span>
                 <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Z&#39;</span><span class="p">,</span><span class="s1">&#39;theta&#39;</span><span class="p">,</span> <span class="s1">&#39;nu&#39;</span><span class="p">,</span> <span class="s1">&#39;theta_hat&#39;</span><span class="p">,</span> <span class="s1">&#39;N&#39;</span><span class="p">,</span> <span class="s1">&#39;M&#39;</span><span class="p">]</span>
                <span class="p">)</span>
    <span class="k">return</span> <span class="n">train_df</span>

<span class="n">train_df_MLE</span> <span class="o">=</span> <span class="n">load_train_df</span><span class="p">(</span><span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_df_MLE</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Z</th>
      <th>theta</th>
      <th>nu</th>
      <th>theta_hat</th>
      <th>N</th>
      <th>M</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.870163</td>
      <td>10.000631</td>
      <td>9.998143</td>
      <td>0.001890</td>
      <td>4.999536</td>
      <td>4.997646</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.336124</td>
      <td>5.770812</td>
      <td>5.775031</td>
      <td>3.654818</td>
      <td>2.582486</td>
      <td>2.582186</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>0.000003</td>
      <td>0.000029</td>
      <td>-8.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>1.000000</td>
      <td>5.003307</td>
      <td>4.993816</td>
      <td>-3.000000</td>
      <td>3.000000</td>
      <td>3.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>1.000000</td>
      <td>10.001563</td>
      <td>9.995425</td>
      <td>0.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.000000</td>
      <td>15.001295</td>
      <td>15.001992</td>
      <td>3.000000</td>
      <td>7.000000</td>
      <td>7.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.000000</td>
      <td>19.999993</td>
      <td>19.999995</td>
      <td>8.000000</td>
      <td>9.000000</td>
      <td>9.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_df_nonMLE</span> <span class="o">=</span> <span class="n">load_train_df</span><span class="p">(</span><span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">train_df_nonMLE</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Z</th>
      <th>theta</th>
      <th>nu</th>
      <th>theta_hat</th>
      <th>N</th>
      <th>M</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
      <td>1000000.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.870748</td>
      <td>10.002000</td>
      <td>10.003980</td>
      <td>1.483516</td>
      <td>5.001212</td>
      <td>4.999616</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.335479</td>
      <td>5.771159</td>
      <td>5.772888</td>
      <td>2.116762</td>
      <td>2.582294</td>
      <td>2.582844</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>0.000009</td>
      <td>0.000026</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>1.000000</td>
      <td>5.002570</td>
      <td>5.003612</td>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>3.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>1.000000</td>
      <td>10.009839</td>
      <td>10.011399</td>
      <td>0.000000</td>
      <td>5.000000</td>
      <td>5.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.000000</td>
      <td>14.995431</td>
      <td>15.000630</td>
      <td>3.000000</td>
      <td>7.000000</td>
      <td>7.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.000000</td>
      <td>19.999991</td>
      <td>19.999975</td>
      <td>8.000000</td>
      <td>9.000000</td>
      <td>9.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
</section>
<section id="make-on-the-fly-generated-evaluation-data">
<h2>Make “on-the-fly” generated evaluation data<a class="headerlink" href="#make-on-the-fly-generated-evaluation-data" title="Permalink to this headline">#</a></h2>
<p>Note that this is one advantage of LFI, where one can always generate more synthetic data (for training as well as evaluation), whereas in traditinoal ML, the raining and evaluation data sets are fixed. Here, we generate binned <span class="math notranslate nohighlight">\(\theta\)</span> with the same ranges as those of the training set, and constants for <span class="math notranslate nohighlight">\(\{ \nu, N, M \}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_eval_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">train_df</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">nbins</span><span class="p">):</span>
    <span class="c1">#if MLE true, load the model that was trained on MLE data and vice versa</span>
    <span class="c1"># N, M = D</span>
    <span class="c1"># nbins=NBINS</span>
    <span class="c1"># thetamin,thetamax=0,20</span>
    <span class="n">thetamin</span><span class="o">=</span><span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
    <span class="n">thetamax</span><span class="o">=</span><span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">thetastep</span> <span class="o">=</span> <span class="p">(</span><span class="n">thetamax</span><span class="o">-</span><span class="n">thetamin</span><span class="p">)</span> <span class="o">/</span> <span class="n">nbins</span>
    <span class="n">bb</span>    <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="o">+</span><span class="n">thetastep</span><span class="p">,</span> <span class="n">thetastep</span><span class="p">)</span><span class="c1">#this is just making a vector of thetas</span>
    <span class="n">X</span>     <span class="o">=</span> <span class="p">(</span><span class="n">bb</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="n">bb</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="n">x</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">theta_hat</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<section id="look-at-an-example-of-a-on-the-fly-generated-evaluation-data">
<h3>Look at an example of a “on-the-fly” generated evaluation data<a class="headerlink" href="#look-at-an-example-of-a-on-the-fly-generated-evaluation-data" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eval_data_example</span><span class="p">,</span> <span class="n">eval_bins_example</span> <span class="o">=</span><span class="n">make_eval_data</span><span class="p">(</span><span class="n">Bprime</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">train_df</span><span class="o">=</span><span class="n">train_df_MLE</span><span class="p">,</span><span class="n">nu</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">nbins</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">eval_data_example</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.0333,  3.0000, -2.0000,  1.0000,  3.0000],
        [ 0.1000,  3.0000, -2.0000,  1.0000,  3.0000],
        [ 0.1667,  3.0000, -2.0000,  1.0000,  3.0000],
        [ 0.2333,  3.0000, -2.0000,  1.0000,  3.0000],
        [ 0.3000,  3.0000, -2.0000,  1.0000,  3.0000]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eval_data_example</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">eval_bins_example</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(torch.Size([301, 5]), (301,))
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="load-and-evaluate-trained-model-at-generated-data">
<h1>Load and Evaluate Trained model at generated data<a class="headerlink" href="#load-and-evaluate-trained-model-at-generated-data" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">usemodel</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">train_df</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span><span class="n">M</span><span class="p">,</span> <span class="n">MLE</span><span class="p">,</span> <span class="n">nbins</span><span class="p">):</span>
    
    <span class="c1">#Generate evaluation data at those fixed nu, N, M values</span>
    <span class="n">eval_data</span><span class="p">,</span> <span class="n">eval_bins</span> <span class="o">=</span><span class="n">make_eval_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span><span class="n">train_df</span><span class="p">,</span><span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span><span class="n">M</span><span class="p">,</span> <span class="n">nbins</span><span class="p">)</span><span class="c1">#eval data is indipendent of MLE, since its just constants witha theta variable</span>

    <span class="c1"># if MLE==True:</span>
    <span class="c1">#     model=model</span>
    <span class="c1">#else load the model trained on non-MLE data</span>
    <span class="c1"># PATH=&#39;models/MLE_TRUE_Regressor_200.0K_training_iter.pt&#39;</span>
    
    <span class="c1">#LOAD TRAINED MODEL</span>
    <span class="n">with_theta_hat</span><span class="o">=</span><span class="kc">False</span>
    <span class="k">if</span> <span class="n">MLE</span><span class="p">:</span>
        
        <span class="n">PATH</span><span class="o">=</span> <span class="s1">&#39;models/MLE_TRUE_Regressor_200.0K_training_iter.pt&#39;</span>
        <span class="n">PATH</span><span class="o">=</span> <span class="s1">&#39;models/MLE_True_Regressor_100.0K_training_iter_with_theta_hat.pt&#39;</span>
        
    <span class="k">else</span><span class="p">:</span>
        <span class="n">PATH</span><span class="o">=</span> <span class="s1">&#39;models/MLE_False_Regressor_200.0K_training_iter.pt&#39;</span>
        <span class="n">PATH</span><span class="o">=</span> <span class="s1">&#39;models/MLE_False_Regressor_100.0K_training_iter_with_theta_hat.pt&#39;</span>
    <span class="n">n_layers</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;n_layers&quot;</span><span class="p">])</span> 
    <span class="n">hidden_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">])</span>
    <span class="n">dropout</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;dropout&quot;</span><span class="p">])</span>
    <span class="n">optimizer_name</span> <span class="o">=</span> <span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;optimizer_name&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_string</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">learning_rate</span> <span class="o">=</span>  <span class="nb">float</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">])</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">])</span>
    <span class="n">model</span> <span class="o">=</span>  <span class="n">RegularizedRegressionModel</span><span class="p">(</span>
        <span class="n">nfeatures</span><span class="o">=</span><span class="n">sample_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> 
        <span class="n">ntargets</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">nlayers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> 
        <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> 
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span>
        <span class="p">)</span>
    <span class="c1">#EVALUATE AT AT EVAL_DATA</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span> <span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">(</span><span class="n">eval_data</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">eval_bins</span>
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="evaluate-model-at-an-example-set-of-eval-data-points-to-see-the-predicted-p-value-hat-p">
<h1>Evaluate model at an example set of “eval_data” points to see the predicted <span class="math notranslate nohighlight">\(p\)</span>-value (<span class="math notranslate nohighlight">\(\hat{p}\)</span>)<a class="headerlink" href="#evaluate-model-at-an-example-set-of-eval-data-points-to-see-the-predicted-p-value-hat-p" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">phat_MLE</span><span class="p">,</span> <span class="n">phatbins_MLE</span> <span class="o">=</span> <span class="n">usemodel</span><span class="p">(</span><span class="n">Bprime</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span><span class="n">train_df</span><span class="o">=</span><span class="n">train_df</span><span class="p">,</span><span class="n">nu</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">nbins</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">phat_MLE</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RegularizedRegressionModel(
  (model): Sequential(
    (0): Linear(in_features=5, out_features=11, bias=True)
    (1): Dropout(p=0.1320798105984151, inplace=False)
    (2): ReLU()
    (3): Linear(in_features=11, out_features=11, bias=True)
    (4): Dropout(p=0.1320798105984151, inplace=False)
    (5): ReLU()
    (6): Linear(in_features=11, out_features=11, bias=True)
    (7): Dropout(p=0.1320798105984151, inplace=False)
    (8): ReLU()
    (9): Linear(in_features=11, out_features=11, bias=True)
    (10): Dropout(p=0.1320798105984151, inplace=False)
    (11): ReLU()
    (12): Linear(in_features=11, out_features=1, bias=True)
    (13): Sigmoid()
  )
)
[[0.26882723]
 [0.29309997]
 [0.32397294]
 [0.34872386]
 [0.37133864]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">phatbins_MLE</span><span class="p">,</span> <span class="n">phat_MLE</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{f}</span><span class="s1">$ MLE Example&#39;</span><span class="p">);</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_92_0.png" src="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_92_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">phat_nonMLE</span><span class="p">,</span> <span class="n">phatbins_nonMLE</span> <span class="o">=</span> <span class="n">usemodel</span><span class="p">(</span><span class="n">Bprime</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span><span class="n">train_df</span><span class="o">=</span><span class="n">train_df</span><span class="p">,</span><span class="n">nu</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">nbins</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">phat_nonMLE</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">phatbins_nonMLE</span><span class="p">,</span> <span class="n">phat_nonMLE</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{f}</span><span class="s1">$ non-MLE Example&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RegularizedRegressionModel(
  (model): Sequential(
    (0): Linear(in_features=5, out_features=11, bias=True)
    (1): Dropout(p=0.1320798105984151, inplace=False)
    (2): ReLU()
    (3): Linear(in_features=11, out_features=11, bias=True)
    (4): Dropout(p=0.1320798105984151, inplace=False)
    (5): ReLU()
    (6): Linear(in_features=11, out_features=11, bias=True)
    (7): Dropout(p=0.1320798105984151, inplace=False)
    (8): ReLU()
    (9): Linear(in_features=11, out_features=11, bias=True)
    (10): Dropout(p=0.1320798105984151, inplace=False)
    (11): ReLU()
    (12): Linear(in_features=11, out_features=1, bias=True)
    (13): Sigmoid()
  )
)
[[0.45620778]
 [0.47124666]
 [0.4863379 ]
 [0.5014541 ]
 [0.5165771 ]]
</pre></div>
</div>
<img alt="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_93_1.png" src="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_93_1.png" />
</div>
</div>
<section id="save-trained-model-if-they-re-good-and-if-you-havent-saved-by-now">
<h2>SAVE TRAINED MODEL (if they’re good, and if you havent saved by now)<a class="headerlink" href="#save-trained-model-if-they-re-good-and-if-you-havent-saved-by-now" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># save_model(MLE=True)</span>
<span class="c1"># save_model(MLE=False)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="this-is-how-you-load-a-trained-model">
<h2>This is how you load a trained model<a class="headerlink" href="#this-is-how-you-load-a-trained-model" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#load</span>
<span class="n">PATH</span><span class="o">=</span><span class="s1">&#39;models/MLE_TRUE_Regressor_20.0K_training_iter.pt&#39;</span>
<span class="n">n_layers</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;n_layers&quot;</span><span class="p">])</span> 
<span class="n">hidden_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">])</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;dropout&quot;</span><span class="p">])</span>
<span class="n">optimizer_name</span> <span class="o">=</span> <span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;optimizer_name&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_string</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">learning_rate</span> <span class="o">=</span>  <span class="nb">float</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">])</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">])</span>
<span class="n">model</span> <span class="o">=</span>  <span class="n">RegularizedRegressionModel</span><span class="p">(</span>
    <span class="n">nfeatures</span><span class="o">=</span><span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> 
    <span class="n">ntargets</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">nlayers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> 
    <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> 
    <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span>
    <span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span> <span class="p">)</span>
<span class="c1">#OR</span>
<span class="c1">#model=torch.load(PATH)#BUT HERE IT WILL BE A DICT (CANT BE EVALUATED RIGHT AWAY) DISCOURAGED!</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RegularizedRegressionModel(
  (model): Sequential(
    (0): Linear(in_features=4, out_features=11, bias=True)
    (1): Dropout(p=0.1320798105984151, inplace=False)
    (2): ReLU()
    (3): Linear(in_features=11, out_features=11, bias=True)
    (4): Dropout(p=0.1320798105984151, inplace=False)
    (5): ReLU()
    (6): Linear(in_features=11, out_features=11, bias=True)
    (7): Dropout(p=0.1320798105984151, inplace=False)
    (8): ReLU()
    (9): Linear(in_features=11, out_features=11, bias=True)
    (10): Dropout(p=0.1320798105984151, inplace=False)
    (11): ReLU()
    (12): Linear(in_features=11, out_features=1, bias=True)
    (13): Sigmoid()
  )
)
</pre></div>
</div>
</div>
</div>
</section>
<section id="you-could-also-evaluate-the-trained-model-on-the-validation-data">
<h2>You could also evaluate the trained model on the validation data<a class="headerlink" href="#you-could-also-evaluate-the-trained-model-on-the-validation-data" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># nbins=100</span>
<span class="c1"># thetamin=train_df[&#39;theta&#39;].min()</span>
<span class="c1"># thetamax=train_df[&#39;theta&#39;].max()</span>
<span class="c1"># thetastep = (thetamax-thetamin) / nbins</span>
<span class="c1"># bb    = np.arange(thetamin, thetamax+thetastep, thetastep)#this is just making a vector of thetas</span>
<span class="c1"># X     = (bb[1:] + bb[:-1])/2</span>
<span class="n">eval_data</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">valid_x</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">phat</span><span class="o">=</span><span class="n">model</span><span class="p">(</span><span class="n">eval_data</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">phat</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.8695913],
       [0.8695913],
       [0.8695913],
       ...,
       [0.8695913],
       [0.8695913],
       [0.8695913]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_data_one_nu_with_model</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">MLE</span><span class="p">,</span> 
                     <span class="n">NBINS</span><span class="p">,</span>
              <span class="n">FONTSIZE</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
              <span class="n">func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">fgsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">save_image</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    
    <span class="c1"># make room for 6 sub-plots</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                           <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
                           <span class="n">figsize</span><span class="o">=</span><span class="n">fgsize</span><span class="p">)</span>
    
    <span class="c1"># padding</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.20</span><span class="p">)</span>
    
    <span class="c1"># use flatten() to convert a numpy array of </span>
    <span class="c1"># shape (nrows, ncols) to a 1-d array. </span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">D</span><span class="p">):</span>
        
        <span class="n">y</span><span class="p">,</span> <span class="n">bb</span> <span class="o">=</span> <span class="n">make_hist_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span>
                              <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span>
                              <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
                              <span class="n">nbins</span><span class="o">=</span><span class="n">NBINS</span><span class="p">,</span>
                              <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">thetamin</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">thetamax</span><span class="o">-</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.03</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf{\theta}$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">FONTSIZE</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf{E(Z|\theta, \nu)}$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">FONTSIZE</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">bb</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">bb</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{h}</span><span class="s1">$ MLE&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
        <span class="c1">#h is histogram approximation</span>

        <span class="n">y_nonMLE</span><span class="p">,</span> <span class="n">bb_nonMLE</span> <span class="o">=</span> <span class="n">make_hist_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span>
                              <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span>
                              <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
                              <span class="n">nbins</span><span class="o">=</span><span class="n">NBINS</span><span class="p">,</span>
                              <span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
        
        <span class="n">x_nonMLE</span> <span class="o">=</span> <span class="p">(</span><span class="n">bb_nonMLE</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">bb_nonMLE</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_nonMLE</span><span class="p">,</span> <span class="n">y_nonMLE</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{h}</span><span class="s1">$ non-MLE&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
        
        
        <span class="k">if</span> <span class="n">func</span><span class="p">:</span>
            <span class="n">train_df_MLE</span> <span class="o">=</span> <span class="n">load_train_df</span><span class="p">(</span><span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">train_df_nonMLE</span> <span class="o">=</span> <span class="n">load_train_df</span><span class="p">(</span><span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            
            <span class="n">f_MLE</span><span class="p">,</span> <span class="n">f_bins_MLE</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">train_df_MLE</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">nbins</span><span class="o">=</span><span class="n">NBINS</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">f_bins_MLE</span><span class="p">,</span> <span class="n">f_MLE</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{f}</span><span class="s1">$ MLE&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
            <span class="c1">#f is model approximation</span>
            
            <span class="n">f_nonMLE</span><span class="p">,</span> <span class="n">f_bins_nonMLE</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">train_df_nonMLE</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">nbins</span><span class="o">=</span><span class="n">NBINS</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">f_bins_nonMLE</span><span class="p">,</span> <span class="n">f_nonMLE</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{f}</span><span class="s1">$ non-MLE&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
            
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">3.1</span><span class="p">,</span> <span class="mf">0.42</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$N, M = </span><span class="si">%d</span><span class="s1">, </span><span class="si">%d</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">font_legend</span><span class="o">-</span><span class="mi">3</span>
                   <span class="c1"># fontsize=FONTSIZE</span>
                  <span class="p">)</span> 

        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">3.1</span><span class="p">,</span> <span class="mf">0.30</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$\nu = </span><span class="si">%5.1f</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="n">nu</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">font_legend</span><span class="o">-</span><span class="mi">3</span>
                   <span class="c1"># fontsize=FONTSIZE</span>
                  <span class="p">)</span> 

        <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="n">font_legend</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
        
    <span class="c1"># hide unused sub-plots</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ax</span><span class="p">)):</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">save_image</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;images/h_MLE_nonMLE_f_MLE_f_nonMLE_one_nu</span><span class="si">%s</span><span class="s1">.png&#39;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">nu</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Compare the histogrammed function <span class="math notranslate nohighlight">\(h(\theta, \nu, N, M)\)</span> to the ML prediction functino <span class="math notranslate nohighlight">\(f(\theta, \nu, N, M)\)</span> (which is trained to regress <span class="math notranslate nohighlight">\(Z\)</span>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_D</span><span class="p">(</span><span class="n">train_df</span><span class="p">):</span>
    <span class="n">Nmin</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;N&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
    <span class="n">Nmax</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;N&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">Mmin</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;M&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
    <span class="n">Mmax</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;M&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">D</span> <span class="o">=</span> <span class="p">[</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span> <span class="k">for</span> <span class="n">N</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Nmin</span><span class="p">,</span> <span class="n">Nmax</span><span class="p">)</span> <span class="k">for</span> <span class="n">M</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Mmin</span><span class="p">,</span> <span class="n">Mmax</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">D</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">make_D</span><span class="p">(</span><span class="n">train_df</span><span class="p">)</span>
<span class="n">D</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thetamin</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="n">thetamax</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;theta&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">plot_data_one_nu_with_model</span><span class="p">(</span><span class="n">Bprime</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">thetamin</span><span class="o">=</span><span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="o">=</span><span class="n">thetamax</span><span class="p">,</span> 
                 <span class="n">nu</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">D</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">NBINS</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">usemodel</span><span class="p">,</span> <span class="n">save_image</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_103_0.png" src="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_103_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_data_one_nu_with_model</span><span class="p">(</span><span class="n">Bprime</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">thetamin</span><span class="o">=</span><span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="o">=</span><span class="n">thetamax</span><span class="p">,</span> 
                 <span class="n">nu</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">D</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">NBINS</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">usemodel</span><span class="p">,</span> <span class="n">save_image</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_104_0.png" src="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_104_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_data_many_nus_with_model</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span> <span class="n">nu_list</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span>
                     <span class="n">NBINS</span><span class="p">,</span>
              <span class="n">FONTSIZE</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
              <span class="n">func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">fgsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">save_image</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    
    <span class="c1"># make room for 6 sub-plots</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                           <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                           <span class="n">figsize</span><span class="o">=</span><span class="n">fgsize</span><span class="p">)</span>
    
    <span class="n">outside</span><span class="o">=</span><span class="s1">&#39;&#39;</span>
    <span class="n">ALPHA</span><span class="o">=</span><span class="mf">0.8</span>
    <span class="n">TITLE_SIZE</span><span class="o">=</span><span class="n">font_legend</span><span class="o">+</span><span class="mi">1</span>
    
    <span class="c1"># padding</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="c1">#horizontal distance</span>
    
    <span class="c1"># use flatten() to convert a numpy array of </span>
    <span class="c1"># shape (nrows, ncols) to a 1-d array. </span>
    
    <span class="k">for</span> <span class="n">nu</span> <span class="ow">in</span> <span class="n">nu_list</span><span class="p">:</span>
        
        <span class="n">N</span><span class="p">,</span> <span class="n">M</span> <span class="o">=</span> <span class="n">D</span>
        <span class="n">y</span><span class="p">,</span> <span class="n">bb</span> <span class="o">=</span> <span class="n">make_hist_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span>
                              <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span>
                              <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
                              <span class="n">nbins</span><span class="o">=</span><span class="n">NBINS</span><span class="p">,</span>
                              <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    

        <span class="k">if</span> <span class="n">nu</span> <span class="o">&gt;</span> <span class="mi">20</span><span class="p">:</span>
            <span class="n">outside</span> <span class="o">=</span> <span class="n">outside</span> <span class="o">+</span> <span class="sa">r</span><span class="s1">&#39; ($&gt;$ train data)&#39;</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">bb</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">bb</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\nu= </span><span class="si">%s</span><span class="s1">$ </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">nu</span><span class="p">),</span> <span class="n">outside</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="n">ALPHA</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{h}</span><span class="s1">$ MLE&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">TITLE_SIZE</span><span class="p">)</span>
        <span class="c1">#h is histogram approximation</span>

        <span class="n">y_nonMLE</span><span class="p">,</span> <span class="n">bb_nonMLE</span> <span class="o">=</span> <span class="n">make_hist_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span>
                              <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span>
                              <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
                              <span class="n">nbins</span><span class="o">=</span><span class="n">NBINS</span><span class="p">,</span>
                              <span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
        
        <span class="n">x_nonMLE</span> <span class="o">=</span> <span class="p">(</span><span class="n">bb_nonMLE</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">bb_nonMLE</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>

        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_nonMLE</span><span class="p">,</span> <span class="n">y_nonMLE</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\nu= </span><span class="si">%s</span><span class="s1">$ </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">nu</span><span class="p">),</span> <span class="n">outside</span><span class="p">)</span> <span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="n">ALPHA</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{h}</span><span class="s1">$ non-MLE&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="n">TITLE_SIZE</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">func</span><span class="p">:</span>
            <span class="c1">#load the correct dataframe</span>
            <span class="n">train_df_MLE</span> <span class="o">=</span> <span class="n">load_train_df</span><span class="p">(</span><span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">train_df_nonMLE</span> <span class="o">=</span> <span class="n">load_train_df</span><span class="p">(</span><span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            
            <span class="n">f_MLE</span><span class="p">,</span> <span class="n">f_bins_MLE</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">train_df_MLE</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">nbins</span><span class="o">=</span><span class="n">NBINS</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f_MLE</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\nu= </span><span class="si">%s</span><span class="s1">$ </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">nu</span><span class="p">),</span> <span class="n">outside</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="n">ALPHA</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{f}</span><span class="s1">$ MLE&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="n">TITLE_SIZE</span><span class="p">)</span>
            <span class="c1">#f is model approximation</span>
            
            <span class="n">f_nonMLE</span><span class="p">,</span> <span class="n">f_bins_nonMLE</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">train_df_nonMLE</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">nbins</span><span class="o">=</span><span class="n">NBINS</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">f_bins_nonMLE</span><span class="p">,</span> <span class="n">f_nonMLE</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\nu= </span><span class="si">%s</span><span class="s1">$ </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">nu</span><span class="p">),</span> <span class="n">outside</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="n">ALPHA</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{f}</span><span class="s1">$ non-MLE&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="n">TITLE_SIZE</span><span class="p">)</span>
        
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">thetamin</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">thetamax</span><span class="o">-</span><span class="mi">7</span><span class="p">)</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.03</span><span class="p">)</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf{\theta}$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">FONTSIZE</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf{E(Z|\theta, \nu)}$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">FONTSIZE</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$N, M = </span><span class="si">%d</span><span class="s1">, </span><span class="si">%d</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">font_legend</span><span class="o">-</span><span class="mi">3</span>
                           <span class="c1"># fontsize=FONTSIZE</span>
                          <span class="p">)</span> 
                
                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>

                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;center right&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="n">font_legend</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">patch</span><span class="o">.</span><span class="n">set_edgecolor</span><span class="p">(</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>  

                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">patch</span><span class="o">.</span><span class="n">set_linewidth</span><span class="p">(</span><span class="s1">&#39;1&#39;</span><span class="p">)</span>  
        <span class="c1"># ax[j].text(3.1, 0.30, r&#39;$\nu = %5.1f$&#39; % nu, fontsize=font_legend-3</span>
        <span class="c1">#            # fontsize=FONTSIZE</span>
        <span class="c1">#           ) </span>

        
        
    <span class="c1"># hide unused sub-plots</span>
<span class="c1">#     for k in range(j+1, len(ax)):</span>
<span class="c1">#         ax[k].set_visible(False)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">save_image</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;images/h_MLE_nonMLE_f_MLE_f_nonMLE_many_nus.png&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">nu_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">]</span>
<span class="n">plot_data_many_nus_with_model</span><span class="p">(</span><span class="n">Bprime</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">thetamin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">thetamax</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">nu_list</span><span class="o">=</span><span class="n">nu_list</span><span class="p">,</span> <span class="n">D</span><span class="o">=</span><span class="n">D</span><span class="p">,</span>
                     <span class="n">NBINS</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
              <span class="n">FONTSIZE</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
              <span class="n">func</span><span class="o">=</span><span class="n">usemodel</span><span class="p">,</span>
              <span class="n">fgsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">save_image</span><span class="o">=</span><span class="kc">False</span>
                             <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_106_0.png" src="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_106_0.png" />
</div>
</div>
<hr class="docutils" />
<hr class="docutils" />
<p>What we observe here is that when <span class="math notranslate nohighlight">\(\hat{\theta}=\hat{\theta}_{\text{MLE}}\)</span>,our model simply reproduces the analytical result (<span class="math notranslate nohighlight">\(\mathbf{h}\)</span>), and when <span class="math notranslate nohighlight">\(\hat{\theta}=\hat{\theta}_{\text{non-MLE}}\)</span> our model arrives at a <span class="math notranslate nohighlight">\(\lambda\)</span> that has minimal sensitivity to <span class="math notranslate nohighlight">\(\nu\)</span>.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="learning-to-pivot">
<h1>Learning to Pivot<a class="headerlink" href="#learning-to-pivot" title="Permalink to this headline">#</a></h1>
<p>We observe in the plots above that in the case of MLE, <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> has very little sensitivity to <span class="math notranslate nohighlight">\(\nu\)</span>, as expected, whereas in the non-MLE case, <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> has a strong dependence on <span class="math notranslate nohighlight">\(\nu\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(\nu\)</span> is not known precisely from first principles, it is good that we see that our models <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> have given predictions that agree with the histogrammed predictions even in regions outside the region of values of the nuissance parameters used in the training. This is because systematic uncertainties are usually present when the training data is not representative of the real data. Therefore, statisticians have always been interested in forming robust inference techniques that are independent based on <em>pivots</em> - quantities whose distributions are independent of nuissance parameters.</p>
<p>The desired outcome is that we want to enforce <span class="math notranslate nohighlight">\(\mathbf{f}(\theta)\)</span> to be pivotal - i.e. for its distribution to be independent of <span class="math notranslate nohighlight">\(\nu\)</span>. In other words we would like a function <span class="math notranslate nohighlight">\(\lambda(\theta,\nu, N,M)\)</span> such that the expectation <span class="math notranslate nohighlight">\(E \big[ y=Z|x=\{\theta, \nu, N, M\} \big]\)</span> is independent of nuissance parameter <span class="math notranslate nohighlight">\(\nu\)</span>. One example is that we can impose this condition by</p>
<div class="math notranslate nohighlight">
\[\frac{\partial E \left[ Z \mid \theta,\nu,N,M \right] }{\partial \nu} =0\]</div>
<p>in our loss function for all <span class="math notranslate nohighlight">\(\theta, N,M\)</span>. This means our loss function will become</p>
<div class="math notranslate nohighlight">
\[L(t, \mathbf{f}) =  \big( y - \mathbf{f}(\theta, \nu, N, M) \big) ^2 - \frac{\kappa}{2} \ \left\| \frac{\partial \ \mathbf{f}(\theta,\nu,N,M)}{\partial \ \nu} \right\|^2\]</div>
<p>Where <span class="math notranslate nohighlight">\(\kappa\)</span> will be a constant and tunable hyperparameter. The operation to square the derivaive is because the derivative will be a vector (an array), so squaring it (or taking the dot product) will reduce it to a single scalar value.</p>
<p>Note that technically this will be the gradient not the derivative, since <span class="math notranslate nohighlight">\(\vec{\nu}\)</span> will be a vector and so we have to take the gradient of a vector with respect to the scalar <span class="math notranslate nohighlight">\(\vec{\mathbf{f}}\)</span></p>
<div class="math notranslate nohighlight">
\[L(\vec{t}, \vec{\mathbf{f}} ) =  \big( \vec{y} - \vec{\mathbf{f}}(\vec{\theta}, \vec{\nu}, \vec{N}, \vec{M}) \big) ^2 - \frac{\kappa}{2} \ \left\| \vec{\nabla}_{\vec{\nu}} \ \mathbf{f}(\vec{\theta},\vec{\nu},\vec{N},\vec{M}) \ \right\|^2\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>Z</th>
      <th>theta</th>
      <th>nu</th>
      <th>N</th>
      <th>M</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1</td>
      <td>3.805055</td>
      <td>9.797354</td>
      <td>5</td>
      <td>5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>15.074683</td>
      <td>0.951402</td>
      <td>8</td>
      <td>5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>1</td>
      <td>10.848134</td>
      <td>19.538657</td>
      <td>7</td>
      <td>7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>1</td>
      <td>4.156004</td>
      <td>14.831727</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>1</td>
      <td>8.898916</td>
      <td>12.550971</td>
      <td>6</td>
      <td>9</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_t</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">test_t</span><span class="p">,</span>  <span class="n">test_x</span> <span class="o">=</span> <span class="n">getwholedata</span><span class="p">(</span><span class="n">MLE_or_nonMLE</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">valid</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train_x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train_t shape =  (800000,) 

train_x shape =  (800000, 5) 

[[ 0.38995527 12.99773017  0.          3.          7.        ]
 [10.59882699  7.89974857  0.          3.          9.        ]
 [ 0.82995922  9.60260357  0.          5.          7.        ]
 ...
 [13.71780439  7.49373071  2.          4.          2.        ]
 [ 2.61903574  5.13160989  2.          6.          4.        ]
 [10.15214404 12.72985859  0.          3.          9.        ]]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\nu\)</span> is the second column in the features data</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_layers_pivot</span><span class="p">,</span> <span class="n">hidden_size_pivot</span><span class="p">,</span> <span class="n">dropout_pivot</span><span class="p">,</span> <span class="n">optimizer_name_pivot</span><span class="p">,</span> <span class="n">learning_rate_pivot</span><span class="p">,</span> <span class="n">batch_size_pivot</span><span class="p">,</span> <span class="n">model_nonMLE_pivot</span><span class="p">,</span> <span class="n">optimizer_nonMLE_pivot</span> <span class="o">=</span> <span class="n">initiate_whose_model</span><span class="p">(</span><span class="s1">&#39;Ali&#39;</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_nonMLE_pivot</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RegularizedRegressionModel(
  (model): Sequential(
    (0): Linear(in_features=5, out_features=11, bias=True)
    (1): Dropout(p=0.1320798105984151, inplace=False)
    (2): ReLU()
    (3): Linear(in_features=11, out_features=11, bias=True)
    (4): Dropout(p=0.1320798105984151, inplace=False)
    (5): ReLU()
    (6): Linear(in_features=11, out_features=11, bias=True)
    (7): Dropout(p=0.1320798105984151, inplace=False)
    (8): ReLU()
    (9): Linear(in_features=11, out_features=11, bias=True)
    (10): Dropout(p=0.1320798105984151, inplace=False)
    (11): ReLU()
    (12): Linear(in_features=11, out_features=1, bias=True)
    (13): Sigmoid()
  )
)
</pre></div>
</div>
</div>
</div>
<section id="aside-a-few-words-on-pytorch-s-autograd-more-on-this-in-https-github-com-alialkadhim-lfi-hep-blob-main-src-two-params-autograd2-ipynb">
<h2>Aside: a few words on pytorch’s autograd (more on this in <a class="reference external" href="https://github.com/AliAlkadhim/LFI_HEP/blob/main/src/Two_params/autograd2.ipynb">https://github.com/AliAlkadhim/LFI_HEP/blob/main/src/Two_params/autograd2.ipynb</a>)<a class="headerlink" href="#aside-a-few-words-on-pytorch-s-autograd-more-on-this-in-https-github-com-alialkadhim-lfi-hep-blob-main-src-two-params-autograd2-ipynb" title="Permalink to this headline">#</a></h2>
<p>for the example below, we have <span class="math notranslate nohighlight">\(F=a b\)</span> so <span class="math notranslate nohighlight">\(\frac{\partial F}{\partial a} = b = 20\)</span>. Let’s see if it works:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">20.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span> <span class="n">b</span>
<span class="n">F</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">a</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(20.)
</pre></div>
</div>
</div>
</div>
<p>Great, it works! Now let’s see if it works for tensors or arrays (non-scalars)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">10.</span><span class="p">,</span><span class="mf">10.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">20.</span><span class="p">,</span><span class="mf">20.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span> <span class="n">b</span>
<span class="n">F</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">a</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">RuntimeError</span><span class="g g-Whitespace">                              </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_52945</span><span class="o">/</span><span class="mf">3506644680.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">20.</span><span class="p">,</span><span class="mf">20.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="n">F</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span> <span class="n">b</span>
<span class="ne">----&gt; </span><span class="mi">4</span> <span class="n">F</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span>

<span class="nn">~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py</span> in <span class="ni">backward</span><span class="nt">(self, gradient, retain_graph, create_graph, inputs)</span>
<span class="g g-Whitespace">    </span><span class="mi">253</span>                 <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">254</span>                 <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">255</span>         <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">retain_graph</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">256</span> 
<span class="g g-Whitespace">    </span><span class="mi">257</span>     <span class="k">def</span> <span class="nf">register_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">):</span>

<span class="nn">~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py</span> in <span class="ni">backward</span><span class="nt">(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)</span>
<span class="g g-Whitespace">    </span><span class="mi">141</span> 
<span class="g g-Whitespace">    </span><span class="mi">142</span>     <span class="n">grad_tensors_</span> <span class="o">=</span> <span class="n">_tensor_or_tensors_to_tuple</span><span class="p">(</span><span class="n">grad_tensors</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">tensors</span><span class="p">))</span>
<span class="ne">--&gt; </span><span class="mi">143</span>     <span class="n">grad_tensors_</span> <span class="o">=</span> <span class="n">_make_grads</span><span class="p">(</span><span class="n">tensors</span><span class="p">,</span> <span class="n">grad_tensors_</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">144</span>     <span class="k">if</span> <span class="n">retain_graph</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">145</span>         <span class="n">retain_graph</span> <span class="o">=</span> <span class="n">create_graph</span>

<span class="nn">~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py</span> in <span class="ni">_make_grads</span><span class="nt">(outputs, grads)</span>
<span class="g g-Whitespace">     </span><span class="mi">48</span>             <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">49</span>                 <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">50</span>                     <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;grad can be implicitly created only for scalar outputs&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">51</span>                 <span class="n">new_grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">))</span>
<span class="g g-Whitespace">     </span><span class="mi">52</span>             <span class="k">else</span><span class="p">:</span>

<span class="ne">RuntimeError</span>: grad can be implicitly created only for scalar outputs
</pre></div>
</div>
</div>
</div>
<p>It doesn’t work. See <a class="reference external" href="https://abishekbashyall.medium.com/playing-with-backward-method-in-pytorch-bd34b58745a0">https://abishekbashyall.medium.com/playing-with-backward-method-in-pytorch-bd34b58745a0</a> if you want to learn more. We can work our way around it by passing a <code class="docutils literal notranslate"><span class="pre">gradient</span></code> argument to <code class="docutils literal notranslate"><span class="pre">backward()</span></code> with the same shape as <code class="docutils literal notranslate"><span class="pre">F</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">10.</span><span class="p">,</span><span class="mf">10.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">20.</span><span class="p">,</span><span class="mf">20.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span> <span class="n">b</span>
<span class="n">F</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">gradient</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">]))</span>
<span class="n">a</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([20., 20.])
</pre></div>
</div>
</div>
</div>
<p>ta-da! we can also do</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">10.</span><span class="p">,</span><span class="mf">10.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">20.</span><span class="p">,</span><span class="mf">20.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">a</span><span class="o">*</span> <span class="n">b</span>
<span class="n">F</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">gradient</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">a</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([20., 20.])
</pre></div>
</div>
</div>
</div>
<p>In our case <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(\nu\)</span> are leaf nodes and so their gradients will not be accumulated, and <span class="math notranslate nohighlight">\(f\)</span> is not a leaf node. We have to <code class="docutils literal notranslate"><span class="pre">retain_grad()</span></code> for non-leaf nodes, and <code class="docutils literal notranslate"><span class="pre">retain_graph()</span></code> for the model during the backward propagation, and then zero out the gradients that got accumulated</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">RMS</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">v</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span><span class="o">**</span><span class="mf">0.5</span>
    
<span class="k">def</span> <span class="nf">average_quadratic_loss_pivot</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">df_dnu</span><span class="p">):</span>
    <span class="n">kappa</span><span class="o">=</span><span class="mf">1.5</span>
    <span class="k">return</span>  <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">f</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">kappa</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span> <span class="n">RMS</span><span class="p">(</span><span class="n">df_dnu</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">avloss</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">df_dnu</span><span class="p">):</span>
    <span class="c1"># make sure we set evaluation mode so that any training specific</span>
    <span class="c1"># operations are disabled.</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="c1"># evaluation mode</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c1"># no need to compute gradients wrt. x and t</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="c1"># remember to reshape!</span>
        <span class="n">o</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">avloss</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">df_dnu</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_layers_pivot</span><span class="p">,</span> <span class="n">hidden_size_pivot</span><span class="p">,</span> <span class="n">dropout_pivot</span><span class="p">,</span> <span class="n">optimizer_name_pivot</span><span class="p">,</span> <span class="n">learning_rate_pivot</span><span class="p">,</span> <span class="n">batch_size_pivot</span><span class="p">,</span> <span class="n">model_nonMLE_pivot</span><span class="p">,</span> <span class="n">optimizer_nonMLE_pivot</span> <span class="o">=</span> <span class="n">initiate_whose_model</span><span class="p">(</span><span class="s1">&#39;Ali&#39;</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_nonMLE_pivot</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RegularizedRegressionModel(
  (model): Sequential(
    (0): Linear(in_features=5, out_features=11, bias=True)
    (1): Dropout(p=0.1320798105984151, inplace=False)
    (2): ReLU()
    (3): Linear(in_features=11, out_features=11, bias=True)
    (4): Dropout(p=0.1320798105984151, inplace=False)
    (5): ReLU()
    (6): Linear(in_features=11, out_features=11, bias=True)
    (7): Dropout(p=0.1320798105984151, inplace=False)
    (8): ReLU()
    (9): Linear(in_features=11, out_features=11, bias=True)
    (10): Dropout(p=0.1320798105984151, inplace=False)
    (11): ReLU()
    (12): Linear(in_features=11, out_features=1, bias=True)
    (13): Sigmoid()
  )
)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_pivotal</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">avloss</span><span class="p">,</span>
          <span class="n">batch_size</span><span class="p">,</span> 
          <span class="n">n_iterations</span><span class="p">,</span> <span class="n">traces</span><span class="p">,</span> 
          <span class="n">step</span><span class="p">,</span> <span class="n">window</span><span class="p">,</span> <span class="n">MLE</span><span class="p">):</span>
    
    <span class="c1"># to keep track of average losses</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy_t</span><span class="p">,</span> <span class="n">yy_v</span><span class="p">,</span> <span class="n">yy_v_avg</span> <span class="o">=</span> <span class="n">traces</span>
    
    
    <span class="k">if</span> <span class="n">MLE</span><span class="p">:</span>
        <span class="n">train_t</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">test_t</span><span class="p">,</span>  <span class="n">test_x</span> <span class="o">=</span> <span class="n">getwholedata</span><span class="p">(</span><span class="n">MLE_or_nonMLE</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">valid</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">train_t</span><span class="p">,</span> <span class="n">train_x</span><span class="p">,</span> <span class="n">test_t</span><span class="p">,</span>  <span class="n">test_x</span> <span class="o">=</span> <span class="n">getwholedata</span><span class="p">(</span><span class="n">MLE_or_nonMLE</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">valid</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_x</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration vs average loss&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%10s</span><span class="se">\t</span><span class="si">%10s</span><span class="se">\t</span><span class="si">%10s</span><span class="s2">&quot;</span> <span class="o">%</span> \
          <span class="p">(</span><span class="s1">&#39;iteration&#39;</span><span class="p">,</span> <span class="s1">&#39;train-set&#39;</span><span class="p">,</span> <span class="s1">&#39;valid-set&#39;</span><span class="p">))</span>
    
    <span class="c1"># training_set_features, training_set_targets, evaluation_set_features, evaluation_set_targets = get_data_sets(simulate_data=False, batchsize=batch_size)</span>
    
    <span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>

        <span class="c1"># set mode to training so that training specific </span>
        <span class="c1"># operations such as dropout are enabled.</span>

        
        <span class="c1"># model.eval()</span>
        <span class="c1"># get a random sample (a batch) of data (as numpy arrays)</span>
        
        <span class="c1">#Harrison-like Loader</span>
        <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_t</span> <span class="o">=</span> <span class="n">get_features_training_batch</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_t</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        
        <span class="c1">#Or Ali&#39;s Loader</span>
        <span class="c1"># batch_x, batch_t = next(training_set_features()), next(training_set_targets())</span>
        <span class="c1"># batch_x_eval, batch_t_eval = next(evaluation_set_features()), next(evaluation_set_targets())</span>

        <span class="c1"># with torch.no_grad(): # no need to compute gradients </span>
            <span class="c1"># wrt. x and t</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="c1"># print(&#39;x is leaf: &#39;, x.is_leaf)</span>
        <span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># x.retain_grad()</span>
        <span class="c1"># print(&#39;x is leaf after retain: &#39;, x.is_leaf)</span>
        <span class="c1"># x.requires_grad_(True)</span>
        <span class="c1"># x.retain_grad()</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1">#multiply the model by its ransverse, remember we can only take gradients of scalars</span>
        <span class="c1">#and f will be a vector before this</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">f</span> <span class="o">@</span> <span class="n">f</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="c1"># f = torch.tensor(f, requires_grad=True)</span>
        <span class="c1"># print(&#39;f shape: &#39;, f.shape)</span>
        <span class="c1"># print(&#39;f is leaf: &#39;, f.is_leaf)</span>
        
        <span class="c1"># f_2 = f**2</span>
        <span class="c1"># print(&#39;f2 shape&#39;, f_2.shape)</span>
        <span class="c1"># nu = torch.autograd.Variable( x[:,1], requires_grad=True)</span>
        
        <span class="c1"># nu=torch.autograd.Variable(x[:,1], requires_grad=True)</span>
        <span class="n">nu</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># print(type(nu))</span>
 
        <span class="c1"># nu.retain_grad()</span>
        
        <span class="c1"># print(&#39;nu shape: &#39;, nu.shape)</span>
        <span class="c1"># print(&#39;nu is leaf: &#39;, nu.is_leaf)</span>
        <span class="c1"># print(&#39;nu type&#39;, type(nu))</span>
        
        
        <span class="c1"># WE NEED TO RETAIN_GRAD ON NON-LEAF NODES </span>
        <span class="n">f</span><span class="o">.</span><span class="n">retain_grad</span><span class="p">()</span>
        <span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">gradient</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">f</span><span class="p">),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1">#or f.backward(torch.ones_like(f) , retain, graph</span>
        <span class="c1"># we cant do f.backward() here because backeard() can only be called for scalara</span>
        <span class="c1"># and f here will be a tensor of shape torch.Size([1000, 1]</span>
        <span class="c1">#e.g. see https://abishekbashyall.medium.com/playing-with-backward-method-in-pytorch-bd34b58745a0</span>
        
        <span class="c1"># PyTorch accumulates the gradient in default, we need to clear the previous</span>
        <span class="c1"># values</span>
        <span class="c1"># df_dnu = nu.grad</span>
        <span class="n">df_dx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span>
        <span class="c1"># print(&#39;df_dnu =&#39;, df_dnu)</span>
        <span class="c1"># print(&#39;df_dx =&#39;, df_dx)</span>
        <span class="c1"># print(&#39;df_dx shape :&#39;, df_dx.shape)</span>
        <span class="c1">#clear the gradient after you take it</span>
        <span class="n">df_dnu</span> <span class="o">=</span> <span class="n">df_dx</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># print(&#39;df_dnu shape: &#39;, df_dnu.shape)</span>
        <span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="c1"># break</span>
        
        
        
        
        
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">batch_t</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>   
        
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        

        

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
   
        <span class="c1"># compute a noisy approximation to the average loss</span>
        <span class="n">empirical_risk</span> <span class="o">=</span> <span class="n">avloss</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">df_dnu</span><span class="p">)</span>
        
        <span class="c1"># use automatic differentiation to compute a </span>
        <span class="c1"># noisy approximation of the local gradient</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>       <span class="c1"># clear previous gradients</span>
        <span class="n">empirical_risk</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>   <span class="c1"># compute gradients</span>
        
        <span class="c1"># finally, advance one step in the direction of steepest </span>
        <span class="c1"># descent, using the noisy local gradient. </span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>            <span class="c1"># move one step</span>
        
        <span class="k">if</span> <span class="n">ii</span> <span class="o">%</span> <span class="n">step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            
            <span class="c1"># this is an example of an x tensor</span>
            <span class="c1"># [17.3352, 10.7722,  6.0000,  8.0000],</span>
            <span class="c1">#[16.7822, 13.3260,  8.0000,  4.0000],</span>
            <span class="c1">#using Harrison-like loader</span>
            <span class="n">acc_t</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">avloss</span><span class="p">,</span> <span class="n">train_x</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> <span class="n">train_t</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> <span class="n">df_dnu</span><span class="p">)</span>
            <span class="n">acc_v</span> <span class="o">=</span> <span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">avloss</span><span class="p">,</span> <span class="n">test_x</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> <span class="n">test_t</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span> <span class="n">df_dnu</span><span class="p">)</span>
            
            <span class="c1">#using Ali&#39;s loader</span>
            <span class="c1"># acc_t = validate(model, avloss, batch_x, batch_t) </span>
            <span class="c1"># acc_v = validate(model, avloss, batch_x_eval, batch_t_eval)</span>
            

            <span class="n">yy_t</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_t</span><span class="p">)</span>
            <span class="n">yy_v</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_v</span><span class="p">)</span>
            
            <span class="c1"># compute running average for validation data</span>
            <span class="n">len_yy_v</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">yy_v</span><span class="p">)</span>
            <span class="k">if</span>   <span class="n">len_yy_v</span> <span class="o">&lt;</span> <span class="n">window</span><span class="p">:</span>
                <span class="n">yy_v_avg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">yy_v</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="p">)</span>
            <span class="k">elif</span> <span class="n">len_yy_v</span> <span class="o">==</span> <span class="n">window</span><span class="p">:</span>
                <span class="n">yy_v_avg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="nb">sum</span><span class="p">(</span><span class="n">yy_v</span><span class="p">)</span> <span class="o">/</span> <span class="n">window</span> <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">acc_v_avg</span>  <span class="o">=</span> <span class="n">yy_v_avg</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">window</span>
                <span class="n">acc_v_avg</span> <span class="o">+=</span> <span class="n">yy_v</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">yy_v</span><span class="p">[</span><span class="o">-</span><span class="n">window</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">yy_v_avg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc_v_avg</span> <span class="o">/</span> <span class="n">window</span><span class="p">)</span>
                        
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">xx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%10d</span><span class="se">\t</span><span class="si">%10.6f</span><span class="se">\t</span><span class="si">%10.6f</span><span class="s2">&quot;</span> <span class="o">%</span> \
                      <span class="p">(</span><span class="n">xx</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yy_t</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yy_v</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">xx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xx</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">step</span><span class="p">)</span>
                    
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\r</span><span class="si">%10d</span><span class="se">\t</span><span class="si">%10.6f</span><span class="se">\t</span><span class="si">%10.6f</span><span class="se">\t</span><span class="si">%10.6f</span><span class="s2">&quot;</span> <span class="o">%</span> \
                          <span class="p">(</span><span class="n">xx</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yy_t</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yy_v</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">yy_v_avg</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> 
                      <span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
            
    <span class="nb">print</span><span class="p">()</span>      
    <span class="k">return</span> <span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy_t</span><span class="p">,</span> <span class="n">yy_v</span><span class="p">,</span> <span class="n">yy_v_avg</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">BATCHSIZE</span><span class="o">=</span><span class="n">batch_size</span>
<span class="n">traces_nonMLE_pivot</span> <span class="o">=</span> <span class="p">([],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[])</span>
<span class="n">traces_step</span> <span class="o">=</span> <span class="mi">400</span>


<span class="n">n_iterations</span><span class="o">=</span><span class="mi">200000</span>
<span class="n">traces_nonMLE_pivot</span><span class="o">=</span> <span class="n">train_pivotal</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_nonMLE_pivot</span><span class="p">,</span> 
              <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer_nonMLE_pivot</span><span class="p">,</span> 
              <span class="n">avloss</span><span class="o">=</span><span class="n">average_quadratic_loss_pivot</span><span class="p">,</span>
              <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCHSIZE</span><span class="p">,</span> 
              <span class="n">n_iterations</span><span class="o">=</span><span class="n">n_iterations</span><span class="p">,</span> 
              <span class="n">traces</span><span class="o">=</span><span class="n">traces_nonMLE_pivot</span><span class="p">,</span> 
              <span class="n">step</span><span class="o">=</span><span class="n">traces_step</span><span class="p">,</span> 
              <span class="n">window</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span>
                <span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train_t shape =  (800000,) 

train_x shape =  (800000, 5) 

Iteration vs average loss
 iteration	 train-set	 valid-set
         0	  0.059070	  0.059300
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>    199600	  0.060305	  0.060473	  0.059433
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">PATH</span><span class="o">=</span><span class="s1">&#39;models/PIVOT_MLE_False_Regressor_</span><span class="si">%s</span><span class="s1">K_training_iter_Kappa_1_5_tuned_with_thetahat.pt&#39;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">n_iterations</span><span class="o">/</span><span class="mi">1000</span><span class="p">))</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_nonMLE_pivot</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>  <span class="n">PATH</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">usemodel_with_pivot</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">train_df</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span><span class="n">M</span><span class="p">,</span> <span class="n">MLE_or_pivot</span><span class="p">,</span> <span class="n">nbins</span><span class="p">,</span> <span class="n">observe_pivot</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    
    <span class="c1">#Generate evaluation data at those fixed nu, N, M values</span>
    <span class="n">eval_data</span><span class="p">,</span> <span class="n">eval_bins</span> <span class="o">=</span><span class="n">make_eval_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span><span class="n">train_df</span><span class="p">,</span><span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span><span class="n">M</span><span class="p">,</span> <span class="n">nbins</span><span class="p">)</span><span class="c1">#eval data is indipendent of MLE, since its just constants witha theta variable</span>
    <span class="c1"># if MLE==True:</span>
    <span class="c1">#     model=model</span>
    <span class="c1">#else load the model trained on non-MLE data</span>
    <span class="c1"># PATH=&#39;models/MLE_TRUE_Regressor_200.0K_training_iter.pt&#39;</span>
    
    <span class="c1">#LOAD TRAINED MODEL</span>
    <span class="k">if</span> <span class="n">MLE_or_pivot</span><span class="o">==</span><span class="s1">&#39;MLE&#39;</span><span class="p">:</span>
        <span class="n">PATH</span><span class="o">=</span> <span class="s1">&#39;models/MLE_True_Regressor_100.0K_training_iter_with_theta_hat.pt&#39;</span>
    <span class="k">elif</span> <span class="n">MLE_or_pivot</span><span class="o">==</span><span class="s1">&#39;nonMLE&#39;</span><span class="p">:</span>
        <span class="n">PATH</span><span class="o">=</span> <span class="s1">&#39;models/MLE_False_Regressor_100.0K_training_iter_with_theta_hat.pt&#39;</span>
    <span class="c1">#both MLE and nonMLE models trained with theta_hat as an additional feature</span>
    <span class="k">elif</span> <span class="n">MLE_or_pivot</span><span class="o">==</span><span class="s1">&#39;PIVOT&#39;</span><span class="p">:</span>
        <span class="n">PATH</span><span class="o">=</span><span class="s1">&#39;models/PIVOT_MLE_False_Regressor_200.0K_training_iter_Kappa_2_tuned_byhand.pt&#39;</span>
        <span class="c1"># PATH=&#39;models/PIVOT_MLE_False_Regressor_200K_training_iter_Kappa_1_5_tuned_with_thetahat.pt&#39;</span>
    <span class="n">n_layers</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;n_layers&quot;</span><span class="p">])</span> 
    <span class="n">hidden_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">])</span>
    <span class="n">dropout</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;dropout&quot;</span><span class="p">])</span>
    <span class="n">optimizer_name</span> <span class="o">=</span> <span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;optimizer_name&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_string</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">learning_rate</span> <span class="o">=</span>  <span class="nb">float</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">])</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">BEST_PARAMS</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">])</span>
    <span class="n">model</span> <span class="o">=</span>  <span class="n">RegularizedRegressionModel</span><span class="p">(</span>
        <span class="n">nfeatures</span><span class="o">=</span><span class="n">sample_x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> 
        <span class="n">ntargets</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">nlayers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> 
        <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> 
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span>
        <span class="p">)</span>
    <span class="c1">#EVALUATE AT AT EVAL_DATA</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">)</span> <span class="p">)</span>
    <span class="k">if</span> <span class="n">observe_pivot</span><span class="p">:</span>
        <span class="c1">#this boolean is if you just want to observe the current pivotal model which is not saved</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_nonMLE_pivot</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">(</span><span class="n">eval_data</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">eval_bins</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_average_loss</span><span class="p">(</span><span class="n">traces_nonMLE_pivot</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_data_many_nus_with_model_pivot</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span> <span class="n">nu_list</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span>
                     <span class="n">NBINS</span><span class="p">,</span>
              <span class="n">FONTSIZE</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
              <span class="n">func</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
              <span class="n">fgsize</span><span class="o">=</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">22</span><span class="p">),</span> <span class="n">save_image</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    
    <span class="c1"># make room for 6 sub-plots</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                           <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
                           <span class="n">figsize</span><span class="o">=</span><span class="n">fgsize</span><span class="p">)</span>
    
    <span class="n">outside</span><span class="o">=</span><span class="s1">&#39;&#39;</span>
    <span class="n">ALPHA</span><span class="o">=</span><span class="mf">0.8</span>
    <span class="n">TITLE_SIZE</span><span class="o">=</span><span class="n">font_legend</span><span class="o">+</span><span class="mi">1</span>
    
    <span class="c1"># padding</span>
    <span class="c1"># plt.subplots_adjust(hspace=3)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="c1">#horizontal distance</span>
    
    <span class="c1"># use flatten() to convert a numpy array of </span>
    <span class="c1"># shape (nrows, ncols) to a 1-d array. </span>
    
    <span class="k">for</span> <span class="n">nu</span> <span class="ow">in</span> <span class="n">nu_list</span><span class="p">:</span>
        
        <span class="n">N</span><span class="p">,</span> <span class="n">M</span> <span class="o">=</span> <span class="n">D</span>
        <span class="n">y</span><span class="p">,</span> <span class="n">bb</span> <span class="o">=</span> <span class="n">make_hist_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span>
                              <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span>
                              <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
                              <span class="n">nbins</span><span class="o">=</span><span class="n">NBINS</span><span class="p">,</span>
                              <span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    

        <span class="k">if</span> <span class="n">nu</span> <span class="o">&gt;</span> <span class="mi">20</span><span class="p">:</span>
            <span class="n">outside</span> <span class="o">=</span>  <span class="sa">r</span><span class="s1">&#39; ($&gt;$ train data)&#39;</span>
            
        
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">bb</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">bb</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\nu= </span><span class="si">%s</span><span class="s1">$ </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">nu</span><span class="p">),</span> <span class="n">outside</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="n">ALPHA</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{h}</span><span class="s1">$ MLE&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">TITLE_SIZE</span><span class="p">)</span>
        <span class="c1">#h is histogram approximation</span>

        <span class="n">y_nonMLE</span><span class="p">,</span> <span class="n">bb_nonMLE</span> <span class="o">=</span> <span class="n">make_hist_data</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span>
                              <span class="n">thetamin</span><span class="p">,</span> <span class="n">thetamax</span><span class="p">,</span>
                              <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
                              <span class="n">nbins</span><span class="o">=</span><span class="n">NBINS</span><span class="p">,</span>
                              <span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        
        
        <span class="n">x_nonMLE</span> <span class="o">=</span> <span class="p">(</span><span class="n">bb_nonMLE</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">bb_nonMLE</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>

        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_nonMLE</span><span class="p">,</span> <span class="n">y_nonMLE</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\nu= </span><span class="si">%s</span><span class="s1">$ </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">nu</span><span class="p">),</span> <span class="n">outside</span><span class="p">)</span> <span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="n">ALPHA</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{h}</span><span class="s1">$ non-MLE&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="n">TITLE_SIZE</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">func</span><span class="p">:</span>
            <span class="c1">#load the correct dataframe</span>
            <span class="n">train_df_MLE</span> <span class="o">=</span> <span class="n">load_train_df</span><span class="p">(</span><span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">train_df_nonMLE</span> <span class="o">=</span> <span class="n">load_train_df</span><span class="p">(</span><span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            
            
            <span class="n">OBSERVE_PIVOT</span><span class="o">=</span><span class="kc">False</span>
            <span class="c1">#plot MLE models</span>
            <span class="n">f_MLE</span><span class="p">,</span> <span class="n">f_bins_MLE</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">train_df_MLE</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">MLE_or_pivot</span><span class="o">=</span><span class="s1">&#39;MLE&#39;</span><span class="p">,</span> <span class="n">nbins</span><span class="o">=</span><span class="n">NBINS</span><span class="p">,</span><span class="n">observe_pivot</span><span class="o">=</span><span class="n">OBSERVE_PIVOT</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">f_bins_MLE</span><span class="p">,</span> <span class="n">f_MLE</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\nu= </span><span class="si">%s</span><span class="s1">$ </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">nu</span><span class="p">),</span> <span class="n">outside</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="n">ALPHA</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{f}</span><span class="s1">$ MLE&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="n">TITLE_SIZE</span><span class="p">)</span>
            <span class="c1">#f is model approximation</span>
            
            <span class="c1">#plot non-MLE models</span>
            <span class="n">f_nonMLE</span><span class="p">,</span> <span class="n">f_bins_nonMLE</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">train_df_nonMLE</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">MLE_or_pivot</span><span class="o">=</span><span class="s1">&#39;nonMLE&#39;</span><span class="p">,</span> <span class="n">nbins</span><span class="o">=</span><span class="n">NBINS</span><span class="p">,</span> <span class="n">observe_pivot</span><span class="o">=</span><span class="n">OBSERVE_PIVOT</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">f_bins_nonMLE</span><span class="p">,</span> <span class="n">f_nonMLE</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\nu= </span><span class="si">%s</span><span class="s1">$ </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">nu</span><span class="p">),</span> <span class="n">outside</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="n">ALPHA</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{f}</span><span class="s1">$ non-MLE&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="n">TITLE_SIZE</span><span class="p">)</span>
            
            
            <span class="n">OBSERVE_PIVOT</span><span class="o">=</span><span class="kc">False</span>
            <span class="c1">#plot pivotal  MLE models</span>
            <span class="n">f_pivot_MLE</span><span class="p">,</span> <span class="n">f_bins_pivot_MLE</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">train_df_MLE</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">MLE_or_pivot</span><span class="o">=</span><span class="s1">&#39;PIVOT&#39;</span><span class="p">,</span> <span class="n">nbins</span><span class="o">=</span><span class="n">NBINS</span><span class="p">,</span> <span class="n">observe_pivot</span><span class="o">=</span><span class="n">OBSERVE_PIVOT</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">f_bins_pivot_MLE</span><span class="p">,</span> <span class="n">f_pivot_MLE</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\nu= </span><span class="si">%s</span><span class="s1">$ </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">nu</span><span class="p">),</span> <span class="n">outside</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="n">ALPHA</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf{f-pivot}$ MLE&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="n">TITLE_SIZE</span><span class="p">)</span>
            
            <span class="c1">#plot pivotal non-MLE models</span>
            <span class="n">f_pivot_nonMLE</span><span class="p">,</span> <span class="n">f_bins_pivot_nonMLE</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span> <span class="n">train_df_nonMLE</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">MLE_or_pivot</span><span class="o">=</span><span class="s1">&#39;PIVOT&#39;</span><span class="p">,</span> <span class="n">nbins</span><span class="o">=</span><span class="n">NBINS</span><span class="p">,</span> <span class="n">observe_pivot</span><span class="o">=</span><span class="n">OBSERVE_PIVOT</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">f_bins_pivot_nonMLE</span><span class="p">,</span> <span class="n">f_pivot_nonMLE</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\nu= </span><span class="si">%s</span><span class="s1">$ </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">nu</span><span class="p">),</span> <span class="n">outside</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="n">ALPHA</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf{f-pivot}$ non-MLE&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="n">TITLE_SIZE</span><span class="p">)</span>
        
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">thetamin</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">thetamax</span><span class="o">-</span><span class="mi">7</span><span class="p">)</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.03</span><span class="p">)</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf{\theta}$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">FONTSIZE</span><span class="p">)</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbf{E(Z|\theta, \nu)}$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">FONTSIZE</span><span class="p">)</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.38</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$N, M = </span><span class="si">%d</span><span class="s1">, </span><span class="si">%d</span><span class="s1">$&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">font_legend</span><span class="o">-</span><span class="mi">3</span>
                           <span class="c1"># fontsize=FONTSIZE</span>
                          <span class="p">)</span> 
                
                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>

                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;center right&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="n">font_legend</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">patch</span><span class="o">.</span><span class="n">set_edgecolor</span><span class="p">(</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>  

                <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">patch</span><span class="o">.</span><span class="n">set_linewidth</span><span class="p">(</span><span class="s1">&#39;1&#39;</span><span class="p">)</span>  
        <span class="c1"># ax[j].text(3.1, 0.30, r&#39;$\nu = %5.1f$&#39; % nu, fontsize=font_legend-3</span>
        <span class="c1">#            # fontsize=FONTSIZE</span>
        <span class="c1">#           ) </span>

        
        
    <span class="c1"># hide unused sub-plots</span>
<span class="c1">#     for k in range(j+1, len(ax)):</span>
<span class="c1">#         ax[k].set_visible(False)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">save_image</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;images/h_MLE_nonMLE_f_MLE_f_nonMLE_many_nus_PIVOT_TUNED_byhand_kappa2.png&#39;</span><span class="p">)</span>
           <span class="c1">#kappa=2  and loss = torch.mean((f - t)**2) - kappa/2 * RMS(df_dnu) and MLE=False</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_df_MLE</span> <span class="o">=</span> <span class="n">load_train_df</span><span class="p">(</span><span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">f_MLE</span><span class="p">,</span> <span class="n">f_bins_MLE</span> <span class="o">=</span> <span class="n">usemodel_with_pivot</span><span class="p">(</span><span class="n">Bprime</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">train_df</span><span class="o">=</span><span class="n">train_df_MLE</span><span class="p">,</span> 
                                        <span class="n">nu</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">MLE_or_pivot</span><span class="o">=</span><span class="s1">&#39;MLE&#39;</span><span class="p">,</span> 
                                        <span class="n">nbins</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span><span class="n">observe_pivot</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f_MLE</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f_bins_MLE</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_527713</span><span class="o">/</span><span class="mf">1294824035.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">train_df_MLE</span> <span class="o">=</span> <span class="n">load_train_df</span><span class="p">(</span><span class="n">MLE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">f_MLE</span><span class="p">,</span> <span class="n">f_bins_MLE</span> <span class="o">=</span> <span class="n">usemodel_with_pivot</span><span class="p">(</span><span class="n">Bprime</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">train_df</span><span class="o">=</span><span class="n">train_df_MLE</span><span class="p">,</span> 
<span class="g g-Whitespace">      </span><span class="mi">3</span>                                         <span class="n">nu</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">MLE_or_pivot</span><span class="o">=</span><span class="s1">&#39;MLE&#39;</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span>                                         <span class="n">nbins</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span><span class="n">observe_pivot</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="nb">print</span><span class="p">(</span><span class="n">f_MLE</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="ne">NameError</span>: name &#39;load_train_df&#39; is not defined
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="load-the-results-image-of-the-pivotal-model-so-far">
<h1>Load the results image of the pivotal model (so far)<a class="headerlink" href="#load-the-results-image-of-the-pivotal-model-so-far" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="s1">&#39;images/h_MLE_nonMLE_f_MLE_f_nonMLE_many_nus_PIVOT_TUNED_byhand_kappa2.png&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_131_0.png" src="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_131_0.png" />
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="explore-the-new-pivotal-models-and-save-if-good">
<h1>Explore the new pivotal models and save if good<a class="headerlink" href="#explore-the-new-pivotal-models-and-save-if-good" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">D</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">nu_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">300</span><span class="p">]</span>
<span class="n">plot_data_many_nus_with_model_pivot</span><span class="p">(</span><span class="n">Bprime</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">thetamin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">thetamax</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">nu_list</span><span class="o">=</span><span class="n">nu_list</span><span class="p">,</span> <span class="n">D</span><span class="o">=</span><span class="n">D</span><span class="p">,</span>
                     <span class="n">NBINS</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
              <span class="n">FONTSIZE</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
              <span class="n">func</span><span class="o">=</span><span class="n">usemodel_with_pivot</span><span class="p">,</span>
              <span class="n">fgsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="n">save_image</span><span class="o">=</span><span class="kc">False</span>
                             <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_527713</span><span class="o">/</span><span class="mf">2261253459.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">D</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">nu_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">300</span><span class="p">]</span>
<span class="ne">----&gt; </span><span class="mi">3</span> <span class="n">plot_data_many_nus_with_model_pivot</span><span class="p">(</span><span class="n">Bprime</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">thetamin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">thetamax</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">nu_list</span><span class="o">=</span><span class="n">nu_list</span><span class="p">,</span> <span class="n">D</span><span class="o">=</span><span class="n">D</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span>                      <span class="n">NBINS</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span>               <span class="n">FONTSIZE</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>

<span class="ne">NameError</span>: name &#39;plot_data_many_nus_with_model_pivot&#39; is not defined
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\mathbf{f}\)</span> MLE and <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> non-MLE show little dependence on whether or not they were trained with <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> as an additional feature.</p>
<p><span class="math notranslate nohighlight">\(\mathbf{f}-\text{pivot}\)</span> MLE and <span class="math notranslate nohighlight">\(\mathbf{f}-\text{pivot}\)</span> non-MLE perform better when they are <em>not</em> trained with  <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> as an additional feature (but still trained with everything else with the correct MLE/nonMLE values)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># PATH=&#39;models/PIVOT_MLE_False_Regressor_%sK_training_iter_Kappa_2_tuned_byhand.pt&#39; % str(n_iterations/1000)</span>
<span class="c1"># torch.save(model_nonMLE_pivot.state_dict(),  PATH)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">phat_PIVOT</span><span class="p">,</span> <span class="n">phatbins_PIVOT</span> <span class="o">=</span> <span class="n">usemodel_with_pivot</span><span class="p">(</span><span class="n">Bprime</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span><span class="n">train_df</span><span class="o">=</span><span class="n">train_df</span><span class="p">,</span><span class="n">nu</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                                                 <span class="n">M</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">MLE_or_pivot</span><span class="o">=</span><span class="s1">&#39;PIVOT&#39;</span><span class="p">,</span> <span class="n">nbins</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">observe_pivot</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">phat_PIVOT</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">phatbins_PIVOT</span><span class="p">,</span> <span class="n">phat_PIVOT</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$\mathbf</span><span class="si">{f}</span><span class="s1">$ PIVOT Example&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.20858869]
 [0.21112828]
 [0.33355924]
 [0.36123356]
 [0.39201   ]]
</pre></div>
</div>
<img alt="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_136_1.png" src="_images/2_Two_Parameter_Problem_and_Pivotal_p_Value_136_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_layers_pivot</span><span class="p">,</span> <span class="n">hidden_size_pivot</span><span class="p">,</span> <span class="n">dropout_pivot</span><span class="p">,</span> <span class="n">optimizer_name_pivot</span><span class="p">,</span> <span class="n">learning_rate_pivot</span><span class="p">,</span> <span class="n">batch_size_pivot</span><span class="p">,</span> <span class="n">model_nonMLE_pivot</span><span class="p">,</span> <span class="n">optimizer_nonMLE_pivot</span> <span class="o">=</span> <span class="n">initiate_whose_model</span><span class="p">(</span><span class="s1">&#39;Ali&#39;</span><span class="p">,</span> <span class="n">MLE</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model_nonMLE_pivot</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RegularizedRegressionModel(
  (model): Sequential(
    (0): Linear(in_features=4, out_features=11, bias=True)
    (1): Dropout(p=0.1320798105984151, inplace=False)
    (2): ReLU()
    (3): Linear(in_features=11, out_features=11, bias=True)
    (4): Dropout(p=0.1320798105984151, inplace=False)
    (5): ReLU()
    (6): Linear(in_features=11, out_features=11, bias=True)
    (7): Dropout(p=0.1320798105984151, inplace=False)
    (8): ReLU()
    (9): Linear(in_features=11, out_features=11, bias=True)
    (10): Dropout(p=0.1320798105984151, inplace=False)
    (11): ReLU()
    (12): Linear(in_features=11, out_features=1, bias=True)
    (13): Sigmoid()
  )
)
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="try-to-automatically-tune-kappa-under-construction">
<h1>Try to automatically tune <span class="math notranslate nohighlight">\(\kappa\)</span> (under construction)<a class="headerlink" href="#try-to-automatically-tune-kappa-under-construction" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Modify the tuning workflow to tune $\kappa$ in our pivotal loss function</span>

<span class="c1"># ### Retune ALL parameters of our model for the pivotal case - they might not converge o the same values as earlier!</span>

<span class="c1"># def average_quadratic_loss_pivot(f, t, x, df_dnu, kappa):</span>
<span class="c1">#     # f and t must be of the same shape</span>
<span class="c1">#     return  torch.mean((f - t)**2) - kappa/2 * torch.mean(df_dnu)</span>

<span class="c1"># def validate_pivot(model, avloss, inputs, targets, df_dnu):</span>
<span class="c1">#     # make sure we set evaluation mode so that any training specific</span>
<span class="c1">#     # operations are disabled.</span>
<span class="c1">#     model.eval() # evaluation mode</span>
    
<span class="c1">#     with torch.no_grad(): # no need to compute gradients wrt. x and t</span>
<span class="c1">#         x = torch.from_numpy(inputs).float()</span>
<span class="c1">#         t = torch.from_numpy(targets).float()</span>
<span class="c1">#         # remember to reshape!</span>
<span class="c1">#         o = model(x).reshape(t.shape)</span>
<span class="c1">#     return avloss(o, t, x, df_dnu, kappa)</span>

<span class="c1"># # we dont want to tune the old parameters so just load them</span>
<span class="c1"># n_layers = int(BEST_PARAMS[&quot;n_layers&quot;]) </span>
<span class="c1"># hidden_size = int(BEST_PARAMS[&quot;hidden_size&quot;])</span>
<span class="c1"># dropout = float(BEST_PARAMS[&quot;dropout&quot;])</span>
<span class="c1"># optimizer_name = BEST_PARAMS[&quot;optimizer_name&quot;].to_string().split()[1]</span>
<span class="c1"># learning_rate =  float(BEST_PARAMS[&quot;learning_rate&quot;])</span>
<span class="c1"># # batch_size = int(BEST_PARAMS[&quot;batch_size&quot;])</span>
<span class="c1"># batch_size=32</span>
    
<span class="c1"># class Engine_kappa:</span>
<span class="c1">#     &quot;&quot;&quot;loss, training and evaluation&quot;&quot;&quot;</span>
<span class="c1">#     def __init__(self, model, optimizer, batch_size, kappa):</span>
<span class="c1">#                  #, device):</span>
<span class="c1">#         self.model = model</span>
<span class="c1">#         #self.device= device</span>
<span class="c1">#         self.optimizer = optimizer</span>
<span class="c1">#         self.batch_size=batch_size</span>
<span class="c1">#         self.kappa=kappa</span>
        
<span class="c1">#     #the loss function returns the loss function. It is a static method so it doesn&#39;t need self</span>
<span class="c1">#     # @staticmethod</span>
<span class="c1">#     # def loss_fun(targets, outputs):</span>
<span class="c1">#     #   tau = torch.rand(outputs.shape)</span>
<span class="c1">#     #   return torch.mean(torch.where(targets &gt;= outputs, </span>
<span class="c1">#     #                                   tau * (targets - outputs), </span>
<span class="c1">#     #                                   (1 - tau)*(outputs - targets)))</span>

<span class="c1"># #     This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, </span>
<span class="c1"># #     by combining the operations into one layer</span>

<span class="c1">#     def train_pivot(self, x, t):</span>
<span class="c1">#         &quot;&quot;&quot;the training function: takes the training dataloader&quot;&quot;&quot;</span>
        
<span class="c1">#         final_loss = 0</span>
<span class="c1">#         for iteration in range(n_iterations):</span>

<span class="c1">#             batch_x, batch_t = get_features_training_batch(x, t,  self.batch_size)#x and t are train_x and train_t</span>
            

            
<span class="c1">#             x = torch.from_numpy(batch_x).float()</span>
<span class="c1">#             # print(&#39;x is leaf: &#39;, x.is_leaf)</span>
<span class="c1">#             x.requires_grad_(True)</span>
<span class="c1">#             # x.retain_grad()</span>
<span class="c1">#             # print(&#39;x is leaf after retain: &#39;, x.is_leaf)</span>
<span class="c1">#             # x.requires_grad_(True)</span>
<span class="c1">#             # x.retain_grad()</span>
<span class="c1">#             f = self.model(x)</span>
<span class="c1">#             f = f.view(-1)</span>
<span class="c1">#             #multiply the model by its ransverse, remember we can only take gradients of scalars</span>
<span class="c1">#             #and f will be a vector before this</span>
<span class="c1">#             f = f @ f.t()</span>
<span class="c1">#             # f = torch.tensor(f, requires_grad=True)</span>
<span class="c1">#             # print(&#39;f shape: &#39;, f.shape)</span>
<span class="c1">#             # print(&#39;f is leaf: &#39;, f.is_leaf)</span>

<span class="c1">#             # f_2 = f**2</span>
<span class="c1">#             # print(&#39;f2 shape&#39;, f_2.shape)</span>
<span class="c1">#             # nu = torch.autograd.Variable( x[:,1], requires_grad=True)</span>

<span class="c1">#             # nu=torch.autograd.Variable(x[:,1], requires_grad=True)</span>
<span class="c1">#             nu=torch.tensor(x[:,1], requires_grad=True)</span>
<span class="c1">#             # print(type(nu))</span>

<span class="c1">#             # nu.retain_grad()</span>

<span class="c1">#             # print(&#39;nu shape: &#39;, nu.shape)</span>
<span class="c1">#             # print(&#39;nu is leaf: &#39;, nu.is_leaf)</span>
<span class="c1">#             # print(&#39;nu type&#39;, type(nu))</span>


<span class="c1">#             # WE NEED TO RETAIN_GRAD ON NON-LEAF NODES </span>
<span class="c1">#             f.retain_grad()</span>
<span class="c1">#             f.backward(gradient=torch.ones_like(f), retain_graph=True)</span>

<span class="c1">#             #or f.backward(torch.ones_like(f) , retain, graph</span>
<span class="c1">#             # we cant do f.backward() here because backeard() can only be called for scalara</span>
<span class="c1">#             # and f here will be a tensor of shape torch.Size([1000, 1]</span>
<span class="c1">#             #e.g. see https://abishekbashyall.medium.com/playing-with-backward-method-in-pytorch-bd34b58745a0</span>

<span class="c1">#             # PyTorch accumulates the gradient in default, we need to clear the previous</span>
<span class="c1">#             # values</span>
<span class="c1">#             # df_dnu = nu.grad</span>
<span class="c1">#             df_dx = x.grad</span>
<span class="c1">#             # print(&#39;df_dnu =&#39;, df_dnu)</span>
<span class="c1">#             # print(&#39;df_dx =&#39;, df_dx)</span>
<span class="c1">#             # print(&#39;df_dx shape :&#39;, df_dx.shape)</span>
<span class="c1">#             #clear the gradient after you take it</span>
<span class="c1">#             df_dnu = df_dx[:,1]</span>
<span class="c1">#             # print(&#39;df_dnu shape: &#39;, df_dnu.shape)</span>
<span class="c1">#             x.grad.zero_()</span>
        
            
<span class="c1">#             #now we can zero grads</span>
<span class="c1">#             self.optimizer.zero_grad()</span>
<span class="c1">#             self.model.train()</span>
<span class="c1">#             # with torch.no_grad():</span>

<span class="c1">#             y=torch.from_numpy(batch_t).float()</span>
            
<span class="c1">#             outputs = self.model(x)</span>
<span class="c1">#             loss = average_quadratic_loss_pivot(f = outputs, t=y, x=x, df_dnu=df_dnu, kappa=self.kappa)</span>
<span class="c1">#             loss.backward()</span>
<span class="c1">#             self.optimizer.step()</span>
<span class="c1">#             final_loss += loss.item()</span>

<span class="c1">#         return final_loss / self.batch_size</span>
    
<span class="c1">#     def evaluate_pivot(self, x, t):</span>
<span class="c1">#         &quot;&quot;&quot;the training function: takes the training dataloader&quot;&quot;&quot;</span>
        
<span class="c1">#         final_loss = 0</span>
<span class="c1">#         for iteration in range(n_iterations):</span>
<span class="c1">#             batch_x, batch_t = get_features_training_batch(x, t,  self.batch_size)#x and t are train_x and train_t</span>

<span class="c1">#             # with torch.no_grad():            </span>
            
            
<span class="c1">#             x = torch.from_numpy(batch_x).float()</span>
<span class="c1">#             # print(&#39;x is leaf: &#39;, x.is_leaf)</span>
<span class="c1">#             x.requires_grad_(True)</span>
<span class="c1">#             # x.retain_grad()</span>
<span class="c1">#             # print(&#39;x is leaf after retain: &#39;, x.is_leaf)</span>
<span class="c1">#             # x.requires_grad_(True)</span>
<span class="c1">#             # x.retain_grad()</span>
<span class="c1">#             f = self.model(x)</span>
<span class="c1">#             f = f.view(-1)</span>
<span class="c1">#             #multiply the model by its ransverse, remember we can only take gradients of scalars</span>
<span class="c1">#             #and f will be a vector before this</span>
<span class="c1">#             f = f @ f.t()</span>
<span class="c1">#             # f = torch.tensor(f, requires_grad=True)</span>
<span class="c1">#             # print(&#39;f shape: &#39;, f.shape)</span>
<span class="c1">#             # print(&#39;f is leaf: &#39;, f.is_leaf)</span>

<span class="c1">#             # f_2 = f**2</span>
<span class="c1">#             # print(&#39;f2 shape&#39;, f_2.shape)</span>
<span class="c1">#             # nu = torch.autograd.Variable( x[:,1], requires_grad=True)</span>

<span class="c1">#             # nu=torch.autograd.Variable(x[:,1], requires_grad=True)</span>
<span class="c1">#             nu=torch.tensor(x[:,1], requires_grad=True)</span>
<span class="c1">#             # print(type(nu))</span>

<span class="c1">#             # nu.retain_grad()</span>

<span class="c1">#             # print(&#39;nu shape: &#39;, nu.shape)</span>
<span class="c1">#             # print(&#39;nu is leaf: &#39;, nu.is_leaf)</span>
<span class="c1">#             # print(&#39;nu type&#39;, type(nu))</span>


<span class="c1">#             # WE NEED TO RETAIN_GRAD ON NON-LEAF NODES </span>
<span class="c1">#             f.retain_grad()</span>
<span class="c1">#             f.backward(gradient=torch.ones_like(f), retain_graph=True)</span>

<span class="c1">#             #or f.backward(torch.ones_like(f) , retain, graph</span>
<span class="c1">#             # we cant do f.backward() here because backeard() can only be called for scalara</span>
<span class="c1">#             # and f here will be a tensor of shape torch.Size([1000, 1]</span>
<span class="c1">#             #e.g. see https://abishekbashyall.medium.com/playing-with-backward-method-in-pytorch-bd34b58745a0</span>

<span class="c1">#             # PyTorch accumulates the gradient in default, we need to clear the previous</span>
<span class="c1">#             # values</span>
<span class="c1">#             # df_dnu = nu.grad</span>
<span class="c1">#             df_dx = x.grad</span>
<span class="c1">#             # print(&#39;df_dnu =&#39;, df_dnu)</span>
<span class="c1">#             # print(&#39;df_dx =&#39;, df_dx)</span>
<span class="c1">#             # print(&#39;df_dx shape :&#39;, df_dx.shape)</span>
<span class="c1">#             #clear the gradient after you take it</span>
<span class="c1">#             df_dnu = df_dx[:,1]</span>
<span class="c1">#             # print(&#39;df_dnu shape: &#39;, df_dnu.shape)</span>
<span class="c1">#             x.grad.zero_()</span>
        
<span class="c1">#             #self.optimizer.zero_grad()</span>

            
<span class="c1">#             self.model.eval()</span>
<span class="c1">#             y=torch.from_numpy(batch_t).float()</span>
<span class="c1">#             outputs = self.model(x)</span>
<span class="c1">#             loss =average_quadratic_loss_pivot(f = outputs, t=y, x=x, df_dnu=df_dnu, kappa=self.kappa)</span>
<span class="c1">#             final_loss += loss.item()</span>
<span class="c1">#         return final_loss / self.batch_size</span>



<span class="c1"># EPOCHS=1</span>
<span class="c1"># def run_train_pivot(params, save_model=False):</span>
<span class="c1">#     &quot;&quot;&quot;For tuning the parameters&quot;&quot;&quot;</span>

<span class="c1"># #     model =  RegularizedRegressionModel(</span>
<span class="c1"># #               nfeatures=train_x.shape[1], </span>
<span class="c1"># #                 ntargets=1,</span>
<span class="c1"># #                 nlayers=params[&quot;nlayers&quot;], </span>
<span class="c1"># #                 hidden_size=params[&quot;hidden_size&quot;],</span>
<span class="c1"># #                 dropout=params[&quot;dropout&quot;]</span>
<span class="c1"># #                 )</span>

    
<span class="c1">#     model =  RegularizedRegressionModel(</span>
<span class="c1">#               nfeatures=sample_x.shape[1], </span>
<span class="c1">#                 ntargets=1,</span>
<span class="c1">#                 nlayers=params[&quot;n_layers&quot;], </span>
<span class="c1">#                 hidden_size=2,</span>
<span class="c1">#                 dropout=0.2</span>
<span class="c1">#                 )</span>
<span class="c1">#     # print(model)</span>
    

<span class="c1">#     learning_rate= learning_rate</span>
    
    
<span class="c1">#     # optimizer_name = params[&quot;optimizer_name&quot;]</span>
<span class="c1">#     # # optimizer = torch.optim.Adam(model.parameters(), lr=params[&quot;learning_rate&quot;]) </span>
<span class="c1">#     # optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=learning_rate)</span>
    
<span class="c1">#     optimizer = optimizer_nonMLE_pivot</span>
    
<span class="c1">#     eng=Engine_kappa(model, optimizer, batch_size=batch_size, kappa = params[&quot;kappa&quot;])</span>
<span class="c1">#     best_loss = np.inf</span>
<span class="c1">#     early_stopping_iter=10</span>
<span class="c1">#     early_stopping_coutner=0</span>

<span class="c1">#     for epoch in range(EPOCHS):</span>
<span class="c1">#         train_loss = eng.train_pivot(train_x, train_t)</span>
<span class="c1">#         valid_loss=eng.evaluate_pivot(test_x, test_t)</span>

<span class="c1">#         print(f&quot;{epoch} \t {train_loss} \t {valid_loss}&quot;)</span>
<span class="c1">#         if valid_loss&lt;best_loss:</span>
<span class="c1">#             best_loss=valid_loss</span>
<span class="c1">#             if save_model:</span>
<span class="c1">#                 model.save(model.state_dict(), &quot;model_m.bin&quot;)</span>
<span class="c1">#         else:</span>
<span class="c1">#             early_stopping_coutner+=1</span>
<span class="c1">#         if early_stopping_coutner &gt; early_stopping_iter:</span>
<span class="c1">#             break</span>
<span class="c1">#     return best_loss</span>

<span class="c1"># # run_train()</span>

<span class="c1"># def objective_pivot(trial):</span>
<span class="c1">#     params = {</span>
<span class="c1">#       &quot;kappa&quot;: trial.suggest_float(&quot;kappa&quot;,1,2),</span>
<span class="c1">#         &quot;n_layers&quot;: trial.suggest_int(&quot;n_layers&quot;, 1, 3)</span>
<span class="c1">#     }</span>
<span class="c1">#     # all_losses=[]</span>

<span class="c1">#     temp_loss = run_train_pivot(params,save_model=False)</span>
<span class="c1">#     # all_losses.append(temp_loss)</span>
<span class="c1">#     return temp_loss</span>

<span class="c1"># def tune_hyperparameters_pivot():</span>
<span class="c1">#     print(&#39;Getting best hyperparameters&#39;)</span>
<span class="c1">#     study=optuna.create_study(direction=&quot;minimize&quot;)</span>
<span class="c1">#     study.optimize(objective, n_trials=100)</span>
<span class="c1">#     best_trial = study.best_trial</span>
<span class="c1">#     print(&#39;best model parameters&#39;, best_trial.params)</span>

<span class="c1">#     best_params=best_trial.params#this is a dictionary</span>
<span class="c1">#     filename=&#39;best_params/kappa_tuned.csv&#39;</span>
<span class="c1">#     # param_df=pd.DataFrame({</span>
<span class="c1">#     #                         &#39;n_layers&#39;:best_params[&quot;nlayers&quot;], </span>
<span class="c1">#     #                         &#39;hidden_size&#39;:best_params[&quot;hidden_size&quot;], </span>
<span class="c1">#     #                         &#39;dropout&#39;:best_params[&quot;dropout&quot;],</span>
<span class="c1">#     #                         &#39;optimizer_name&#39;:best_params[&quot;optimizer_name&quot;],</span>
<span class="c1">#     #                         &#39;learning_rate&#39;: best_params[&quot;learning_rate&quot;], </span>
<span class="c1">#     #                         &#39;batch_size&#39;:best_params[&quot;batch_size&quot;] },</span>
<span class="c1">#                                     # index=[0]</span>
<span class="c1">#     # )</span>
<span class="c1">#     param_df = pd.DataFrame({&#39;kappa&#39;: best_params[&quot;kappa&quot;], &#39;n_layers&#39;: best_params[&quot;n_layers&quot;]},  index=[0]  )</span>

<span class="c1">#     param_df.to_csv(filename)   </span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># tune_hyperparameters_pivot()</span>
</pre></div>
</div>
</div>
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="1_Intro_and_One_Parameter_Problem.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">1 - Pivotal LFI for Count Data in Particle Physics: Background and one-parameter Problem</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="3_Replacing_Data_with_Lambda_and_2D_Inference.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">3 -  2-Dimensional Inference in <span class="math notranslate nohighlight">\(\theta - \nu\)</span> Space and Replacing Observed Data with Test Statistics</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Ali Al Kadhim and Harrison Prosper<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>