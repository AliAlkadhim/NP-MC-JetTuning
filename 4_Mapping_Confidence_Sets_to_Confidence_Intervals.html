
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>4 - Inference on POI in presence of a nuissance parameter: Pivots, and Mapping Confidence Sets to Confidence Intervals &#8212; Pivotal Likelihood-Free Inference for Particle Physics</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="5. Imposing Pivotal Conditions on \(\lambda\)" href="5_Imposing_Pivotal_Conditions_on_Lambda.html" />
    <link rel="prev" title="3 - 2-Dimensional Inference in \(\theta - \nu\) Space and Replacing Observed Data with Test Statistics" href="3_Replacing_Data_with_Lambda_and_2D_Inference.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Pivotal Likelihood-Free Inference for Particle Physics</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the Likelihood-Free Inference for Particle Physics Jupyter book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1_Intro_and_One_Parameter_Problem.html">
   1 - Pivotal LFI for Count Data in Particle Physics: Background and one-parameter Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2_Two_Parameter_Problem_and_Pivotal_p_Value.html">
   2 - Two-Parameter Problem and Pivotal Likelihood-Free p-Values
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3_Replacing_Data_with_Lambda_and_2D_Inference.html">
   3 -  2-Dimensional Inference in
   <span class="math notranslate nohighlight">
    \(\theta - \nu\)
   </span>
   Space and Replacing Observed Data with Test Statistics
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   4 - Inference on POI in presence of a nuissance parameter: Pivots, and Mapping Confidence Sets to Confidence Intervals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5_Imposing_Pivotal_Conditions_on_Lambda.html">
   5. Imposing Pivotal Conditions on
   <span class="math notranslate nohighlight">
    \(\lambda\)
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6_More_Ideas.html">
   More Discussions
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/AliAlkadhim/LFI_HEP/master?urlpath=tree/JupyterBook/4_Mapping_Confidence_Sets_to_Confidence_Intervals.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/4_Mapping_Confidence_Sets_to_Confidence_Intervals.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   4 - Inference on POI in presence of a nuissance parameter: Pivots, and Mapping Confidence Sets to Confidence Intervals
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pivotal-likelihood-free-inference-for-count-data-in-particle-physics">
     Pivotal Likelihood-Free Inference for Count Data in Particle Physics
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#please-see-the-other-noteboooks-especially-notebooks-2-3-5">
       please see the other noteboooks, especially notebooks 2,3,5.
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background-on-all-the-ways-to-calculate-confidence-intervals-and-relation-to-alffi">
   4.1 Background on all the ways to calculate confidence intervals and relation to ALFFI
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#where-hat-hat-nu-means-the-profiled-values-values-that-maximize-l-for-a-specified-mu">
     where
     <span class="math notranslate nohighlight">
      \(\hat{\hat{\nu}}\)
     </span>
     means the profiled values (values that maximize
     <span class="math notranslate nohighlight">
      \(L\)
     </span>
     ) for a specified
     <span class="math notranslate nohighlight">
      \(\mu\)
     </span>
     .
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#this-means-that-in-order-to-derive-confidence-intervals-we-can-either">
       This means that in order to derive
       <strong>
        confidence intervals
       </strong>
       , we can either:
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mapping-confidence-regions-sets-to-confidence-intervals">
   4.2: Mapping Confidence Regions (Sets) to Confidence intervals
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#connection-to-gaussian-distributed-measurements">
     4.3: Connection to Gaussian Distributed Measurements
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gaussian-1d">
       4.3.1: Gaussian 1D
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#multivariate-normal">
       4.3.2: Multivariate normal
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bivariate-normal-confidence-ellipses">
       4.4: Bivariate normal: Confidence ellipses
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#aside-a-quick-review-of-ellipses">
       4.5: Aside: a quick review of ellipses
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#calculating-the-multivariate-coverage-that-is-the-area-volume-of-the-ellipse-ellipsoid">
       4.6: Calculating the multivariate coverage, that is, the area (volume) of the ellipse (ellipsoid)
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#toy-experiments-sample-bivariate-normal">
       Toy experiments: Sample bivariate normal
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hep-signal-background-problem-statement">
     HEP Signal-Background Problem Statement
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#todo-2-the-pivotal-lambda-model-in-notebook-5-does-the-same-thing-inference-on-mu-independently-of-nu-and-seems-to-work-we-should-include-it-in-this-paper-the-constraints-could-be-done-in-other-ways-than-just-adding-the-term-to-the-loss">
       TODO 2: The pivotal
       <span class="math notranslate nohighlight">
        \(\lambda\)
       </span>
       model in notebook 5 does the same thing (inference on
       <span class="math notranslate nohighlight">
        \(\mu\)
       </span>
       independently of
       <span class="math notranslate nohighlight">
        \(\nu\)
       </span>
       and seems to work, we should include it in this paper. The constraints could be done in other ways than just adding the term to the loss.
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#todo-3-sample-mu-sim-pi-mu-then-nu-mu-sim-pi-mu-nu-which-is-equivalent-to-marginalizing-pi-nu">
       TODO 3: sample
       <span class="math notranslate nohighlight">
        \(\mu \sim \pi(\mu)\)
       </span>
       then
       <span class="math notranslate nohighlight">
        \(\nu(\mu) \sim \pi(\mu,\nu)\)
       </span>
       which is equivalent to marginalizing
       <span class="math notranslate nohighlight">
        \(\pi(\nu)\)
       </span>
       .
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#todo-4-verify-correct-coverage-of-1-parameter-via-neyman-6th">
       TODO 4: verify correct coverage of 1 parameter via Neyman.6th
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#todo-4-use-differentiable-programming-as-in-jax-or-relaxxed-to-get-better-results-for-these-step-functions-during-trainig-or-use-a-kernel-density-estimate-as-a-histogram-alternative">
       TODO 4: use differentiable programming (as in jax or relaxxed) to get better results for these step functions during trainig, or use a kernel density estimate as a histogram alternative.
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#todo-5-apply-it-to-high-er-dimensional-parameter-space">
       TODO 5: apply it to high(er) dimensional parameter space
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#suppose-we-now-make-an-observation-and-remid-yourself-of-definitions-of-our-parameters">
   Suppose we now make an observation, and remid yourself of definitions of our parameters
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#observed-data">
     Observed Data:
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parameters">
     Parameters:
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#auxiliary-simulated-data-simulated-on-the-fly-for-each-observation">
     Auxiliary (simulated) Data (simulated on-the-fly for each observation):
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#compute-coverage-for-set-and-show-it-s-satisfied-then-compute-coverage-for-individual-arameter-theta-with-neyman-construction-and-show-that-it-has-nominal-coverage">
   Compute coverage for set and show it’s satisfied. Then compute coverage for individual arameter
   <span class="math notranslate nohighlight">
    \(\theta\)
   </span>
   with Neyman construction and show that it has nominal coverage
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#alffi-algorithm-see-chapters-2-and-3-and-compute-coveragage-py">
   ALFFI algorithm (see chapters 2 and 3 and
   <code class="docutils literal notranslate">
    <span class="pre">
     compute_coveragage.py
    </span>
   </code>
   )
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#allffi-cpi">
   ALLFFI-CPI
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#alffi-cpi-for-inference-using-one-single-cl-tau-1">
     ALFFI-CPI For inference using one single CL
     <span class="math notranslate nohighlight">
      \(\tau_1\)
     </span>
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#alffi-cpi-for-any-all-cls-tau">
     ALFFI-CPI for any (all) CLs
     <span class="math notranslate nohighlight">
      \(\tau\)
     </span>
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>4 - Inference on POI in presence of a nuissance parameter: Pivots, and Mapping Confidence Sets to Confidence Intervals</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   4 - Inference on POI in presence of a nuissance parameter: Pivots, and Mapping Confidence Sets to Confidence Intervals
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pivotal-likelihood-free-inference-for-count-data-in-particle-physics">
     Pivotal Likelihood-Free Inference for Count Data in Particle Physics
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#please-see-the-other-noteboooks-especially-notebooks-2-3-5">
       please see the other noteboooks, especially notebooks 2,3,5.
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background-on-all-the-ways-to-calculate-confidence-intervals-and-relation-to-alffi">
   4.1 Background on all the ways to calculate confidence intervals and relation to ALFFI
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#where-hat-hat-nu-means-the-profiled-values-values-that-maximize-l-for-a-specified-mu">
     where
     <span class="math notranslate nohighlight">
      \(\hat{\hat{\nu}}\)
     </span>
     means the profiled values (values that maximize
     <span class="math notranslate nohighlight">
      \(L\)
     </span>
     ) for a specified
     <span class="math notranslate nohighlight">
      \(\mu\)
     </span>
     .
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#this-means-that-in-order-to-derive-confidence-intervals-we-can-either">
       This means that in order to derive
       <strong>
        confidence intervals
       </strong>
       , we can either:
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mapping-confidence-regions-sets-to-confidence-intervals">
   4.2: Mapping Confidence Regions (Sets) to Confidence intervals
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#connection-to-gaussian-distributed-measurements">
     4.3: Connection to Gaussian Distributed Measurements
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#gaussian-1d">
       4.3.1: Gaussian 1D
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#multivariate-normal">
       4.3.2: Multivariate normal
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bivariate-normal-confidence-ellipses">
       4.4: Bivariate normal: Confidence ellipses
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#aside-a-quick-review-of-ellipses">
       4.5: Aside: a quick review of ellipses
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#calculating-the-multivariate-coverage-that-is-the-area-volume-of-the-ellipse-ellipsoid">
       4.6: Calculating the multivariate coverage, that is, the area (volume) of the ellipse (ellipsoid)
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#toy-experiments-sample-bivariate-normal">
       Toy experiments: Sample bivariate normal
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hep-signal-background-problem-statement">
     HEP Signal-Background Problem Statement
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#todo-2-the-pivotal-lambda-model-in-notebook-5-does-the-same-thing-inference-on-mu-independently-of-nu-and-seems-to-work-we-should-include-it-in-this-paper-the-constraints-could-be-done-in-other-ways-than-just-adding-the-term-to-the-loss">
       TODO 2: The pivotal
       <span class="math notranslate nohighlight">
        \(\lambda\)
       </span>
       model in notebook 5 does the same thing (inference on
       <span class="math notranslate nohighlight">
        \(\mu\)
       </span>
       independently of
       <span class="math notranslate nohighlight">
        \(\nu\)
       </span>
       and seems to work, we should include it in this paper. The constraints could be done in other ways than just adding the term to the loss.
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#todo-3-sample-mu-sim-pi-mu-then-nu-mu-sim-pi-mu-nu-which-is-equivalent-to-marginalizing-pi-nu">
       TODO 3: sample
       <span class="math notranslate nohighlight">
        \(\mu \sim \pi(\mu)\)
       </span>
       then
       <span class="math notranslate nohighlight">
        \(\nu(\mu) \sim \pi(\mu,\nu)\)
       </span>
       which is equivalent to marginalizing
       <span class="math notranslate nohighlight">
        \(\pi(\nu)\)
       </span>
       .
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#todo-4-verify-correct-coverage-of-1-parameter-via-neyman-6th">
       TODO 4: verify correct coverage of 1 parameter via Neyman.6th
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#todo-4-use-differentiable-programming-as-in-jax-or-relaxxed-to-get-better-results-for-these-step-functions-during-trainig-or-use-a-kernel-density-estimate-as-a-histogram-alternative">
       TODO 4: use differentiable programming (as in jax or relaxxed) to get better results for these step functions during trainig, or use a kernel density estimate as a histogram alternative.
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#todo-5-apply-it-to-high-er-dimensional-parameter-space">
       TODO 5: apply it to high(er) dimensional parameter space
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#suppose-we-now-make-an-observation-and-remid-yourself-of-definitions-of-our-parameters">
   Suppose we now make an observation, and remid yourself of definitions of our parameters
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#observed-data">
     Observed Data:
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#parameters">
     Parameters:
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#auxiliary-simulated-data-simulated-on-the-fly-for-each-observation">
     Auxiliary (simulated) Data (simulated on-the-fly for each observation):
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#compute-coverage-for-set-and-show-it-s-satisfied-then-compute-coverage-for-individual-arameter-theta-with-neyman-construction-and-show-that-it-has-nominal-coverage">
   Compute coverage for set and show it’s satisfied. Then compute coverage for individual arameter
   <span class="math notranslate nohighlight">
    \(\theta\)
   </span>
   with Neyman construction and show that it has nominal coverage
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#alffi-algorithm-see-chapters-2-and-3-and-compute-coveragage-py">
   ALFFI algorithm (see chapters 2 and 3 and
   <code class="docutils literal notranslate">
    <span class="pre">
     compute_coveragage.py
    </span>
   </code>
   )
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#allffi-cpi">
   ALLFFI-CPI
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#alffi-cpi-for-inference-using-one-single-cl-tau-1">
     ALFFI-CPI For inference using one single CL
     <span class="math notranslate nohighlight">
      \(\tau_1\)
     </span>
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#alffi-cpi-for-any-all-cls-tau">
     ALFFI-CPI for any (all) CLs
     <span class="math notranslate nohighlight">
      \(\tau\)
     </span>
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <p><a name="notebook_4"></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="inference-on-poi-in-presence-of-a-nuissance-parameter-pivots-and-mapping-confidence-sets-to-confidence-intervals">
<h1>4 - Inference on POI in presence of a nuissance parameter: Pivots, and Mapping Confidence Sets to Confidence Intervals<a class="headerlink" href="#inference-on-poi-in-presence-of-a-nuissance-parameter-pivots-and-mapping-confidence-sets-to-confidence-intervals" title="Permalink to this headline">#</a></h1>
<section id="pivotal-likelihood-free-inference-for-count-data-in-particle-physics">
<h2>Pivotal Likelihood-Free Inference for Count Data in Particle Physics<a class="headerlink" href="#pivotal-likelihood-free-inference-for-count-data-in-particle-physics" title="Permalink to this headline">#</a></h2>
<p>Ali Al Kadhim and Harrison B. Prosper <br>
Department of Physics, Florida State University <br></p>
<section id="please-see-the-other-noteboooks-especially-notebooks-2-3-5">
<h3>please see the other <a class="reference external" href="https://alialkadhim.github.io/LFI_HEP">noteboooks</a>, especially notebooks 2,3,5.<a class="headerlink" href="#please-see-the-other-noteboooks-especially-notebooks-2-3-5" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span><span class="p">;</span> <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span><span class="p">;</span> <span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">st</span>
<span class="kn">import</span> <span class="nn">torch</span><span class="p">;</span> <span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="c1">#use numba&#39;s just-in-time compiler to speed things up</span>
<span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">njit</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">;</span> <span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mp</span><span class="p">;</span> <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span><span class="p">;</span> 
<span class="c1">#reset matplotlib stle/parameters</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">mpl</span><span class="o">.</span><span class="n">rcParamsDefault</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-deep&#39;</span><span class="p">)</span>
<span class="n">mp</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;agg.path.chunksize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">font_legend</span> <span class="o">=</span> <span class="mi">15</span><span class="p">;</span> <span class="n">font_axes</span><span class="o">=</span><span class="mi">15</span>
<span class="c1"># %matplotlib inline</span>
<span class="kn">import</span> <span class="nn">copy</span><span class="p">;</span> <span class="kn">import</span> <span class="nn">sys</span><span class="p">;</span> <span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">importlib</span> <span class="kn">import</span> <span class="n">import_module</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">optuna</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;optuna is only used for hyperparameter tuning, not critical!&#39;</span><span class="p">)</span>
    <span class="k">pass</span>
<span class="c1"># import sympy as sy</span>
<span class="c1">#sometimes jupyter doesnt initialize MathJax automatically for latex, so do this</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">wid</span><span class="p">;</span> <span class="n">wid</span><span class="o">.</span><span class="n">HTMLMath</span><span class="p">(</span><span class="s1">&#39;$\LaTeX$&#39;</span><span class="p">)</span>
<span class="c1"># from compute_coverage import *</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "15a6ba636ce944c3a9eb79efa750cca7", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">LFI_PIVOT_BASE</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;LFI_PIVOT_BASE&#39;</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;BASE directoy properly set = &#39;</span><span class="p">,</span> <span class="n">LFI_PIVOT_BASE</span><span class="p">)</span>
    <span class="c1"># utils_dir = os.path.join(LFI_PIVOT_BASE, &#39;utils&#39;)</span>
    <span class="c1"># sys.path.append(utils_dir)</span>
    <span class="c1"># import utils</span>
    <span class="c1">#usually its not recommended to import everything from a module, but we know</span>
    <span class="c1">#whats in it so its fine</span>
    <span class="c1"># from utils import *</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;BASE directory not properly set. Read repo README.</span><span class="se">\</span>
<span class="s2">    If you need a function from utils, use the decorator below, or add utils to sys.path&quot;&quot;&quot;</span><span class="p">)</span>
    <span class="k">pass</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BASE directoy properly set =  /home/ali/Desktop/Pulled_Github_Repositories/LFI_HEP
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="background-on-all-the-ways-to-calculate-confidence-intervals-and-relation-to-alffi">
<h1>4.1 Background on all the ways to calculate confidence intervals and relation to ALFFI<a class="headerlink" href="#background-on-all-the-ways-to-calculate-confidence-intervals-and-relation-to-alffi" title="Permalink to this headline">#</a></h1>
<p>The objective now is to derive confidence intervals or limits for the parameter of interest <span class="math notranslate nohighlight">\(\mu\)</span>. What makes this challenging is the presence of the <strong>nuissance parameter</strong> <span class="math notranslate nohighlight">\(\nu\)</span>, which is a parameter whose value is not known precisely, but affects the inference on the parameter of interest <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<blockquote>
<div><p>Basu (1977) also discusses the problem nicely: basically, if <span class="math notranslate nohighlight">\(\theta \leftrightarrow P_{\theta}\)</span> i.e. if <span class="math notranslate nohighlight">\(\theta\)</span> maps oone-to-one onto <span class="math notranslate nohighlight">\(P_\theta\)</span> where <span class="math notranslate nohighlight">\(P_{\theta}\)</span> is a probability measure on the space of observation (i.e. <span class="math notranslate nohighlight">\(P_{\theta} = L(x|\theta)\)</span> in modern language), then there is no problem and the inference on <span class="math notranslate nohighlight">\(\theta\)</span> is good. However, in most cases, we have to work with a (total) likelihood that is indexed as <span class="math notranslate nohighlight">\(L(X| \theta \in \Theta, \phi \in \Phi)\)</span> where <span class="math notranslate nohighlight">\(\theta\)</span>, the POI, is a member of parameter space <span class="math notranslate nohighlight">\(\Theta\)</span> and <span class="math notranslate nohighlight">\(\phi\)</span> is an additional unknown parameter in family <span class="math notranslate nohighlight">\(\Phi\)</span>.</p>
</div></blockquote>
<p>In dealing with statistical inference it is important to deal with nuissance parameters (i.e. systematics, e.g. jet energy scale, or any other parameters dealing with experiment or theory that are not known exactly) because we want to isolate to what extent your result is sensetive to a particular nuissance parameter (as opposed to, say, statistical fluctuations).</p>
<p>Bayesian and Frequentist methods alike treat nuissance parameters in a similary way: they <strong>try to</strong> partition the likelihood into components representing information on the parameter of interest, and into components representing the nuissance parameter(s). In HEP, we can have many nuissance parameters that repressents our ignorance of certain quantities, which are called “systematic uncertainties”.</p>
<p>Broadly speaking, The standard procedure for removal of nuissance parameters, as discussed above is <strong>Bayesian marginalization</strong>. In the Bayesian approach, nuissance parameters are assigned prior probabilities <span class="math notranslate nohighlight">\(\pi(\nu) d \nu\)</span> and are integrated out in order to arrive at the posterior for <span class="math notranslate nohighlight">\(\mu\)</span> :</p>
<div class="math notranslate nohighlight">
\[L(\mu) = \int L(\mu,\nu) \pi (\nu) d \nu. \]</div>
<p>However, this introduces a fair bit of subjectivity into the problem through the choice of the prior.</p>
<!--$$ L(\theta) = P(x|\theta) = \int P(x \mid \theta, \nu) \pi (\nu) d \nu$$-->
<p>where the uncertainty in the nuissance parameter is characterized by a prior PDF <span class="math notranslate nohighlight">\(\pi(\nu)\)</span> (and <span class="math notranslate nohighlight">\(p(x|\theta)\)</span> is sometimes called a marginal model of the data and POI, which is a 1-D posterior for he POI)</p>
<p>Frequentist methods deal with nuissance parameters by profiling them the likelihood in order to arrive at their MLEs.
i.e. integrate out the dependence of the nuissance parameter in the posterior to arrive at the posterior marginal distribution of the parameter of interest. This is known as <strong>Frequentist profiling</strong>:</p>
<div class="math notranslate nohighlight">
\[L_{\text{profile}}(\mu) = L(\mu, \hat{\hat{\nu}}(\mu) ),  \]</div>
<section id="where-hat-hat-nu-means-the-profiled-values-values-that-maximize-l-for-a-specified-mu">
<h2>where <span class="math notranslate nohighlight">\(\hat{\hat{\nu}}\)</span> means the profiled values (values that maximize <span class="math notranslate nohighlight">\(L\)</span>) for a specified <span class="math notranslate nohighlight">\(\mu\)</span>.<a class="headerlink" href="#where-hat-hat-nu-means-the-profiled-values-values-that-maximize-l-for-a-specified-mu" title="Permalink to this headline">#</a></h2>
<section id="this-means-that-in-order-to-derive-confidence-intervals-we-can-either">
<h3>This means that in order to derive <strong>confidence intervals</strong>, we can either:<a class="headerlink" href="#this-means-that-in-order-to-derive-confidence-intervals-we-can-either" title="Permalink to this headline">#</a></h3>
<ol class="simple">
<li><p>Follow the <strong>Neyman construction</strong> (as was done in the <a class="reference external" href="#notebook_1">first notebook</a> )</p></li>
</ol>
<blockquote>
<div><p>Neyman has shown a method for calculation of confidence intervals that guarantees coverage via a method usually referred to as “Neyman Construction”. One problem with Neyman construction, however, is that with multi-parameter problems, it is very difficult to construct intervals that guarantee coverage (that is, they have coverage of at least <span class="math notranslate nohighlight">\(1-\alpha\)</span>). The reason for this difficulty is that one has to repeat this construction for each point in the space of nuissance parameters, by treating the nuissance parameter as fixed, and constructing the interval for the POI.</p>
</div></blockquote>
<ol class="simple">
<li><p><strong>Construct Bayesian (“credible”) intervals</strong> <span class="math notranslate nohighlight">\([\mu^{lo}, \mu^{up}]\)</span> by solving for <span class="math notranslate nohighlight">\(\mu^{lo}, \mu^{up}\)</span> in the equation (this process is called marginalization):</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ CL = \int_{\mu^{low} }^{\mu^{up}} \text{posterior}(D | \mu)\  d \mu = \int_{\mu^{low} }^{\mu^{up}} \mathcal{L} (D;\mu) \ \text{prior}(\mu)\ d \mu\]</div>
<p>where <span class="math notranslate nohighlight">\(CL=1-\alpha\)</span> and <span class="math notranslate nohighlight">\(\int_{\mu^{low} }^{\mu^{up}} \text{posterior}(D | \mu)\ d \mu=\mathbb{P} (\mu \in [\mu^{low}, \mu^{up}] )\)</span>
Which means the confidence interval is solved from the limits of integration above.</p>
<p>In our probelm (see more below) this ammounts to solving</p>
<div class="math notranslate nohighlight">
\[ \int_{\mu^{lo}}^{\mu^{up}} \int_{\nu^{lo}}^{\nu^{up}} \mathcal{L}(D; \mu, \nu) \ \pi(\mu) \ \pi(\nu) \  d \mu \ d \nu = 1-\alpha\]</div>
<blockquote>
<div><p>As discussed in the first notebook, there is a big instinsic problem in the choice this prior (both for subjective priors and non-subjective, e.g. “reference priors”). Furthermore, it is a big computational and technical challenge to solve the integral, which is often calculated by MC, but becomes exponentially harder to compute as the number of nuissance parameters increases.</p>
</div></blockquote>
<ol class="simple">
<li><p>Using the likelihood ratio (<span class="math notranslate nohighlight">\(\lambda_{\text{NP}}\)</span>).</p></li>
</ol>
<blockquote>
<div><p>We follow this procedure, discussed at length in this work, in constructing</p>
</div></blockquote>
<p>Each of these methods has many problems, discussed throughout this jupyterbook, and they all don’t guarantee coverage for multi-parameter problems with nuissance parameters. Guaranteeing coverage for such problems for POIs and nuissance parameters remains a topic of statistical research.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">LFI_PIVOT_BASE</span><span class="p">,</span><span class="s1">&#39;images&#39;</span><span class="p">,</span><span class="s1">&#39;ways_to_make_CIs_screenshot.png&#39;</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4_Mapping_Confidence_Sets_to_Confidence_Intervals_4_0.png" src="_images/4_Mapping_Confidence_Sets_to_Confidence_Intervals_4_0.png" />
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="mapping-confidence-regions-sets-to-confidence-intervals">
<h1>4.2: Mapping Confidence Regions (Sets) to Confidence intervals<a class="headerlink" href="#mapping-confidence-regions-sets-to-confidence-intervals" title="Permalink to this headline">#</a></h1>
<p>We would like to extend map confidence region (or set) <span class="math notranslate nohighlight">\(R(D)\)</span> to univariate confidence interval for a particular parameter. What we currently have, is that
$<span class="math notranslate nohighlight">\(\mathbb{P} [ R(D) \text{ will cover the true }\{\mu,\nu\} \text{pair} ] \ge 1-\alpha \)</span>$</p>
<p>Or, in other words,</p>
<div class="math notranslate nohighlight">
\[\mathbb{P} \left( (\mu_0,\nu_0) \in R(D) \right) = \mathbb{P}\left( \mu_0 \in [\mu^{lo},\mu^{up} ] \text{ and } \nu_0 \in [\nu^{lo},\nu^{up} ] \right) \ge 1-\alpha \]</div>
<p>We would like to map the statement in Eq 5 to a confidence interval on an individual parameter
$<span class="math notranslate nohighlight">\(\mathbb{P}\left( \mu_0 \in [\mu^{lo},\mu^{up} ] \right) \ge 1-\alpha \)</span>$</p>
<blockquote>
<div><p>Aside: it is interesting to note that the value <span class="math notranslate nohighlight">\(\theta_0\)</span> is fixed and unknown while the variables <span class="math notranslate nohighlight">\(\{\mu^{lo},\mu^{up} \}\)</span> is a set a random (like a random variable) set of the data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. One other benefit of LFI is that we can always generate synthetic data, whereas in traditional ML, the training dataset is fixed.</p>
</div></blockquote>
<p>i.e. map The confidence region to a coverage probability of an individual parameter, say parameter of interest</p>
<div class="math notranslate nohighlight">
\[\mathbb{CR}({\mu,\nu}) \rightarrow \mathbb{CPI}(\mu)\]</div>
<section id="connection-to-gaussian-distributed-measurements">
<h2>4.3: Connection to Gaussian Distributed Measurements<a class="headerlink" href="#connection-to-gaussian-distributed-measurements" title="Permalink to this headline">#</a></h2>
<section id="gaussian-1d">
<h3>4.3.1: Gaussian 1D<a class="headerlink" href="#gaussian-1d" title="Permalink to this headline">#</a></h3>
<p>Suppose we have a univariate random variable <span class="math notranslate nohighlight">\(X\)</span> (that is, <span class="math notranslate nohighlight">\(x \in \mathbb{R}^1\)</span>) that is sampled from a normal distribution <span class="math notranslate nohighlight">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span> The PDF of this RV (which is usually an estimate of the true mean) is parameterized by <span class="math notranslate nohighlight">\(\mu, \sigma^2\)</span></p>
<div class="math notranslate nohighlight">
\[p_X (x) = p(x) = p(x;\mu, \sigma^2) = \frac{1}{\sqrt{2\pi} \sigma} e^{\frac{- (x-\mu)^2 }{2 \sigma^2 }}\]</div>
<p>Suppose <span class="math notranslate nohighlight">\(\sigma^2\)</span> is known and one would like to make inference on <span class="math notranslate nohighlight">\(\mu_{\text{true}}\)</span>. The probability that this RV falls within <span class="math notranslate nohighlight">\(\pm \delta\)</span> of  <span class="math notranslate nohighlight">\(\mu_{\text{true}}\)</span>
is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\mathbb{P}(x \in [\mu_{\text{true}} - \delta,\mu_{\text{true}} + \delta]  &amp;= \frac{1}{\sqrt{2 \pi} \sigma} \int_{\mu_{\text{true}}-\delta}^{\mu+\delta} e^{-(x-\mu)^2  2 \sigma^2} d x \\
&amp; =\operatorname{erf}\left(\frac{\delta}{\sqrt{2} \sigma}\right) \\
&amp;= 2 \Phi\left(\frac{\delta}{\sigma}\right)-1,
\end{aligned}\end{split}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\Phi(x)\)</span> is the CDF of a standard normal RV (a standard normal means a RV sampled from <span class="math notranslate nohighlight">\(\mathcal{N}(\mu=0, \sigma^2=1)\)</span>.
$<span class="math notranslate nohighlight">\( \Phi(x)\equiv\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^x e^{-t^2 / 2} d t\)</span>$</p>
<p>and <span class="math notranslate nohighlight">\(\delta=z\sigma\)</span> where <span class="math notranslate nohighlight">\(z\)</span> is an integer and <span class="math notranslate nohighlight">\(\sigma\)</span> is the standard deviation, so that <span class="math notranslate nohighlight">\(\delta\)</span> is the number of standard deviations. The equation above says that the probability that <span class="math notranslate nohighlight">\(x\)</span> will fall within <span class="math notranslate nohighlight">\(\pm \delta\)</span> of the true value of <span class="math notranslate nohighlight">\(\mu\)</span> is
$<span class="math notranslate nohighlight">\( 2 \Phi \left( \frac{\delta}{\sigma} \right) -1\)</span><span class="math notranslate nohighlight">\(
If we set this probability to the confidence level, and plug in for \)</span>\delta$ we have</p>
<div class="math notranslate nohighlight">
\[2 \Phi \left( \frac{z \sigma}{\sigma} \right) -1 = 1-\alpha \tag{7}\]</div>
<p>Example: Eq 7 means that if we want the <span class="math notranslate nohighlight">\(2\sigma\)</span> inteval for <span class="math notranslate nohighlight">\(\theta\)</span>, we have
$<span class="math notranslate nohighlight">\( \mathbb{P} \left( \theta \in [\mu_{\text{true}} -2\sigma, \mu_{\text{true}} +2\sigma] \right) =  2 \Phi \left( 2 \right) -1 = 1-\alpha \)</span>$</p>
<p>And, using the <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution approximation,</p>
<div class="math notranslate nohighlight">
\[ \mathbb{P} \left( \theta \in [\mu_{\text{true}} -2\sigma, \mu_{\text{true}} +2\sigma] \right) =  2 \Phi \left( 2 \right) -1 = 1-\alpha  = F(\chi^2_n)\]</div>
<p>(Therefore, by setting the number of standard deviations, we are setting the <span class="math notranslate nohighlight">\(1-\alpha\)</span> level, whose value can be found either by the CDF of the gaussian or of the <span class="math notranslate nohighlight">\(\chi^2\)</span>, where in both cases these values are tabulated, so the results can be immediately attained for any Gaussian distrubtued value.</p>
</section>
<section id="multivariate-normal">
<h3>4.3.2: Multivariate normal<a class="headerlink" href="#multivariate-normal" title="Permalink to this headline">#</a></h3>
<p>Recall that the if we have a multivariate normal random variable <span class="math notranslate nohighlight">\(Y \in \mathbb{R}^d\)</span>. Then <span class="math notranslate nohighlight">\(Y \sim \mathcal{N}(\mu, \sigma^2)\)</span> if</p>
<div class="math notranslate nohighlight">
\[ p(y)=\frac{1}{(2 \pi)^{d / 2}|\Sigma|^{1 / 2}} \exp \left(-\frac{1}{2}(y-\mu)^T \Sigma^{-1}(y-\mu)\right)\]</div>
<p>For this multivariate distribution of size <span class="math notranslate nohighlight">\(p\)</span> (which usually represents an estimates <span class="math notranslate nohighlight">\(\mathbf{y}=\widehat{\boldsymbol{\theta}}=\left(\widehat{\theta}_1, \ldots, \widehat{\theta}_p\right)\)</span>), the construction of the <span class="math notranslate nohighlight">\(p-\)</span>dimensional confidence set requires the full covriance matrix <span class="math notranslate nohighlight">\(\mathrm{K}_{X_i X_j}\)</span>. The covariance matrix <span class="math notranslate nohighlight">\(\text{Cov}(X_i X_j)\)</span> is a matrix whose <span class="math notranslate nohighlight">\((i,j)\)</span> component is the covariance</p>
<div class="math notranslate nohighlight">
\[\operatorname{cov}\left[X_i, X_j\right]=\mathbb{E}\left[\left(X_i-\mathbb{E}\left[X_i\right]\right)\left(X_j-\mathbb{E}\left[X_j\right]\right)\right]\]</div>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> be a random vector, that is, a sequence of random variables <span class="math notranslate nohighlight">\((X_1, ..., X_d)\)</span>. We say that each realization of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is an element in <span class="math notranslate nohighlight">\(\mathcal{R}^d\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{equation}
\mathbb{E}[\mathbf{X}]:=\left(\begin{array}{c}
\mathbb{E}\left[X_1\right] \\
\mathbb{E}\left[X_2\right] \\
\vdots \\
\mathbb{E}\left[X_d\right]
\end{array}\right)=\left(\begin{array}{c}
\mu_1 \\
\mu_2 \\
\vdots \\
\mu_k
\end{array}\right)=: \boldsymbol{\mu}
\end{equation} \end{split}\]</div>
<p>And the covariance matrix (by the definition above) is</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{equation}
=\left(\begin{array}{ccc}
\mathbb{E}\left[\left(X_1-\mu_1\right)\left(X_1-\mu_1\right)\right] &amp; \cdots &amp; \mathbb{E}\left[\left(X_1-\mu_1\right)\left(X_k-\mu_k\right)\right] \\
\mathbb{E}\left[\left(X_2-\mu_2\right)\left(X_1-\mu_1\right)\right] &amp; \cdots &amp; \mathbb{E}\left[\left(X_2-\mu_2\right)\left(X_k-\mu_k\right)\right] \\
\vdots &amp; \vdots &amp; \vdots \\
\mathbb{E}\left[\left(X_d-\mu_d\right)\left(X_1-\mu_1\right)\right] &amp; \cdots &amp; \mathbb{E}\left[\left(X_d-\mu_k\right)\left(X_d-\mu_d\right)\right]
\end{array}\right)
\end{equation}
\end{split}\]</div>
</section>
<section id="bivariate-normal-confidence-ellipses">
<h3>4.4: Bivariate normal: Confidence ellipses<a class="headerlink" href="#bivariate-normal-confidence-ellipses" title="Permalink to this headline">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\vec{\theta}\)</span> be a bivariate normal variable</p>
<div class="math notranslate nohighlight">
\[\Theta \sim \mathcal{N}_2 (\vec{\mu},\Sigma) = [\theta_1.\theta_2] \equiv \vec{\theta}\]</div>
<p>That is, its PDF is given by</p>
<div class="math notranslate nohighlight">
\[ p(\vec{\theta})=\frac{1}{|2 \pi \Sigma|^{1 / 2}} \exp {\left(-\frac{1}{2}(\vec{\theta}-\vec{\mu})^T \Sigma^{-1}(\vec{\theta}-\vec{\mu})\right)} \]</div>
<p>The confidence region (or set) for a bivariate normal random variable, that is, is determined by a <em>confidence ellipse</em>. A confidence ellipse is defined to be the set of <span class="math notranslate nohighlight">\(\vec{\theta}\)</span> such that the PDF is equal to some constant <span class="math notranslate nohighlight">\(c\)</span>. And this means that the argument in the exponential is constant, that is</p>
<div class="math notranslate nohighlight">
\[\left(-\frac{1}{2}(\vec{\theta}-\mu)^T \Sigma^{-1}(\vec{\theta}-\mu)\right) = c.\]</div>
<p>That is, the confidence ellipse contours represent lines of constant density. We will show below that drawing lines of constant density results in drawing ellipses.</p>
</section>
<section id="aside-a-quick-review-of-ellipses">
<h3>4.5: Aside: a quick review of ellipses<a class="headerlink" href="#aside-a-quick-review-of-ellipses" title="Permalink to this headline">#</a></h3>
<p>The equation of an ellipse, cenntered at <span class="math notranslate nohighlight">\((x,y)=(0,0)\)</span> is</p>
<div class="math notranslate nohighlight">
\[\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1 \]</div>
<p>Where the length of the major axis is <span class="math notranslate nohighlight">\(a\)</span> and the length of the minor axis is <span class="math notranslate nohighlight">\(b\)</span> if <span class="math notranslate nohighlight">\(a&gt;b\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span>
<span class="c1"># of course, the sllipse can be prameterized as </span>
<span class="c1"># x = mu_1 + a cos(t), y = mu_2 + b sin(t)</span>
<span class="c1"># where (mu_1,mu_2) is the center of the ellipse</span>
<span class="k">def</span> <span class="nf">ellipse</span><span class="p">(</span><span class="n">major_ax</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">semi_major_ax</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">mu1</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mu2</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">math</span>
    <span class="n">t</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu1</span> <span class="o">+</span> <span class="n">major_ax</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">mu2</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
    <span class="n">vlmin</span><span class="p">,</span> <span class="n">vlmax</span> <span class="o">=</span> <span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span>
    <span class="n">hlmin</span><span class="p">,</span> <span class="n">hlmax</span> <span class="o">=</span> <span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=</span><span class="n">vlmin</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="n">vlmax</span><span class="p">);</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">hlmin</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">hlmax</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">mu1</span><span class="p">,</span> <span class="n">ymin</span><span class="o">=</span><span class="n">vlmin</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="n">vlmax</span><span class="p">);</span> 
    <span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">mu2</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">hlmin</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">hlmax</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$y$&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">ellipse</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4_Mapping_Confidence_Sets_to_Confidence_Intervals_10_0.png" src="_images/4_Mapping_Confidence_Sets_to_Confidence_Intervals_10_0.png" />
</div>
</div>
<p>To see how</p>
<div class="math notranslate nohighlight">
\[(\vec{\theta}-\mu)^T \Sigma^{-1}(\vec{\theta}-\mu) = c \tag{7}\]</div>
<p>is equivalent to an equation of an ellipse</p>
<div class="math notranslate nohighlight">
\[ \frac{x^2}{a^2} + \frac{y^2}{b^2}=1 \tag{8} \]</div>
<p>Note the following: let</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\vec{\theta}=\left[\begin{array}{l}
x \\
y
\end{array}\right]
\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{equation}
A=\left(\begin{array}{ll}
a^2 &amp; 0 \\
0 &amp; b^2
\end{array}\right)
\end{equation}\end{split}\]</div>
<p>Then</p>
<div class="math notranslate nohighlight">
\[ \vec{\theta}^T A^{-1} \vec{\theta} = \frac{x^2}{a^2} + \frac{y^2}{b^2} \]</div>
<p>And so Eq. 8 is equivlent to</p>
<div class="math notranslate nohighlight">
\[ \vec{\theta}^T A^{-1} \vec{\theta} =1. \tag{9}\]</div>
<p>Now, uppose that the ellipse is not centered around the origin, say that it’s centered at <span class="math notranslate nohighlight">\((\mu_1, \mu_2)\)</span>, then we can just subtract the vector of the center of the ellipse <span class="math notranslate nohighlight">\(\vec{\mu}\)</span> from Eq. 9:</p>
<div class="math notranslate nohighlight">
\[ (\vec{\theta}-\vec{\mu})^T A^{-1} (\vec{\theta}-\vec{\mu}) =1 \tag{10}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ellipse</span><span class="p">(</span><span class="n">mu1</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">mu2</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4_Mapping_Confidence_Sets_to_Confidence_Intervals_12_0.png" src="_images/4_Mapping_Confidence_Sets_to_Confidence_Intervals_12_0.png" />
</div>
</div>
<p>Now, <span class="math notranslate nohighlight">\(\Sigma\)</span> in Eq 8 might not be a diagonal matrix, as <span class="math notranslate nohighlight">\(A\)</span> is in Eq. 10. The effect of a non-diagonal matrix is to rotate the ellipse, so that rather than the major/minor axces being parallel to the coordinate axes, they would be parallel to some other axes.</p>
<p>To see this, apply the spectral decomposition to <span class="math notranslate nohighlight">\(\Sigma\)</span>, that is</p>
<div class="math notranslate nohighlight">
\[\Sigma = V \Lambda V^T\]</div>
<p>So Eq. 10 for <span class="math notranslate nohighlight">\(\Sigma\)</span> becomes</p>
<div class="math notranslate nohighlight">
\[ (\vec{\theta}-\vec{\mu})^T (V \Lambda V^T)^{-1} (\vec{\theta}-\vec{\mu}) =1. \tag{11}\]</div>
<p>Now, let <span class="math notranslate nohighlight">\(\vec{y}=V^T (\vec{\theta}-\vec{\mu})\)</span>. So, Eq. 11 becomes</p>
<div class="math notranslate nohighlight">
\[ y^T \Lambda y = 1. \tag{12}\]</div>
<p>Now, Eq. 12 is the equation of a standard ellipse, so <span class="math notranslate nohighlight">\(\Lambda\)</span> is now diagonal</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{equation}
\Lambda=\left(\begin{array}{ll}
\lambda_1 &amp; 0 \\
0 &amp; \lambda_2
\end{array}\right)
\end{equation}. \tag{13}\end{split}\]</div>
<p>So, Eq. 13 represents an equation of an ellipse centered at the origin with major axis <span class="math notranslate nohighlight">\(y_1\)</span> and semi-major axis <span class="math notranslate nohighlight">\(y_2\)</span>, with lengths of major and semi-major axes being <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\lambda_2\)</span>, respectively.</p>
<p>To convert <span class="math notranslate nohighlight">\(\vec{\theta}\)</span> in terms of <span class="math notranslate nohighlight">\(\vec{y}\)</span> ,</p>
<div class="math notranslate nohighlight">
\[\vec{\theta}=V \vec{y} + \vec{\mu}\]</div>
<p>So that the ellipse is oriented along the two eigenvectors <span class="math notranslate nohighlight">\(v_1, v_2\)</span> (as opposed to the original coordinate axes) and centered at <span class="math notranslate nohighlight">\(\vec{\mu}\)</span>.</p>
</section>
<section id="calculating-the-multivariate-coverage-that-is-the-area-volume-of-the-ellipse-ellipsoid">
<h3>4.6: Calculating the multivariate coverage, that is, the area (volume) of the ellipse (ellipsoid)<a class="headerlink" href="#calculating-the-multivariate-coverage-that-is-the-area-volume-of-the-ellipse-ellipsoid" title="Permalink to this headline">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\( \Theta \sim \mathcal{N}_p (\vec{\mu}, \Sigma)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[ (\vec{\theta}-\vec{\mu})^T \Sigma^{-1} (\vec{\theta}-\vec{\mu}) \sim \chi^2_p \]</div>
<p>Therefore, the <span class="math notranslate nohighlight">\((1-\alpha)\)</span> confidence set for bivariate normal <span class="math notranslate nohighlight">\(\vec{\theta}\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[ \mathbb{P} \left( (\vec{\theta} -\vec{\mu})^T \Sigma^{-1} (\vec{\theta}-\vec{\mu}) \le \chi^{2}_{2} (\alpha)  \right) = 1-\alpha, \]</div>
<p>where <span class="math notranslate nohighlight">\(\chi^2_2 (\alpha)\)</span> is the upper (100<span class="math notranslate nohighlight">\(\alpha\)</span>)th percentile of the <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution with 2 degrees of freedom. <span class="math notranslate nohighlight">\(1-\alpha\)</span> is the probability (or confidence level) that the value of the random vector <span class="math notranslate nohighlight">\(\vec{\theta}\)</span> will be inside the ellipse.</p>
<p>For example, take <span class="math notranslate nohighlight">\(\alpha=0.05\)</span>, so that <span class="math notranslate nohighlight">\(\chi^2_2 (0.05) = 5.99\)</span>, then, the <span class="math notranslate nohighlight">\((1-\alpha)\)</span>, or the 95 %, condidence ellopse is defined by</p>
<div class="math notranslate nohighlight">
\[ \mathbb{P} \left( (\vec{\theta} -\vec{\mu})^T \Sigma^{-1} (\vec{\theta}-\vec{\mu}) \le 5.99  \right) = 0.95. \tag{13}\]</div>
<p>So, say we want the 95% confidence ellipse, that is, the argument of Eq. 13 is</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{equation}
\left[\theta_1-\mu_1 \theta_2-\mu_2\right] \Sigma^{-1}\left[\begin{array}{l}
\theta_1-\mu_1 \\
\theta_2-\mu_2
\end{array}\right] \leq \chi_2^2(0.05)
\end{equation}\end{split}\]</div>
<p>Recall</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{equation}
\Sigma=\left[\begin{array}{ll}
\sigma_{11} &amp; \sigma_{12} \\
\sigma_{21} &amp; \sigma_{22}
\end{array}\right]
\end{equation}\end{split}\]</div>
<p>Using the above results for writing  <span class="math notranslate nohighlight">\(\Sigma=V \Lambda V^T\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{equation}
\Lambda=\left(\begin{array}{ll}
\lambda_1 &amp; 0 \\
0 &amp; \lambda_2
\end{array}\right)
\end{equation}.\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{equation}
\left[\begin{array}{ll}
\theta_1-\mu_1 &amp; \theta_2-\mu_2
\end{array}\right] V \Lambda^{-1} V^{T}\left[\begin{array}{l}
\theta_1-\mu_1 \\
\theta_2-\mu_2
\end{array}\right] \leq \chi_2^2(0.05)
\end{equation}\end{split}\]</div>
<p>Denote</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{equation}
\left[\begin{array}{l}
v_1 \\
v_2
\end{array}\right]=\Lambda^{-1}\left[\begin{array}{l}
\theta_1-\mu_1 \\
\theta_2-\mu_2
\end{array}\right]
\end{equation}
\end{split}\]</div>
<p>So the confidence ellipse will be given by</p>
<div class="math notranslate nohighlight">
\[ \begin{equation}
\frac{v_1^2}{\chi_2^2(0.05) \lambda_1}+\frac{v_2^2}{\chi_2^2(0.05) \lambda_2} \leq 1
\end{equation}\]</div>
<p>Where the eigenvalues are</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{array}{l}
\lambda_1=\frac{1}{2}\left[\sigma_{11}+\sigma_{22}+\sqrt{\left(\sigma_{11}-\sigma_{22}\right)^2+4 \sigma_{11} \sigma_{22} \rho^2}\right] \\
\lambda_2=\frac{1}{2}\left[\sigma_{11}+\sigma_{22}-\sqrt{\left(\sigma_{11}-\sigma_{22}\right)^2+4 \sigma_{11} \sigma_{22} \rho^2}\right]
\end{array}
\end{equation} \end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">Display</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">LFI_PIVOT_BASE</span><span class="p">,</span><span class="s1">&#39;images&#39;</span><span class="p">,</span><span class="n">filename</span><span class="p">)))</span>

<span class="n">Display</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="toy-experiments-sample-bivariate-normal">
<h3>Toy experiments: Sample bivariate normal<a class="headerlink" href="#toy-experiments-sample-bivariate-normal" title="Permalink to this headline">#</a></h3>
<p>Let’s take <span class="math notranslate nohighlight">\(\mathbf{\theta}=(\mu,\nu)\)</span> sampled from a bivariate normal with</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mu=\left[\begin{array}{l}
2 \\
2
\end{array}\right]
\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Sigma=\text{Cov}(\mu,\nu)=
\left[\begin{array}{cc}
1 &amp; -0.8 \\
-0.8 &amp; 1
\end{array}\right]
\end{split}\]</div>
<p>That is, <span class="math notranslate nohighlight">\(\mathbf{\theta} \)</span> is a <span class="math notranslate nohighlight">\(2\times 1\)</span> vector <span class="math notranslate nohighlight">\(\mathbf{\theta} \sim \mathcal{N}_2(\mu, \Sigma)\)</span>. And select the 68% confidence set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="n">mu</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mf">0.8</span><span class="p">],[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">bivariate</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">mu</span><span class="p">)</span>
<span class="n">samples</span><span class="o">=</span><span class="n">bivariate</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">samples</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mu$&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\nu$&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 2.60882699  1.39898454]
 [-0.64123783  3.29990794]
 [ 1.18221868  3.21031479]
 ...
 [ 4.61586531  0.68544702]
 [-0.34048567  3.7540929 ]
 [ 2.36199034  1.79477032]]
</pre></div>
</div>
<img alt="_images/4_Mapping_Confidence_Sets_to_Confidence_Intervals_20_1.png" src="_images/4_Mapping_Confidence_Sets_to_Confidence_Intervals_20_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">LFI_PIVOT_BASE</span><span class="p">,</span><span class="s1">&#39;images&#39;</span><span class="p">,</span><span class="s1">&#39;Gaussian_conf_ellipse.png&#39;</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4_Mapping_Confidence_Sets_to_Confidence_Intervals_21_0.png" src="_images/4_Mapping_Confidence_Sets_to_Confidence_Intervals_21_0.png" />
</div>
</div>
<p>For Gaussian distributed parameters, e.g. bivariate normal distribution, we can say something about the cconfidence interval of one of the parameters irrespective and independetly of the other parameter.</p>
<p>For the 2D case, i.e. say you have a pair of bivariate normally-distributed estimates <span class="math notranslate nohighlight">\(\mathbf{\theta}=(\theta_1,\theta_2)\)</span> the construction of the confidence ellipse requires the covariance matrix of the estimates</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\text{Cov}[\theta_i,\theta_j]$$.\\We then have an ellipse, and hence we can plot every point on that ellipse which coreresponds to our critical region. We can then map this confidence ellipse to the confidence interval of each parameter by\\$$\tan{ 2 \phi} = \frac{2 \rho_{ij} \sigma_i \sigma_j }{ \sigma_j^2 - \sigma_i^2}\end{aligned}\end{align} \]</div>
<p>Where <span class="math notranslate nohighlight">\(\rho_{ij} = \frac{\text{Cov}[\theta_i,\theta_j]}{\sigma_i, \sigma_j}\)</span></p>
<p>See Refs [1], [2] and [2] for the full perscription</p>
<p>The algorithm goes as follows:</p>
<ol class="simple">
<li><p>One first determines the confidence level. (For example, if you want a 95% confidence ellipse, you use a critical value of 1.06 which correspends to 2 standard deviations from the mean of the normal distribution).</p></li>
<li><p>one then calculates the eigenvectors and eigenvalues of the covariance matrix</p></li>
<li><p>Then calculate the two eigenvalues and eigenvectors of the covariance matrix</p></li>
<li><p>Calculate the angle of rotation for the ellipse using the eigenvectors. This angle is given by the arctangent of the ratio of the second eigenvector to the first.</p></li>
<li><p>Calculate the lengths of the semi-major and semi-minor axes of the ellipse. These are given by the square roots of the eigenvalues of the covariance matrix, scaled by the critical value for the desired level of confidence.</p></li>
<li><p>finally, plot the confidence ellipse centered at the mean of the distribution, rotated by the angle of rotation and  semi-major and semi-minor axes calculated in the steps above.</p></li>
</ol>
<p>[1] Johnson, R. A., &amp; Wichern, D. W. (2007). Applied multivariate statistical analysis (6th ed.). Pearson.</p>
<p>[2] Rencher, A. C. (2002). Methods of multivariate analysis (2nd ed.). Wiley.</p>
<p>[3] “Chew, V. (1966). Confidence, Prediction, and Tolerance Regions for the Multivariate Normal Distribution. Journal of the American Statistical Association, 61(315), 605. doi:10.2307/2282774</p>
</section>
</section>
<section id="hep-signal-background-problem-statement">
<h2>HEP Signal-Background Problem Statement<a class="headerlink" href="#hep-signal-background-problem-statement" title="Permalink to this headline">#</a></h2>
<p>In particle physics, the most important experiment is a counting experiment, represented by a Poisson probability model.Suppose we have a count experiment, we we observe in each bin (or channel) <span class="math notranslate nohighlight">\(k\)</span> a count <span class="math notranslate nohighlight">\(n_k\)</span>. Given <span class="math notranslate nohighlight">\(N\)</span> total channels, the probability of obtaining the observed result is given by the Poisson</p>
<div class="math notranslate nohighlight">
\[ L(\theta,\nu)= \prod_{k=1}^{N} \frac{e^{-(\epsilon_k \sigma + b_k)} (\epsilon_k \sigma + b_k)^{n_k}}{n_k !} \]</div>
<p>Where <span class="math notranslate nohighlight">\(\sigma\)</span>, the cross section (<strong>the parameter of interest</strong>), <span class="math notranslate nohighlight">\(b_k\)</span> is expected background for the <span class="math notranslate nohighlight">\(k\)</span>th channel (in this context, it is <strong>the nuissance parameter</strong>, which is a parameter that is not unkown precisely, typically constrained in a control measurement).In HEP, <span class="math notranslate nohighlight">\(\nu\)</span> can be related to the physics (theory) or the experimental aparatus/detector. <span class="math notranslate nohighlight">\(\epsilon_k\)</span> is the “acceptance parameter”, for the <span class="math notranslate nohighlight">\(k\)</span>th channel, which is typically a product of the detector efficiency, branching fraction, and luminosity.</p>
<p>This is the prototype of many statistical models in astronomy and particle physics in which data are binned and the count in each bin consist a priori of the sum of counts from signal and background. Written in terms of the simplified 2-parameter model, the expected count in each bin <span class="math notranslate nohighlight">\(k\)</span> takes the form</p>
<div class="amsmath math notranslate nohighlight" id="equation-f69f01a4-8374-4857-923c-ab26aa151754">
<span class="eqno">(10)<a class="headerlink" href="#equation-f69f01a4-8374-4857-923c-ab26aa151754" title="Permalink to this equation">#</a></span>\[\begin{equation}
    n_{expected}=\theta+\nu
\end{equation}\]</div>
<p>Where <span class="math notranslate nohighlight">\(n_{expected}\)</span> is the expected signal count, <span class="math notranslate nohighlight">\(\theta\)</span> is the unknown mean count, which is the parameter of interest (the cross section), and <span class="math notranslate nohighlight">\(\nu\)</span> is the background unknown mean count, which is the nuissance parameter. In other words,  the total number of evented, <span class="math notranslate nohighlight">\(n_{expected}\)</span>, in a single bin is sampled from a Poisson with mean with mean <span class="math notranslate nohighlight">\(\epsilon \sigma + b\)</span>.</p>
<p>The joint likelihood can then be constructed as</p>
<div class="math notranslate nohighlight">
\[\begin{align}
    \mathcal{L} (D ; \mu, \nu) &amp; = \frac{(\mu + \nu)^N \exp(-(\mu + \nu))}{N!} \,\frac{\nu^M\exp(-\nu)}{M!},
\end{align}\]</div>
<p>Note that this is different than the profiled likelihood over <span class="math notranslate nohighlight">\(\nu\)</span>
$<span class="math notranslate nohighlight">\(
\mathcal{L}_{\text{prof}}(D; \mu) \equiv \frac{e^{-(\mu+\hat{\nu})} (\mu+\hat{\nu})^N }{N !} \ \frac{e^{-\hat{\nu}} \hat{\nu}^M}{M !}.
\)</span>$</p>
<p>Notice that the likelihood also depends on nuissnace parameter <span class="math notranslate nohighlight">\(\nu\)</span>, which we don’t know precisely. Suppose then, that instead of measuring everying all at once, we control our measurement a bit more, such that we divide the space into a “control measurement”, where we are sure that the signal doesn’t occor (e.g. by selecting a region of phase space that we are sure a particular event doesnt happen from physics), and a “signal region”, in which both signal and background can occur. the conditional probability of observing <span class="math notranslate nohighlight">\(N\)</span> signal counts and <span class="math notranslate nohighlight">\(M\)</span> background counts (in a control measurement which is a single Poisson value) in a <em>single channel or bin</em> is given by</p>
<div class="math notranslate nohighlight">
\[
    P(N, M \mid \mu, \nu)= \mathcal{L} (D ; \mu, \nu) =  L(\mu, \nu) =  \frac{e^{-(\mu+v)}(\theta+v)^{N}}{N !}} \frac{e^{-v} v^{M}}{M !}
    \label{prob_model}
\]</div>
<p>where, once the counts <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(m\)</span> have been observed becomes the likelihood <span class="math notranslate nohighlight">\(L(\theta,\nu)\)</span>. <span class="math notranslate nohighlight">\(L(\theta,\nu)\)</span> could be viewed as a product of the main measurement where signal events (<span class="math notranslate nohighlight">\(s\)</span>) could be present, and a control measurement where only background events (<span class="math notranslate nohighlight">\(b\)</span>) exist, which helps us constrain the nuissance parameters.</p>
<p>This factorization of <span class="math notranslate nohighlight">\(L(\mu, \nu)\)</span> is called a controlled measurement, e.g. <span class="math notranslate nohighlight">\( L(\mu n_{\text{signal}} + b(\nu)) \)</span>.</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(D;\mu,\nu) = \mathcal{L}_{s+b}(\mu,\nu) \mathcal{L} _b(\nu)\]</div>
<p><span class="math notranslate nohighlight">\(\mathcal{L}_{s+b}(\mu,\nu)=\frac{e^{-(\mu+\nu)}(\mu+\nu)^{N}}{N !} \)</span> is called the signal+background region and <span class="math notranslate nohighlight">\(\mathcal{L} _b(\nu)=\frac{e^{-v} v^{M}}{M !}\)</span> is called the background region.</p>
<p>If you’re familiar with using “strength parameter” <span class="math notranslate nohighlight">\(\mu\)</span>,</p>
<div class="math notranslate nohighlight">
\[\mu \equiv \frac{\text{Lum.} \sigma_{\text{observed}} (\theta)}{\text{Lum.} \sigma_{\text{expected}} (\theta)},\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta\)</span> is the parameter of interest (such as <span class="math notranslate nohighlight">\(m_\text{Higgs}\)</span>), and “expected” means expected by the SM, of course.</p>
<p>then <span class="math notranslate nohighlight">\(L_{s+b}\)</span> corresponds to having <span class="math notranslate nohighlight">\(\mu=1\)</span> and <span class="math notranslate nohighlight">\(L_b\)</span> corresponds to <span class="math notranslate nohighlight">\(\mu=0\)</span> (backround-only, i.e. control, region).</p>
<p>The likelihood function (the compatibility of a hypothesis given a data set) contains all the information from the experiment that is relevent to inference for the parameters (also known as the <em>likelihood principle</em>). The likelihood function is defined only upto an arbitrary constant, and so only ratios of likelihoods containing different values of the parameters are meaningful.</p>
<div class="math notranslate nohighlight">
\[\begin{align}
    \mathcal{L} (D ; \hat{\mu}, \hat{\nu}) &amp; = \frac{(\hat{\mu} + \hat{\nu})^N \exp(-(\hat{\mu} + \hat{\nu}))}{N!} \,\frac{\hat{\nu}^M\exp(-\nu)}{M!},
\end{align},\]</div>
<p>where
$$</p>
<div class="amsmath math notranslate nohighlight" id="equation-6f328300-e5a3-4b2d-b25d-43916677c6ce">
<span class="eqno">(11)<a class="headerlink" href="#equation-6f328300-e5a3-4b2d-b25d-43916677c6ce" title="Permalink to this equation">#</a></span>\[\begin{equation}
     \hat{\mu} =\left\{
    \begin{array}{ll}
        N-M &amp; \text{ if } \quad  N&gt;M \\
        0 &amp; \quad \textrm{ otherwise.}
    \end{array}
    \right. 
\end{equation}\]</div>
<div class="math notranslate nohighlight">
\[
and
\]</div>
<div class="amsmath math notranslate nohighlight" id="equation-f95b97a0-72a4-484c-a9e8-396f3d9900f3">
<span class="eqno">(12)<a class="headerlink" href="#equation-f95b97a0-72a4-484c-a9e8-396f3d9900f3" title="Permalink to this equation">#</a></span>\[\begin{equation}
     \hat{\nu} =\left\{
    \begin{array}{ll}
        M &amp; \text{ if } \quad  \hat{\mu} = N - M \\
        (M+N)/2 &amp; \quad \textrm{ otherwise.}
    \end{array}
    \right. 
\end{equation}\]</div>
<p>$$</p>
<p>In this study we adopt LFI with robust frequentest critical value estimation. We generate data comprising the quadruplets <span class="math notranslate nohighlight">\(\{\mu_i, \nu_i, N_i, M_i, Z_i \}\)</span> where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\mu &amp; \sim \textrm{uniform}(0, 20), \\
\nu &amp; \sim \textrm{uniform}(0, 20), \\
\end{align}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\left\{
\begin{align}
n &amp; \sim \textrm{poisson}(\mu + \nu),\\
m &amp; \sim \textrm{poisson}(\nu),\\
\end{align}
\right\} \rightarrow \lambda_\text{gen}(n, m \mid \mu,\nu)
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\left\{
\begin{align}
N &amp; \sim \textrm{uniform}(0,10),\\
M &amp; \sim \textrm{uniform}(0, 10), \\
\end{align}
\right\} \rightarrow \lambda_D(N, M \mid \mu,\nu)
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
Z  = \mathbb{I}
\left[ \lambda_\text{gen}(n, m \mid \mu,\nu) \leq \lambda_D(N, M \mid \mu,\nu) \right], \tag{4}
\]</div>
<p>where the size of each of these samples is <span class="math notranslate nohighlight">\(B'\)</span>, <span class="math notranslate nohighlight">\(\mathbb{I}\)</span> is the indicator function, and <span class="math notranslate nohighlight">\(\lambda_D (D, \mu) = \lambda_{NP}(N, M, \mu)\)</span> is our chosen test statistic. In our new notation, eq. 4 is equivalent to</p>
<div class="math notranslate nohighlight">
\[
Z  = \mathbb{I}
\left[ \lambda(\mathcal{D} \mid \mu,\nu) \leq \lambda^{\prime}(D \mid \mu,\nu) \right]
\]</div>
<section id="todo-2-the-pivotal-lambda-model-in-notebook-5-does-the-same-thing-inference-on-mu-independently-of-nu-and-seems-to-work-we-should-include-it-in-this-paper-the-constraints-could-be-done-in-other-ways-than-just-adding-the-term-to-the-loss">
<h3>TODO 2: The pivotal <span class="math notranslate nohighlight">\(\lambda\)</span> model in notebook 5 does the same thing (inference on <span class="math notranslate nohighlight">\(\mu\)</span> independently of <span class="math notranslate nohighlight">\(\nu\)</span> and seems to work, we should include it in this paper. The constraints could be done in other ways than just adding the term to the loss.<a class="headerlink" href="#todo-2-the-pivotal-lambda-model-in-notebook-5-does-the-same-thing-inference-on-mu-independently-of-nu-and-seems-to-work-we-should-include-it-in-this-paper-the-constraints-could-be-done-in-other-ways-than-just-adding-the-term-to-the-loss" title="Permalink to this headline">#</a></h3>
</section>
<section id="todo-3-sample-mu-sim-pi-mu-then-nu-mu-sim-pi-mu-nu-which-is-equivalent-to-marginalizing-pi-nu">
<h3>TODO 3: sample <span class="math notranslate nohighlight">\(\mu \sim \pi(\mu)\)</span> then <span class="math notranslate nohighlight">\(\nu(\mu) \sim \pi(\mu,\nu)\)</span> which is equivalent to marginalizing <span class="math notranslate nohighlight">\(\pi(\nu)\)</span>.<a class="headerlink" href="#todo-3-sample-mu-sim-pi-mu-then-nu-mu-sim-pi-mu-nu-which-is-equivalent-to-marginalizing-pi-nu" title="Permalink to this headline">#</a></h3>
</section>
<section id="todo-4-verify-correct-coverage-of-1-parameter-via-neyman-6th">
<h3>TODO 4: verify correct coverage of 1 parameter via Neyman.6th<a class="headerlink" href="#todo-4-verify-correct-coverage-of-1-parameter-via-neyman-6th" title="Permalink to this headline">#</a></h3>
</section>
<section id="todo-4-use-differentiable-programming-as-in-jax-or-relaxxed-to-get-better-results-for-these-step-functions-during-trainig-or-use-a-kernel-density-estimate-as-a-histogram-alternative">
<h3>TODO 4: use differentiable programming (as in jax or relaxxed) to get better results for these step functions during trainig, or use a kernel density estimate as a histogram alternative.<a class="headerlink" href="#todo-4-use-differentiable-programming-as-in-jax-or-relaxxed-to-get-better-results-for-these-step-functions-during-trainig-or-use-a-kernel-density-estimate-as-a-histogram-alternative" title="Permalink to this headline">#</a></h3>
</section>
<section id="todo-5-apply-it-to-high-er-dimensional-parameter-space">
<h3>TODO 5: apply it to high(er) dimensional parameter space<a class="headerlink" href="#todo-5-apply-it-to-high-er-dimensional-parameter-space" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mu_hat_</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">):</span>
    <span class="c1"># np.where(condition, equals, otherwise_equals_to)</span>
    <span class="n">mu_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">n</span><span class="o">&gt;</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="o">-</span><span class="n">m</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mu_hat</span>

<span class="k">def</span> <span class="nf">nu_hat_</span><span class="p">(</span><span class="n">mu_hat</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">vectorize_int</span><span class="p">(</span><span class="n">scalar</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
    
    <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">mu_hat</span> <span class="o">=</span> <span class="n">vectorize_int</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">vectorize_int</span><span class="p">(</span><span class="n">m</span><span class="p">),</span> <span class="n">vectorize_int</span><span class="p">(</span><span class="n">mu_hat</span><span class="p">)</span>
    <span class="n">nu_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mu_hat</span> <span class="o">==</span> <span class="n">n</span><span class="o">-</span><span class="n">m</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="p">(</span><span class="n">m</span><span class="o">+</span><span class="n">n</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nu_hat</span>

<span class="k">def</span> <span class="nf">L_theta_nu</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">nu</span><span class="p">):</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">theta</span><span class="o">+</span><span class="n">nu</span><span class="p">)</span>
    <span class="n">p2</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nu</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p1</span><span class="o">*</span><span class="n">p2</span>

<span class="k">def</span> <span class="nf">L_prof_global</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">):</span>
    <span class="c1"># mu_hat = n-m</span>
    <span class="n">mu_hat</span><span class="o">=</span><span class="n">mu_hat_</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">)</span>
    <span class="n">nu_hat</span> <span class="o">=</span> <span class="n">nu_hat_</span><span class="p">(</span><span class="n">mu_hat</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>

    <span class="n">p1</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mu_hat</span><span class="o">+</span><span class="n">nu_hat</span><span class="p">)</span>
    <span class="n">p2</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nu_hat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p1</span><span class="o">*</span><span class="n">p2</span>
</pre></div>
</div>
</div>
</div>
<div class="math notranslate nohighlight">
\[ \begin{equation}
    \lambda(D, \theta)  = -2 \log \left[  \frac{\mathcal{L}(D ; \mu, \nu)}{\mathcal{L}(D ; \hat{\mu}, \hat{\nu})} \right],
    \label{Eq:lambda_on_off}
\end{equation}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">lambda_test_2d</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">nu</span><span class="p">):</span>
    <span class="n">Ln</span><span class="o">=</span> <span class="n">L_theta_nu</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">,</span><span class="n">theta</span><span class="p">,</span><span class="n">nu</span><span class="p">)</span>
    
    <span class="n">Ld</span><span class="o">=</span> <span class="n">L_prof_global</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">)</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-14</span>
    <span class="n">Ld</span><span class="o">=</span><span class="n">Ld</span><span class="o">+</span><span class="n">eps</span>
    <span class="n">lambda_</span>  <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">Ln</span><span class="o">/</span><span class="n">Ld</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">lambda_</span><span class="p">)</span>

<span class="n">chi2_exp_size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_hist_2d_data_2d_inference</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span>
              <span class="n">mumin</span><span class="p">,</span> <span class="n">mumax</span><span class="p">,</span>
              <span class="n">numin</span><span class="p">,</span> <span class="n">numax</span><span class="p">,</span> 
                      <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
               <span class="n">nbinsmu</span><span class="p">,</span> <span class="n">nbinsnu</span><span class="p">):</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">mumin</span><span class="p">,</span> <span class="n">mumax</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">numin</span><span class="p">,</span> <span class="n">numax</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="n">nu</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nu</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="c1">#lambda_test_2d(n,m, mu, nu)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="p">(</span><span class="n">lambda_test_2d</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">nu</span><span class="p">)</span><span class="o">&lt;</span> 
         <span class="n">lambda_test_2d</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">M</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">nu</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="n">murange</span> <span class="o">=</span> <span class="p">(</span><span class="n">mumin</span><span class="p">,</span> <span class="n">mumax</span><span class="p">)</span>
    <span class="n">nurange</span> <span class="o">=</span> <span class="p">(</span><span class="n">numin</span><span class="p">,</span> <span class="n">numax</span><span class="p">)</span>
    <span class="c1"># bins = binsize(Bprime)</span>

    <span class="c1"># Z-weighted histogram   (count the number of ones per bin)</span>
    <span class="c1">#mu will be on axis and nu on y axis</span>
    <span class="n">y_mu_nu_w</span><span class="p">,</span> <span class="n">bb_mu_edges</span><span class="p">,</span> <span class="n">bb_nu_edges</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram2d</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span>
                          <span class="n">bins</span><span class="o">=</span><span class="p">(</span><span class="n">nbinsmu</span><span class="p">,</span> <span class="n">nbinsnu</span><span class="p">),</span> 
                          <span class="nb">range</span><span class="o">=</span><span class="p">(</span><span class="n">murange</span><span class="p">,</span> <span class="n">nurange</span><span class="p">),</span> 
                          <span class="n">weights</span><span class="o">=</span><span class="n">Z</span><span class="p">)</span>
    
    <span class="c1"># unweighted histogram (count number of ones and zeros per bin)</span>
    <span class="n">y_mu_nu_uw</span><span class="p">,</span> <span class="n">bb_mu_edges</span><span class="p">,</span> <span class="n">bb_nu_edges</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram2d</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">nu</span><span class="p">,</span>
                          <span class="n">bins</span><span class="o">=</span><span class="p">(</span><span class="n">nbinsmu</span><span class="p">,</span> <span class="n">nbinsnu</span><span class="p">),</span> 
                          <span class="nb">range</span><span class="o">=</span><span class="p">(</span><span class="n">murange</span><span class="p">,</span> <span class="n">nurange</span><span class="p">))</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-15</span>
    <span class="n">P_mu_nu</span> <span class="o">=</span>  <span class="n">y_mu_nu_w</span> <span class="o">/</span> <span class="p">(</span><span class="n">y_mu_nu_uw</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>    
    <span class="c1">#P_mu_nu approximates E[Z]</span>
    <span class="k">return</span> <span class="n">P_mu_nu</span><span class="p">,</span> <span class="n">bb_mu_edges</span><span class="p">,</span> <span class="n">bb_nu_edges</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_P_byhist_global_2d</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">Bprime</span><span class="p">,</span>
              <span class="n">mumin</span><span class="p">,</span> <span class="n">mumax</span><span class="p">,</span>
              <span class="n">numin</span><span class="p">,</span> <span class="n">numax</span><span class="p">,</span> 
                      <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
               <span class="n">nbinsmu</span><span class="p">,</span> <span class="n">nbinsnu</span><span class="p">,</span>                    
                 <span class="n">save_plot</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    
    <span class="n">P_mu_nu</span><span class="p">,</span> <span class="n">bb_mu_edges</span><span class="p">,</span> <span class="n">bb_nu_edges</span> <span class="o">=</span> <span class="n">make_hist_2d_data_2d_inference</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span>
                  <span class="n">mumin</span><span class="p">,</span> <span class="n">mumax</span><span class="p">,</span>
                  <span class="n">numin</span><span class="p">,</span> <span class="n">numax</span><span class="p">,</span> 
                          <span class="n">N</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span>
                   <span class="n">nbinsmu</span><span class="p">,</span> <span class="n">nbinsnu</span><span class="p">)</span>
    
    <span class="n">bin_centers_mu</span> <span class="o">=</span> <span class="p">(</span><span class="n">bb_mu_edges</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">bb_mu_edges</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">bin_centers_nu</span> <span class="o">=</span> <span class="p">(</span><span class="n">bb_nu_edges</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">bb_nu_edges</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mi">2</span>
    <span class="c1">#WHOLE RANGE</span>
    <span class="n">murange</span> <span class="o">=</span> <span class="p">(</span><span class="n">mumin</span><span class="p">,</span> <span class="n">mumax</span><span class="p">)</span>
    <span class="n">nurange</span> <span class="o">=</span> <span class="p">(</span><span class="n">numin</span><span class="p">,</span> <span class="n">numax</span><span class="p">)</span>
    <span class="c1"># murange = (mumin, 10)</span>
    <span class="c1"># nurange = (numin, 10)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.75</span><span class="p">,</span><span class="mf">0.9</span><span class="p">,</span> <span class="sa">r</span><span class="s1">&#39;$\mu \in [</span><span class="si">%s</span><span class="s1">,</span><span class="si">%s</span><span class="s1">], \nu \in [</span><span class="si">%s</span><span class="s1">,</span><span class="si">%s</span><span class="s1">] $&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">mumin</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">mumax</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">numin</span><span class="p">),</span><span class="nb">str</span><span class="p">(</span><span class="n">numax</span><span class="p">)),</span> 
            <span class="n">horizontalalignment</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">verticalalignment</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span><span class="n">transform</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>

    <span class="c1">#Remember mu is on x and nu on y axes, so next line, each will be 2d</span>
    <span class="n">MU</span><span class="p">,</span> <span class="n">NU</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">bin_centers_mu</span><span class="p">,</span> <span class="n">bin_centers_nu</span><span class="p">)</span>
    
    <span class="n">MU_1d</span><span class="p">,</span> <span class="n">NU_1d</span> <span class="o">=</span> <span class="n">MU</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">NU</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    
    <span class="n">P_mu_nu</span> <span class="o">=</span> <span class="n">P_mu_nu</span><span class="o">.</span><span class="n">T</span>
    <span class="n">P_mu_nu_byhist</span> <span class="o">=</span> <span class="n">P_mu_nu</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    
    <span class="n">ax</span><span class="o">.</span><span class="n">hist2d</span><span class="p">(</span><span class="n">MU_1d</span><span class="p">,</span> <span class="n">NU_1d</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="p">(</span><span class="n">nbinsmu</span><span class="p">,</span> <span class="n">nbinsnu</span><span class="p">),</span> 
               <span class="nb">range</span><span class="o">=</span><span class="p">(</span><span class="n">murange</span><span class="p">,</span> <span class="n">nurange</span><span class="p">),</span>
               <span class="n">weights</span><span class="o">=</span><span class="n">P_mu_nu_byhist</span><span class="p">,</span>
             <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues_r&#39;</span>
             <span class="p">)</span>
    
    <span class="n">CLs</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.683</span><span class="p">,</span><span class="mf">0.90</span><span class="p">,</span><span class="mf">0.95</span><span class="p">])</span>
    
    <span class="c1">#To draw contours, the intensity (the p-value) must be 2D again</span>
    <span class="n">P_mu_nu_byhist_2d</span> <span class="o">=</span> <span class="n">P_mu_nu_byhist</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">MU</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="c1">#for contours everything has to be 2d</span>
    <span class="n">contours</span><span class="o">=</span><span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">MU</span><span class="p">,</span> <span class="n">NU</span><span class="p">,</span> <span class="n">P_mu_nu_byhist_2d</span><span class="p">,</span>
              <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="n">mumin</span><span class="p">,</span> <span class="n">mumax</span><span class="p">,</span> <span class="n">numin</span><span class="p">,</span><span class="n">numax</span><span class="p">),</span>
              <span class="n">levels</span><span class="o">=</span><span class="n">CLs</span><span class="p">,</span>
                        <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gist_earth_r&#39;</span>
                       <span class="p">)</span>
    
<span class="c1">#     #label the contours</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">contours</span><span class="p">,</span> <span class="n">contours</span><span class="o">.</span><span class="n">levels</span><span class="p">,</span> <span class="n">inline</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
              <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%4.2f</span><span class="s1">&#39;</span><span class="p">,</span> 
              <span class="c1"># colors=&#39;black&#39;,</span>
              <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mu$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\nu$&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;CS$(\mu,\nu)$ by $\mathbf</span><span class="si">{h}</span><span class="s1">$: $N = </span><span class="si">%s</span><span class="s1">$ , $M  = </span><span class="si">%s</span><span class="s1"> $ &#39;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">N</span><span class="p">),</span><span class="nb">str</span><span class="p">(</span><span class="n">M</span><span class="p">)),</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    <span class="c1">#add contour color bar</span>
    <span class="c1"># cbar = fig.colorbar(contours, ax=ax)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">mumin</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">numin</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">save_plot</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;LFI_PIVOT_BASE&#39;</span><span class="p">],</span> <span class="s1">&#39;images&#39;</span><span class="p">,</span> 
                                 <span class="sa">f</span><span class="s2">&quot;2D_Hist_mumin_</span><span class="si">{</span><span class="n">mumin</span><span class="si">}</span><span class="s2">_mumax_</span><span class="si">{</span><span class="n">mumax</span><span class="si">}</span><span class="s2">_numin_</span><span class="si">{</span><span class="n">numin</span><span class="si">}</span><span class="s2">_numax_</span><span class="si">{</span><span class="n">numax</span><span class="si">}</span><span class="s2">_2D_INFERENCE.png&quot;</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="suppose-we-now-make-an-observation-and-remid-yourself-of-definitions-of-our-parameters">
<h1>Suppose we now make an observation, and remid yourself of definitions of our parameters<a class="headerlink" href="#suppose-we-now-make-an-observation-and-remid-yourself-of-definitions-of-our-parameters" title="Permalink to this headline">#</a></h1>
<section id="observed-data">
<h2>Observed Data:<a class="headerlink" href="#observed-data" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> (observed counts for signal)</p></li>
<li><p><span class="math notranslate nohighlight">\(M\)</span> (observed counts for background)</p></li>
</ul>
</section>
<section id="parameters">
<h2>Parameters:<a class="headerlink" href="#parameters" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu\)</span>: parameter of interest, proportional to <span class="math notranslate nohighlight">\(\sigma\)</span> in HEP (unknown signal mean)</p></li>
<li><p><span class="math notranslate nohighlight">\(\nu\)</span>: nuissance parameter (unknown background mean)</p></li>
</ul>
</section>
<section id="auxiliary-simulated-data-simulated-on-the-fly-for-each-observation">
<h2>Auxiliary (simulated) Data (simulated on-the-fly for each observation):<a class="headerlink" href="#auxiliary-simulated-data-simulated-on-the-fly-for-each-observation" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span>: expected signal count</p></li>
<li><p><span class="math notranslate nohighlight">\(m\)</span>: expected backround count</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="compute-coverage-for-set-and-show-it-s-satisfied-then-compute-coverage-for-individual-arameter-theta-with-neyman-construction-and-show-that-it-has-nominal-coverage">
<h1>Compute coverage for set and show it’s satisfied. Then compute coverage for individual arameter <span class="math notranslate nohighlight">\(\theta\)</span> with Neyman construction and show that it has nominal coverage<a class="headerlink" href="#compute-coverage-for-set-and-show-it-s-satisfied-then-compute-coverage-for-individual-arameter-theta-with-neyman-construction-and-show-that-it-has-nominal-coverage" title="Permalink to this headline">#</a></h1>
<p>The idea was that in the case that <span class="math notranslate nohighlight">\(\theta =(\mu,\nu) \in \mathbb{R}^2\)</span> we were able to get a confidence set for <span class="math notranslate nohighlight">\((\mu,\nu)\)</span> by using trained on <span class="math notranslate nohighlight">\(\lambda(D=\{N,M\})\)</span> as data:</p>
<div class="math notranslate nohighlight">
\[ X_{train} = \{ \mu_{\text{continuous}}, \nu_{\text{continuous}},\lambda(D=\{ N, M\} ) \}\]</div>
<div class="math notranslate nohighlight">
\[t_{train} = Z_{(\theta,\nu)} \]</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="alffi-algorithm-see-chapters-2-and-3-and-compute-coveragage-py">
<h1>ALFFI algorithm (see chapters 2 and 3 and <code class="docutils literal notranslate"><span class="pre">compute_coveragage.py</span></code>)<a class="headerlink" href="#alffi-algorithm-see-chapters-2-and-3-and-compute-coveragage-py" title="Permalink to this headline">#</a></h1>
<p>Recall that the idea of our ALFFI algorithm was</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">LFI_PIVOT_BASE</span><span class="p">,</span><span class="s1">&#39;images&#39;</span><span class="p">,</span><span class="s1">&#39;ALFFI_screenshot.png&#39;</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4_Mapping_Confidence_Sets_to_Confidence_Intervals_37_0.png" src="_images/4_Mapping_Confidence_Sets_to_Confidence_Intervals_37_0.png" />
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="allffi-cpi">
<h1>ALLFFI-CPI<a class="headerlink" href="#allffi-cpi" title="Permalink to this headline">#</a></h1>
<p>We want to map from the confidence set (CS) of a <span class="math notranslate nohighlight">\((\mu,\nu)\)</span> point <span class="math notranslate nohighlight">\(CS(\mu,\nu)\)</span> to a confidence interval (CI) of each individual parameter <span class="math notranslate nohighlight">\(CI(\mu), CI(\nu)\)</span>.</p>
<p>We want an input being the data <span class="math notranslate nohighlight">\(D\)</span> and paramters <span class="math notranslate nohighlight">\(\theta\)</span> generated on-the-fly, and the output being a CI on a single parameter.</p>
<p>Another advantage of this approach is that it can be done in an arbitrary number of dimensions</p>
<p>The essential idea of the algorithm is to simply generate parameter points inside a particular confidence set, and take the minimum and maximum of those points as the upper and lower confidence intervals. Once you have the edges of a parameter at a particular CL, you can map the CS to the CI of that parameter at that CL.</p>
<p>Essentially, for a given CL, we have the contours in the region, so the edges of a parameter at the given CL can be found
$<span class="math notranslate nohighlight">\(\{ \vec{\mu}_{edge}^{CL}, \vec{\nu}_{edge}^{CL} \}=  \{ \vec{\mu}, \vec{\nu} : \hat{p} \le \tau \}\)</span>$</p>
<p>So that the intervals for a parameter at a given CL is simply
$<span class="math notranslate nohighlight">\((\mu_{lo}, \mu_{up} ) = \left( min(\vec{\mu}_{edge}^{CL}), max(\vec{\mu}_{edge}^{CL}) \right)  \)</span>$</p>
<p>(idea another way we can do this is potentially to use <span class="math notranslate nohighlight">\(\lambda_{2D}\)</span> to get the confidence set, but then use <span class="math notranslate nohighlight">\(\lambda_{1D}(\theta)\)</span> to get the interval of <span class="math notranslate nohighlight">\(\theta\)</span> within the set.</p>
<p>So the NN is just a function of <span class="math notranslate nohighlight">\(\theta\)</span> and a particular CL <span class="math notranslate nohighlight">\(\tau\)</span> , and gives the coverage probability of an individual parameter <span class="math notranslate nohighlight">\(\mathbb{CPI}(\theta,\tau)\)</span> . Once we have this function, we can say “I would like to have a confidence level on <span class="math notranslate nohighlight">\(\theta\)</span> of 68%, and a way to achieve that is to construct a set whose coverage probability is 73%.</p>
<p>Currently, we can’t do that mapping because we only have the confidence region, i.e. a confidence interval on one parameter only in the existence of the confidence interval on the other parameter. We would like to do it globally, such that if we have confidence region, we can map that to confidence interval of other parameter regardless of the value of the other parameter. We use the Gaussian conffidence ellipse as a heuristic motivation of what we would like to do.</p>
<section id="alffi-cpi-for-inference-using-one-single-cl-tau-1">
<h2>ALFFI-CPI For inference using one single CL <span class="math notranslate nohighlight">\(\tau_1\)</span><a class="headerlink" href="#alffi-cpi-for-inference-using-one-single-cl-tau-1" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">LFI_PIVOT_BASE</span><span class="p">,</span><span class="s1">&#39;images&#39;</span><span class="p">,</span><span class="s1">&#39;CPI_one_tau.png&#39;</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4_Mapping_Confidence_Sets_to_Confidence_Intervals_39_0.png" src="_images/4_Mapping_Confidence_Sets_to_Confidence_Intervals_39_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mumin</span><span class="p">,</span> <span class="n">mumax</span> <span class="o">=</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">20</span>
<span class="n">numin</span><span class="p">,</span> <span class="n">numax</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">20</span>
<span class="n">Nmin</span><span class="p">,</span> <span class="n">Nmax</span> <span class="o">=</span>  <span class="mi">1</span><span class="p">,</span><span class="mi">10</span>
<span class="n">Mmin</span><span class="p">,</span> <span class="n">Mmax</span> <span class="o">=</span>  <span class="mi">1</span> <span class="p">,</span> <span class="mi">10</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_data_CPI</span><span class="p">(</span><span class="n">Bprime</span><span class="p">,</span><span class="n">save_data</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># $\theta \sim \pi_\theta$</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">mumin</span><span class="p">,</span> <span class="n">mumax</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="n">nu</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">numin</span><span class="p">,</span> <span class="n">numax</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="c1"># $\mathca{D} \sim F_theta$</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">mu</span><span class="o">+</span><span class="n">nu</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nu</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="c1"># $D$</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">Nmin</span><span class="p">,</span> <span class="n">Nmax</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">Mmin</span><span class="p">,</span> <span class="n">Mmax</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">Bprime</span><span class="p">)</span>
    <span class="c1"># Z_mu_nu</span>
    <span class="n">lambda_mu_nu</span> <span class="o">=</span> <span class="n">lambda_test_2d</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">nu</span><span class="p">)</span>
    <span class="n">lambda_prime_mu_nu</span> <span class="o">=</span> <span class="n">lambda_test_2d</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">M</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">nu</span><span class="p">)</span>
    <span class="n">Z_mu_nu</span> <span class="o">=</span> <span class="p">(</span><span class="n">lambda_mu_nu</span> <span class="o">&lt;=</span><span class="n">lambda_prime_mu_nu</span> <span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">data_CPI</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;mu&#39;</span><span class="p">:</span> <span class="n">mu</span><span class="p">,</span> <span class="s1">&#39;nu&#39;</span><span class="p">:</span> <span class="n">nu</span><span class="p">,</span> <span class="s1">&#39;n&#39;</span><span class="p">:</span><span class="n">n</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">:</span><span class="n">m</span><span class="p">,</span> <span class="s1">&#39;N&#39;</span><span class="p">:</span><span class="n">N</span><span class="p">,</span> <span class="s1">&#39;M&#39;</span><span class="p">:</span><span class="n">M</span><span class="p">,</span> <span class="s1">&#39;Z_mu_nu&#39;</span><span class="p">:</span><span class="n">Z_mu_nu</span><span class="p">}</span>
    
    <span class="n">data_CPI</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">data_CPI</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data_CPI</span>
    
    
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generate_data_CPI</span><span class="p">(</span><span class="n">Bprime</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">save_data</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
</div>
</div>
<p>Check coverage of the set at several <span class="math notranslate nohighlight">\(\tau\)</span> values and observe that for the 1D case, using our 2D algorithm the coverage is exact</p>
</section>
<section id="alffi-cpi-for-any-all-cls-tau">
<h2>ALFFI-CPI for any (all) CLs <span class="math notranslate nohighlight">\(\tau\)</span><a class="headerlink" href="#alffi-cpi-for-any-all-cls-tau" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">Image</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">LFI_PIVOT_BASE</span><span class="p">,</span><span class="s1">&#39;images&#39;</span><span class="p">,</span><span class="s1">&#39;Confidence_set_any_tau_algorithm.png&#39;</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4_Mapping_Confidence_Sets_to_Confidence_Intervals_45_0.png" src="_images/4_Mapping_Confidence_Sets_to_Confidence_Intervals_45_0.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="3_Replacing_Data_with_Lambda_and_2D_Inference.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">3 -  2-Dimensional Inference in <span class="math notranslate nohighlight">\(\theta - \nu\)</span> Space and Replacing Observed Data with Test Statistics</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="5_Imposing_Pivotal_Conditions_on_Lambda.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">5. Imposing Pivotal Conditions on <span class="math notranslate nohighlight">\(\lambda\)</span></p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Ali Al Kadhim and Harrison Prosper<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>